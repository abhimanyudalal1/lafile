{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f68f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "import langchain\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "\n",
    "#initialize model\n",
    "lafilamod= OllamaLLM(\n",
    "    model=\"Llama3:latest\",\n",
    "    temperature=0,\n",
    "    num_ctx=2048,\n",
    "    verbose=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccec985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88264cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Read pdf, get text\n",
    "reader = PdfReader('/Users/abhimanyu/Downloads/Hands-On_Large_Language_Models_compressed.pdf')\n",
    "page_ids = []\n",
    "page_texts = []\n",
    "for i, p in enumerate(reader.pages):\n",
    "    page_ids.append(str(i))\n",
    "    page_texts.append(p.extract_text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0090ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597']\n"
     ]
    }
   ],
   "source": [
    "print(page_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5e2a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "# # Create a MarkdownTextSplitter for semantic chunking\n",
    "# semantic_splitter = MarkdownTextSplitter(\n",
    "#     chunk_size=200,    # Number of characters per chunk\n",
    "#     chunk_overlap=50   # Overlap to preserve context\n",
    "# )\n",
    "\n",
    "# # Split the text into semantic chunks\n",
    "# semantic_chunks = semantic_splitter.split_text(page_texts)\n",
    "# print(\"\\nSemantic Chunks (Markdown-based):\\n\", semantic_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccef48a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ql/hdbyk3lx6vq0x6fs9227fc_m0000gn/T/ipykernel_50628/1651442095.py:10: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model=HuggingFaceBgeEmbeddings(model_name='thenlper/gte-small')\n",
      "/Users/abhimanyu/Desktop/geospacy/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W0804 09:57:47.185000 50628 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "/Users/abhimanyu/Desktop/geospacy/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# #now embeddings for the text\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# embed_model=SentenceTransformer(\"\")\n",
    "\n",
    "# text_embeddings=embed_model.encode(page_texts)\n",
    "\n",
    "# index_embeddings=embed_model.encode(page_ids)\n",
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "embed_model=HuggingFaceBgeEmbeddings(model_name='thenlper/gte-small')\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "metadata= [{\"page\":int(i)} for i in page_ids]\n",
    "database= FAISS.from_texts(page_texts, embed_model, metadatas=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f2bc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#query rewriting\n",
    "# def querewrite(query):\n",
    "rewtemp=(\n",
    "        \"You are an AI assistant. Rewrite the following user query into a clear, concise search query suitable for retrieving relevant documents. \"\n",
    "        \"User Query:+ {query} \\nRewritten Query:\"\n",
    "    )\n",
    "rewprompt= PromptTemplate(template= rewtemp, input_variables=['query'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b90191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1=  rewprompt | lafilamod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e934d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"],\n",
    "     template=\"{page_content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a70db602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import RegexParser\n",
    "parser= RegexParser(\n",
    "    regex=r\"answer: (.*?)\\nScore:(.*)\",\n",
    "    output_keys=[\"answer\",\"score\"],\n",
    ")\n",
    "# template1=\"\"\"\n",
    "# <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "# you are a search retrieval agent that answers precisely to the query.the format needs to be like:\n",
    "# Answer: <some answer>\n",
    "# Score: <number between 0 and 1>\n",
    "# <|eot_id|>\n",
    "# <|start_header_id|>user<|end_header_id|>\n",
    "# provide a relevant answer according to the user query:\n",
    "# {context}\n",
    "# the format needs to be like:\n",
    "# Answer: <some answer>\n",
    "# Score: <number between 0 and 1>\n",
    "\n",
    "# <|eod_id|>\n",
    "# <|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# rag_prompt = PromptTemplate.from_template(\"\"\"\n",
    "# Given the following document and a question, provide a helpful answer and a score between 0 and 1 indicating how well the document answers the question.\n",
    "\n",
    "# Document:\n",
    "# {context}\n",
    "\n",
    "# Question:\n",
    "# {question}\n",
    "\n",
    "# Answer the question and rate the relevance.\n",
    "\n",
    "# Output format:\n",
    "# Answer: <your answer>\n",
    "# Score: <score between 0 and 1>\n",
    "# \"\"\")\n",
    "template1map=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an intelligent assistant helping to answer a user's question using a provided document.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "provide a relevant answer according to the user query:\n",
    "{question}.\n",
    "\n",
    "Document: {summaries}\n",
    "\n",
    "the format needs to be like:\n",
    "Answer: <some answer>\n",
    "Score: <number between 0 and 1>\n",
    "\n",
    "<|eod_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "template1combine=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a smart assistant combining multiple partial answers from different documents to answer a user’s question.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "question:{question}\n",
    "partial answers: {summaries}\n",
    "\n",
    "based on the above construct a concrete final answer for the user:\n",
    "<|eod_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "map_prompt= PromptTemplate(template=template1map, input_variables=[ \"question\",\"summaries\"])\n",
    "combine_prompt= PromptTemplate(template=template1combine, input_variables=[\"question\", \"summaries\"])\n",
    "document_variable_name=\"summaries\"\n",
    "# from langchain.chains import RetrievalQA\n",
    "# rag= RetrievalQA.from_chain_type(llm=lafilamod, chain_type=\"map_rerank\", retriever= database.as_retriever(), chain_type_kwargs={'prompt': rag_prompt},output_parser=parser, verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90a911ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA, LLMChain, MapReduceDocumentsChain, StuffDocumentsChain, ReduceDocumentsChain\n",
    "\n",
    "# rag= RetrievalQA.from_chain_type(llm=lafilamod, chain_type=\"map_reduce\", retriever= database.as_retriever(),chain_type_kwargs={'map_prompt': map_prompt,\"combine_prompt\": combine_prompt})\n",
    "\n",
    "llm_chain_map= LLMChain(llm=lafilamod, prompt= map_prompt)\n",
    "llm_chain_combine=LLMChain(llm=lafilamod, prompt=combine_prompt)\n",
    "\n",
    "combine_docschain=StuffDocumentsChain(llm_chain=llm_chain_combine,     document_prompt=document_prompt,\n",
    "document_variable_name=document_variable_name)\n",
    "\n",
    "reduce_doc_chain=ReduceDocumentsChain(combine_documents_chain=combine_docschain)\n",
    "\n",
    "map_reduce_chain= MapReduceDocumentsChain(\n",
    "    llm_chain= llm_chain_map,\n",
    "    reduce_documents_chain= reduce_doc_chain,\n",
    "    document_variable_name=document_variable_name\n",
    ")\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag= RetrievalQA( retriever= database.as_retriever(), combine_documents_chain= map_reduce_chain )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bae54fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain2= rag | chain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a1531b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Here's a rewritten version of your query that can help retrieve relevant documents:\n",
      "\n",
      "**Search Query:** \"Large Language Model (LLM) Agent Definition\"\n",
      "\n",
      "This rewritten query uses specific keywords and phrases to help search engines or databases quickly identify relevant documents. The use of parentheses around \"Large Language Model\" helps to clarify the meaning of LLM, which is a type of AI model.\n"
     ]
    }
   ],
   "source": [
    "rag_response=chain2.invoke(\"what is an llm agent?\")\n",
    "print(rag_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5a3daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "you are a search retrieval agent that answers precisely to the query. \n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "provide a relevant answer according to the user query:\n",
    "{question}\n",
    "Relevant text:\n",
    "{context}\n",
    "\n",
    "<|eod_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "stuff_prompt=PromptTemplate(template= template1, input_variables=[\"question\",\"context\"])\n",
    "ragstuff= RetrievalQA.from_chain_type(llm=lafilamod, chain_type=\"stuff\", retriever= database.as_retriever(), chain_type_kwargs={'prompt': stuff_prompt, 'document_variable_name': 'context'}, verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1efdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(question):\n",
    "    response= ragstuff.invoke(question)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cdfd8b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is an llm agent, how to use it for search retrieval',\n",
       " 'result': '**What is an LLM Agent?**\\n\\nAn LLM (Large Language Model) agent is a system that leverages a language model to determine its actions and make decisions. It can interact with the real world through the use of tools, allowing it to perform tasks beyond what an LLM can do in isolation.\\n\\n**How to Use an LLM Agent for Search Retrieval**\\n\\nTo create an LLM agent for search retrieval, you can follow these steps:\\n\\n1. **Define the tools**: Identify the tools that the agent can use to interact with the outside world. For example, you can create a tool that allows the agent to access a web search engine (e.g., DuckDuckGo) and a math tool that provides basic calculator functionality.\\n2. **Create the LLM agent**: Use a framework like ReAct to create an agent that can reason about its thoughts, take actions, and observe the results. This involves defining the agent\\'s name, description, and function (e.g., searching the web or using a calculator).\\n3. **Prepare the tools**: Load the tools into the agent using the `load_tools` function.\\n4. **Create an AgentExecutor**: Use the AgentExecutor to handle executing the steps defined by the agent.\\n\\n**Example Code**\\n\\nHere\\'s some example code that demonstrates how to create an LLM agent for search retrieval:\\n```python\\nfrom langchain.agents import AgentExecutor, create_react_agent\\nfrom langchain.tools import DuckDuckGoSearchResults\\n\\n# Create a search tool\\nsearch = DuckDuckGoSearchResults()\\nsearch_tool = Tool(\\n    name=\"duckduck\",\\n    description=\"A web search engine. Use this to as a search engine for general queries.\",\\n    func=search.run,\\n)\\n\\n# Prepare the tools\\ntools = load_tools([\"llm-math\"], llm=openai_llm)\\ntools.append(search_tool)\\n\\n# Create the ReAct agent and AgentExecutor\\nagent = create_react_agent()\\nexecutor = AgentExecutor(agent)\\n\\n# Use the agent to search for a MacBook Pro price\\ninput_text = \"What is the price of a MacBook Pro?\"\\noutput = executor.execute(input_text)\\n\\nprint(output)\\n```\\nIn this example, we create an LLM agent that can use a web search engine (DuckDuckGo) and a math tool (basic calculator). We then define an input text (\"What is the price of a MacBook Pro?\") and execute the agent using the AgentExecutor. The output will be the result of the search query, which may include multiple results or a summary of the findings.\\n\\n**Note**: This code assumes you have the `langchain` library installed and have set up your LLM model (e.g., OpenAI\\'s LLaMA). You can modify the code to suit your specific use case and requirements.'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(\"what is an llm agent, how to use it for search retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9f90de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding conversation summary to the llm now\n",
    "##but do we need memory??##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faad4cb",
   "metadata": {},
   "source": [
    "what we need is. a better retrieval logic that very accurately takes on to the right page(end goal) \n",
    "but right now we need that **at least the top 2** has the right one in the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d3cd9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 318\n",
      "Content: Figur e 8-3. A RAG system formulates an answer to a question and (pr eferably) cites its information\n",
      "sources.\n",
      "The rest of the chapter covers these three types of systems in more detail.\n",
      "While these are the major categories, they are not the only LLM\n",
      "applications in the domain of search.\n",
      "Semantic Search with Language Models\n",
      "Let’s now dive into more detail on the major categories of systems that can\n",
      "upgrade the search capabilities of our language models. We’ll start with\n",
      "dense retrieval and then move on through reranking and RAG.\n",
      "Dense Retrieval\n",
      "Recall that embeddings turn text into numeric representations. Those can be\n",
      "thought of as points in space, as we can see in Figure 8-4 . Points that are\n",
      "close together mean that the text they represent is similar . So in this\n",
      "example, text 1 and text 2 are more similar to each other (because they are\n",
      "near each other) than text 3 (because it’ s farther away).\n",
      "\n",
      "Page: 356\n",
      "Content: Query routing\n",
      "An additional enhancement is to give the model the ability to search\n",
      "multiple data sources. We can, for example, specify for the model that if it\n",
      "gets a question about HR, it should search the company’ s HR information\n",
      "system (e.g., Notion) but if the question is about customer data, that it\n",
      "should search the customer relationship management (CRM) (e.g.,\n",
      "Salesforce).\n",
      "Agentic RAG\n",
      "You may be able to now see that the list of previous enhancements slowly\n",
      "delegates more and more responsibility to the LLM to solve more and more\n",
      "complex problems. This relies on the LLM’ s capability to gauge the\n",
      "required information needs as well as its ability to utilize multiple data\n",
      "sources. This new nature of the LLM starts to become closer and closer to\n",
      "an agent that acts on the world. The data sources can also now be abstracted\n",
      "into tools. We saw , for example, that we can search Notion, but by the same\n",
      "token, we should be able to post to Notion as well.\n",
      "Not all LLMs will have the RAG capabilities mentioned here. At the time\n",
      "of writing, likely only the lar gest managed models may be able to attempt\n",
      "this behavior . Thankfully , Cohere’ s Command R+  excels at these tasks and\n",
      "is available as an open-weights model  as well.\n",
      "RAG Evaluation\n",
      "There  are still ongoing developments in how to evaluate RAG models. A\n",
      "good paper to read on this topic is “Evaluating verifiability in generative\n",
      "search engines”  (2023), which runs human evaluations on dif ferent\n",
      "generative search systems.2\n",
      "It evaluates results along four axes:\n",
      "Fluency\n",
      "Whether the generated text is fluent and cohesive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#current retrieval not great but fast\n",
    "results = database.similarity_search(\"agentic RAG\", k=2)\n",
    "for doc in results:\n",
    "    page = doc.metadata.get(\"page\")\n",
    "    print(f\"Page: {page}\\nContent: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a602d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query={'query':\"what is agentic rag\"}\n",
    "actual_query=chain1.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1690e53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a rewritten version of the user query that can help retrieve relevant documents:\n",
      "\n",
      "**Search Query:** \"definition of agentic role\" OR \"characteristics of agentic behavior\"\n",
      "\n",
      "This rewritten query uses specific keywords and phrases to capture the essence of the original question. By using terms like \"definition\", \"role\", and \"behavior\", we can search for documents that provide a clear explanation of what an \"agentic rag\" is, or related concepts that describe its characteristics.\n"
     ]
    }
   ],
   "source": [
    "print(actual_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4be7c2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 320\n",
      "Content: Figur e 8-5. Dense r etrieval r elies on the pr operty that sear ch queries will be close to their r elevant\n",
      "results.\n",
      "Judging by the distances in Figure 8-5 , “text 2” is the best result for this\n",
      "query , followed by “text 1.” Two questions could arise here, however:\n",
      "Should text 3 even be returned as a result? That’ s a decision for\n",
      "you, the system designer . It’s sometimes desirable to have a max\n",
      "threshold of similarity score to filter out irrelevant results (in case\n",
      "the corpus has no relevant results for the query).\n",
      "Are a query and its best result semantically similar? Not always.\n",
      "This is why language models need to be trained on question-\n",
      "answer pairs to become better at retrieval. This process is\n",
      "explained in more detail in Chapter 10 .\n",
      "Figure 8-6  shows how we chunk a document before proceeding to embed\n",
      "each chunk. Those embedding vectors are then stored in the vector database\n",
      "and are ready for retrieval.\n",
      "\n",
      "Page: 212\n",
      "Content:     verbose=True\n",
      ").fit(abstracts , embeddings )\n",
      "Let us start by exploring the topics that were created. The\n",
      "get_topic_info()  method  is useful to get a quick description of the\n",
      "topics that we found:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#current retrieval not great but fast\n",
    "results = database.similarity_search(\"actual_query\", k=2)\n",
    "for doc in results:\n",
    "    page = doc.metadata.get(\"page\")\n",
    "    print(f\"Page: {page}\\nContent: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d23966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing keyword search\n",
    "# from sklearn.feature_extraction import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from tqdm import tqdm\n",
    "def tokenizer(text):\n",
    "    tokenizer_corpus=[]\n",
    "    for text in text.lower().split():\n",
    "        text=text.strip(string.punctuation)\n",
    "    \n",
    "    if len(text)>0 and text not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "        tokenizer_corpus.append(text)\n",
    "    return tokenizer_corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ef7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 598/598 [00:00<00:00, 3578.73it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_doc=[]\n",
    "for passage in tqdm(page_texts):\n",
    "    tokenized_doc.append(tokenizer(passage))\n",
    "\n",
    "bm25= BM25Okapi(tokenized_doc) #notice how naming it tokenize_doc=... will give error later \n",
    "def keyword_search(query, top_k=3, num_candidates=10):\n",
    "    print(\"input question\", query)\n",
    "\n",
    "    bm_25_scores= bm25.get_scores(tokenizer(query))\n",
    "    top_n= np.argpartition(bm_25_scores, -num_candidates)[-num_candidates:]\n",
    "    bm25_hits= [{'corpus_id': idx , 'score':bm_25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits=sorted(bm25_hits, key= lambda x: x['score'], reverse=True)\n",
    "\n",
    "    print(\"top 3 results\")\n",
    "    for hit in bm25_hits[0:top_k]:\n",
    "        print(\"\\t{:.3f}\\t\".format(hit['score']),\n",
    "        page_texts[hit['corpus_id']].replace(\"\\n\",\" \")) # fetch the content then replace line breaks with space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8014aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input question what is agentic RAG\n",
      "top 3 results\n",
      "\t5.009\t training configuration , Training Configuration quantization , Model I/O: Loading Quantized Models with LangChain , Compressing the model for (more) ef ficient training -Compressing the model for (more) ef ficient training quantization_config parameter , LoRA  Configuration quantized low-rank adaptation  (see QLoRA)  Querying Transformer (Q-Former) , BLIP-2: Bridging the Modality Gap - BLIP-2: Bridging the Modality Gap R r parameter , LoRA  Configuration RAG (retrieval-augmented generation) , Token Embeddings , Text Embeddings (for Sentences and Whole Documents) , Overview of Semantic Search and RAG , Retrieval-Augmented Generation (RAG) -RAG Evaluation agentic RAG , Agentic RAG basic pipeline , Retrieval-Augmented Generation (RAG) converting search system to , From Search to RAG evaluating results , RAG Evaluation grounded generation , Example: Grounded Generation with an LLM API with local models , Example: RAG with Local Models -The RAG prompt multi-hop RAG , Multi-hop RAG multi-query RAG , Multi-query RAG\n",
      "\t0.000\t Before you use this over a potentially lar ge dataset, it is important to always keep track of your usage. External APIs such as OpenAI’ s offering can quickly become costly if you perform many requests. At the time of writing, running our test dataset using the “gpt-3.5-turbo-0125” model costs 3 cents, which is covered by the free account, but this might change in the future. TIP When dealing with external APIs, you might run into  rate limit errors. These appear when you call the API too often as some APIs might limit the rate with which you can use it per minute or hour . To prevent these errors, we can implement several methods for retrying the request, including something referred to as exponential backoff . It performs a short sleep each time we hit a rate limit error and then retries the unsuccessful request. Whenever it is unsuccessful again, the sleep length is increased until the request is successful or we hit a maximum number of retries. To use it with OpenAI, there is a great guide  that can help you get started. Next, we can run this for all reviews in the test dataset to get its predictions. You can skip this if you want to save your (free) credits for other tasks. # You can skip this if you want to save your (free) credits predictions  = [     chatgpt_generation (prompt, doc) for doc in tqdm(data[\"test\"] [\"text\"]) ] Like the previous example, we need to convert the output from strings to integers to evaluate its performance: # Extract predictions y_pred = [int(pred) for pred in predictions ]\n",
      "\t0.000\t Figur e 5-4. Dimensionality r eduction allows data in high-dimensional space to be compr essed to a lower -dimensional r epresentation. Note that this is a compression technique and that the underlying algorithm is not arbitrarily removing dimensions. To help the cluster model create meaningful clusters, the second step in our clustering pipeline is therefore dimensionality reduction, as shown in Figure 5-5 . Figur e 5-5. Step 2: The embeddings ar e reduced to a lower -dimensional space using dimensionality reduction.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "keyword_search(\"what is agentic RAG\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d7359",
   "metadata": {},
   "source": [
    "**langchain stores the text in doc.page_content and metadata in .metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13850c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'page-number': 318, 'score': 0.8240956707105538, 'text': 'Figur e 8-3. A RAG system formulates an answer to a question and (pr eferably) cites its information\\nsources.\\nThe rest of the chapter covers these three types of systems in more detail.\\nWhile these are the major categories, they are not the only LLM\\napplications in the domain of search.\\nSemantic Search with Language Models\\nLet’s now dive into more detail on the major categories of systems that can\\nupgrade the search capabilities of our language models. We’ll start with\\ndense retrieval and then move on through reranking and RAG.\\nDense Retrieval\\nRecall that embeddings turn text into numeric representations. Those can be\\nthought of as points in space, as we can see in Figure 8-4 . Points that are\\nclose together mean that the text they represent is similar . So in this\\nexample, text 1 and text 2 are more similar to each other (because they are\\nnear each other) than text 3 (because it’ s farther away).'}, {'page-number': 356, 'score': 0.8204204760571159, 'text': 'Query routing\\nAn additional enhancement is to give the model the ability to search\\nmultiple data sources. We can, for example, specify for the model that if it\\ngets a question about HR, it should search the company’ s HR information\\nsystem (e.g., Notion) but if the question is about customer data, that it\\nshould search the customer relationship management (CRM) (e.g.,\\nSalesforce).\\nAgentic RAG\\nYou may be able to now see that the list of previous enhancements slowly\\ndelegates more and more responsibility to the LLM to solve more and more\\ncomplex problems. This relies on the LLM’ s capability to gauge the\\nrequired information needs as well as its ability to utilize multiple data\\nsources. This new nature of the LLM starts to become closer and closer to\\nan agent that acts on the world. The data sources can also now be abstracted\\ninto tools. We saw , for example, that we can search Notion, but by the same\\ntoken, we should be able to post to Notion as well.\\nNot all LLMs will have the RAG capabilities mentioned here. At the time\\nof writing, likely only the lar gest managed models may be able to attempt\\nthis behavior . Thankfully , Cohere’ s Command R+  excels at these tasks and\\nis available as an open-weights model  as well.\\nRAG Evaluation\\nThere  are still ongoing developments in how to evaluate RAG models. A\\ngood paper to read on this topic is “Evaluating verifiability in generative\\nsearch engines”  (2023), which runs human evaluations on dif ferent\\ngenerative search systems.2\\nIt evaluates results along four axes:\\nFluency\\nWhether the generated text is fluent and cohesive.'}, {'page-number': 316, 'score': 0.7958415930438966, 'text': 'Overview of Semantic Search and RAG\\nThere’ s a lot of research on how to best use language models for search.\\nThree broad categories of these models are dense retrieval, reranking, and\\nRAG. Here is an overview of these three categories that the rest of the\\nchapter will then explain in more detail:\\nDense r etrieval\\nDense  retrieval systems rely on the concept of embeddings, the same\\nconcept we’ve encountered in the previous chapters, and turn the search\\nproblem into retrieving the nearest neighbors of the search query (after\\nboth the query and the documents are converted into embeddings).\\nFigure 8-1  shows how dense retrieval takes a search query , consults its\\narchive of texts, and outputs a set of relevant results.\\nFigur e 8-1. Dense r etrieval is one of the key types of semantic sear ch, relying on the similarity\\nof text embeddings to r etrieve r elevant r esults.\\nReranking\\nSearch  systems are often pipelines of multiple steps. A reranking\\nlanguage model is one of these steps and is tasked with scoring the'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = [\n",
    "    {\n",
    "    'page-number': doc.metadata.get(\"page\"),\n",
    "    'score' : score,\n",
    "    'text': doc.page_content\n",
    "    }\n",
    "\n",
    "    for doc, score in database.similarity_search_with_relevance_scores(\"what is agentic rag\", k=3)\n",
    "]\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b49a435",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9be3d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers.weaviate_hybrid_search import WeaviateHybridSearchRetriever\n",
    "# retriever = WeaviateHybridSearchRetriever(\n",
    "#     alpha = 0.5,               # defaults to 0.5, which is equal weighting between keyword and semantic search\n",
    "#     client = query,           # keyword arguments to pass to the Weaviate client\n",
    "#     index_name = \"page_ids\",  # The name of the index to use\n",
    "#     text_key = \"page_texts\",         # The name of the text key to use\n",
    "#     attributes = [],           # The attributes to return in the results\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c0e80e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
