{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f68f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "import langchain\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "#initialize model\n",
    "lafilamod= OllamaLLM(\n",
    "    model=\"Llama3:latest\",\n",
    "    temperature=0,\n",
    "    num_ctx=2048,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccec985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88264cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Read pdf, get text\n",
    "reader = PdfReader('/Users/abhimanyu/Downloads/Hands-On_Large_Language_Models_compressed.pdf')\n",
    "page_ids = []\n",
    "page_texts = []\n",
    "for i, p in enumerate(reader.pages):\n",
    "    page_ids.append(str(i))\n",
    "    page_texts.append(p.extract_text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0090ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597']\n"
     ]
    }
   ],
   "source": [
    "print(page_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5e2a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "# # Create a MarkdownTextSplitter for semantic chunking\n",
    "# semantic_splitter = MarkdownTextSplitter(\n",
    "#     chunk_size=200,    # Number of characters per chunk\n",
    "#     chunk_overlap=50   # Overlap to preserve context\n",
    "# )\n",
    "\n",
    "# # Split the text into semantic chunks\n",
    "# semantic_chunks = semantic_splitter.split_text(page_texts)\n",
    "# print(\"\\nSemantic Chunks (Markdown-based):\\n\", semantic_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccef48a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ql/hdbyk3lx6vq0x6fs9227fc_m0000gn/T/ipykernel_50628/1651442095.py:10: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model=HuggingFaceBgeEmbeddings(model_name='thenlper/gte-small')\n",
      "/Users/abhimanyu/Desktop/geospacy/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W0804 09:57:47.185000 50628 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "/Users/abhimanyu/Desktop/geospacy/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# #now embeddings for the text\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# embed_model=SentenceTransformer(\"\")\n",
    "\n",
    "# text_embeddings=embed_model.encode(page_texts)\n",
    "\n",
    "# index_embeddings=embed_model.encode(page_ids)\n",
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "embed_model=HuggingFaceBgeEmbeddings(model_name='thenlper/gte-small')\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "metadata= [{\"page\":int(i)} for i in page_ids]\n",
    "database= FAISS.from_texts(page_texts, embed_model, metadatas=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e934d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"],\n",
    "     template=\"{page_content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a70db602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import RegexParser\n",
    "parser= RegexParser(\n",
    "    regex=r\"answer: (.*?)\\nScore:(.*)\",\n",
    "    output_keys=[\"answer\",\"score\"],\n",
    ")\n",
    "# template1=\"\"\"\n",
    "# <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "# you are a search retrieval agent that answers precisely to the query.the format needs to be like:\n",
    "# Answer: <some answer>\n",
    "# Score: <number between 0 and 1>\n",
    "# <|eot_id|>\n",
    "# <|start_header_id|>user<|end_header_id|>\n",
    "# provide a relevant answer according to the user query:\n",
    "# {context}\n",
    "# the format needs to be like:\n",
    "# Answer: <some answer>\n",
    "# Score: <number between 0 and 1>\n",
    "\n",
    "# <|eod_id|>\n",
    "# <|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# rag_prompt = PromptTemplate.from_template(\"\"\"\n",
    "# Given the following document and a question, provide a helpful answer and a score between 0 and 1 indicating how well the document answers the question.\n",
    "\n",
    "# Document:\n",
    "# {context}\n",
    "\n",
    "# Question:\n",
    "# {question}\n",
    "\n",
    "# Answer the question and rate the relevance.\n",
    "\n",
    "# Output format:\n",
    "# Answer: <your answer>\n",
    "# Score: <score between 0 and 1>\n",
    "# \"\"\")\n",
    "template1map=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an intelligent assistant helping to answer a user's question using a provided document.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "provide a relevant answer according to the user query:\n",
    "{question}.\n",
    "\n",
    "Document: {summaries}\n",
    "\n",
    "the format needs to be like:\n",
    "Answer: <some answer>\n",
    "Score: <number between 0 and 1>\n",
    "\n",
    "<|eod_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "template1combine=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a smart assistant combining multiple partial answers from different documents to answer a user’s question.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "question:{question}\n",
    "partial answers: {summaries}\n",
    "\n",
    "based on the above construct a concrete final answer for the user:\n",
    "<|eod_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "map_prompt= PromptTemplate(template=template1map, input_variables=[ \"question\",\"summaries\"])\n",
    "combine_prompt= PromptTemplate(template=template1combine, input_variables=[\"question\", \"summaries\"])\n",
    "document_variable_name=\"summaries\"\n",
    "# from langchain.chains import RetrievalQA\n",
    "# rag= RetrievalQA.from_chain_type(llm=lafilamod, chain_type=\"map_rerank\", retriever= database.as_retriever(), chain_type_kwargs={'prompt': rag_prompt},output_parser=parser, verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90a911ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA, LLMChain, MapReduceDocumentsChain, StuffDocumentsChain, ReduceDocumentsChain\n",
    "\n",
    "# rag= RetrievalQA.from_chain_type(llm=lafilamod, chain_type=\"map_reduce\", retriever= database.as_retriever(),chain_type_kwargs={'map_prompt': map_prompt,\"combine_prompt\": combine_prompt})\n",
    "\n",
    "llm_chain_map= LLMChain(llm=lafilamod, prompt= map_prompt)\n",
    "llm_chain_combine=LLMChain(llm=lafilamod, prompt=combine_prompt)\n",
    "\n",
    "combine_docschain=StuffDocumentsChain(llm_chain=llm_chain_combine,     document_prompt=document_prompt,\n",
    "document_variable_name=document_variable_name)\n",
    "\n",
    "reduce_doc_chain=ReduceDocumentsChain(combine_documents_chain=combine_docschain)\n",
    "\n",
    "map_reduce_chain= MapReduceDocumentsChain(\n",
    "    llm_chain= llm_chain_map,\n",
    "    reduce_documents_chain= reduce_doc_chain,\n",
    "    document_variable_name=document_variable_name\n",
    ")\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag= RetrievalQA( retriever= database.as_retriever(), combine_documents_chain= map_reduce_chain )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bae54fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 283\n",
      "Content: llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\" )\n",
      "''\n",
      "Unfortunately , we get no output! As we have seen in previous chapters, Phi-\n",
      "3 requires a specific prompt template. Compared to our examples with\n",
      "transformers , we will need to explicitly use a template ourselves.\n",
      "Instead of copy-pasting this template each time we use Phi-3  in LangChain,\n",
      "we can use one of LangChain’ s core functionalities, namely “chains.”\n",
      "TIP\n",
      "All examples in this chapter can be run with any LLM. This means that you can choose\n",
      "whether to use Phi-3, ChatGPT , Llama 3 or anything else when going through the\n",
      "examples. We will use Phi-3 as a default throughout, but the state-of-the-art changes\n",
      "quickly , so consider using a newer model instead. You can use  the Open LLM\n",
      "Leaderboard  (a ranking of open source LLMs) to choose whichever works best for your\n",
      "use case.\n",
      "If you do not have access to a device that can run LLMs locally , consider using\n",
      "ChatGPT  instead:\n",
      "from langchain .chat_models  import ChatOpenAI\n",
      "# Create a chat-based LLM\n",
      "chat_model  = ChatOpenAI (openai_api_key =\"MY_KEY\")\n",
      "Chains: Extending the Capabilities of LLMs\n",
      "LangChain  is named after one of its main methods, chains. Although we\n",
      "can run LLMs in isolation, their power is shown when used with additional\n",
      "components or even when used in conjunction with each other . Chains not\n",
      "only allow for extending the capabilities of LLMs but also for multiple\n",
      "chains to be connected together .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = database.similarity_search(\"where is that part where langchain is used to agent of llm\", k=1)\n",
    "for doc in results:\n",
    "    page = doc.metadata.get(\"page\")\n",
    "    print(f\"Page: {page}\\nContent: {doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1531b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is an llm agent?', 'result': 'Based on the provided partial answers, I can construct a concrete final answer for the user:\\n\\nAn LLM Agent is a type of artificial intelligence that combines the capabilities of a Large Language Model (LLM) with the ability to use various tools or APIs to solve complex problems. It interacts with the outside world using these tools, allowing it to perform specific tasks such as searching and calculating. To create an LLM agent, you need to load these tools and pass them to a ReAct (Reasoning and Acting) agent, which is responsible for executing the steps defined by the tools.'}\n"
     ]
    }
   ],
   "source": [
    "rag_response=rag.invoke(\"what is an llm agent?\")\n",
    "print(rag_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "you are a search retrieval agent that answers precisely to the query.the format needs to be like:\n",
    "Answer: <some answer>\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "provide a relevant answer according to the user query:\n",
    "{context}\n",
    "\n",
    "<|eod_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "stuff_prompt=PromptTemplate(template= template1, input_variables=[\"context\"])\n",
    "ragstuff= RetrievalQA.from_chain_type(llm=lafilamod, chain_type=\"stuff\", retriever= database.as_retriever(), chain_type_kwargs={'prompt': stuff_prompt}, verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1efdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(question):\n",
    "    response= ragstuff.invoke(question)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdfd8b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is an llm agent',\n",
       " 'result': 'Answer: Memory type Pros Cons\\n\\nScore: 0.8'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(\"what is an llm agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f90de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
