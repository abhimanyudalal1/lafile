{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f68f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "import langchain\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "\n",
    "#initialize model\n",
    "lafilamod= OllamaLLM(\n",
    "    model=\"Llama3:latest\",\n",
    "    temperature=0,\n",
    "    num_ctx=2048,\n",
    "    verbose=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccec985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d3f2d",
   "metadata": {},
   "source": [
    "**pypdf>> PyPDF2** \n",
    "\n",
    "also pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "88264cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "# Read pdf, get text\n",
    "reader = PdfReader('/Users/abhimanyu/Downloads/Hands-On_Large_Language_Models_compressed.pdf')\n",
    "page_ids = []\n",
    "page_texts = []\n",
    "for i, p in enumerate(reader.pages):\n",
    "    page_ids.append(str(i))\n",
    "    page_texts.append(p.extract_text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1c0090ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Praise for Hands-On Large Language Models\\nThis is an exceptional guide to the world of language models and their\\npractical applications in industry. Its highly-visual coverage of\\ngenerative, representational, and retrieval applications of language\\nmodels empowers readers to quickly understand, use, and refine LLMs.\\nHighly recommended!\\n—Nils Reimers, Director of Machine Learning at Cohere |\\ncreator of sentence-transformers\\nJay and Maarten have continued their tradition of providing beautifully\\nillustrated and insightful descriptions of complex topics in their new\\nbook. Bolstered with working code, timelines, and references to key\\npapers, their book is a valuable resource for anyone looking to\\nunderstand the main techniques behind how Large Language Models are\\nbuilt.\\n—Andrew Ng, founder of DeepLearning.AI\\nI can’t think of another book that is more important to read right now. On\\nevery single page, I learned something that is critical to success in this\\nera of language models.\\n—Josh Starmer, StatQuest\\nIf you’re looking to get up to speed in everything regarding LLMs, look\\nno further! In this wonderful book, Jay and Maarten will take you from\\nzero to expert in the history and latest advances in large language\\nmodels. With very intuitive explanations, great real-life examples, clear\\nillustrations, and comprehensive code labs, this book lifts the curtain on\\nthe complexities of transformer models, tokenizers, semantic search,\\nRAG, and many other cutting-edge technologies. A must read for anyone\\ninterested in the latest AI technology!\\n—Luis Serrano, PhD, Founder and CEO of Serrano\\nAcademy', 'This book is a must-read for anyone interested in the rapidly-evolving\\nfield of generative AI. With a focus on both text and visual embeddings,\\nit’s a great blend of algorithmic evolution, theoretical rigor, and practical\\nguidance. Whether you are a student, researcher, or industry\\nprofessional, this book will equip you with the use cases and solutions\\nneeded to level-up your knowledge of generative AI. Well done!\\n—Chris Fregly, Principal Solution Architect, Generative\\nAI at AWS\\nIn the heart of the GenAI revolution, this indispensable guide masterfully\\nbalances theory and practice, navigating the vast landscape of large\\nlanguage models to equip readers with the knowledge needed for\\nimmediate and transformative impact in the field of AI.\\n—Tarun Narayanan Venkatachalam, AI Researcher,\\nUniversity of Washington\\nTimely reading to get hands-on experience with language models.\\n—Emir Muñoz, Genesys\\nHands-On Large Language Models brings clarity and practical examples\\nto cut through the hype of AI. It provides a wealth of great diagrams and\\nvisual aids to supplement the clear explanations. The worked examples\\nand code make concrete what other books leave abstract. The book starts\\nwith simple introductory beginnings, and steadily builds in scope. By the\\nfinal chapters, you will be fine-tuning and building your own large\\nlanguage models with confidence.\\n—Leland McInnes, Researcher at the Tutte Institute for\\nMathematics and Computing\\nFinally, a book that not only avoids superficial coverage of large\\nlanguage models but also thoroughly explores the background in a way\\nthat is both accessible and engaging. The authors have masterfully\\ncreated a definitive guide that will remain essential reading despite the\\nfast-paced advancements in the field.\\n—Prof. DDr. Roman Egger, CEO of Smartvisions.at and\\nModul University Vienna', 'OceanofPDF.com', 'Hands-On Large Language\\nModels\\nLanguage Understanding and Generation\\nJay Alammar and Maarten Grootendorst\\nOceanofPDF.com', 'Hands-On Large Language Models\\nby Jay Alammar and Maarten Grootendorst\\nCopyright © 2024 Jay Alammar and Maarten Pieter Grootendorst. All rights\\nreserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North,\\nSebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales\\npromotional use. Online editions are also available for most titles\\n(http://oreilly.com). For more information, contact our\\ncorporate/institutional sales department: 800-998-9938 or\\ncorporate@oreilly.com.\\nAcquisitions Editor: Nicole Butterfield\\nDevelopment Editor: Michele Cronin\\nProduction Editor: Ashley Stussy\\nCopyeditor: Charles Roumeliotis\\nProofreader: Kim Cofer\\nIndexer: BIM Creatives, LLC\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\nSeptember 2024: First Edition', 'Revision History for the First Edition\\n2024-09-10: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098150969 for release\\ndetails.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-\\nOn Large Language Models, the cover image, and related trade dress are\\ntrademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the authors and do not\\nrepresent the publisher’s views. While the publisher and the authors have\\nused good faith efforts to ensure that the information and instructions\\ncontained in this work are accurate, the publisher and the authors disclaim\\nall responsibility for errors or omissions, including without limitation\\nresponsibility for damages resulting from the use of or reliance on this\\nwork. Use of the information and instructions contained in this work is at\\nyour own risk. If any code samples or other technology this work contains\\nor describes is subject to open source licenses or the intellectual property\\nrights of others, it is your responsibility to ensure that your use thereof\\ncomplies with such licenses and/or rights.\\n978-1-098-15096-9\\n[LSI]\\nOceanofPDF.com', 'Preface\\nLarge language models (LLMs) have had a profound and far-reaching\\nimpact on the world. By enabling machines to better understand and\\ngenerate human-like language, LLMs have opened new possibilities in the\\nfield of AI and impacted entire industries.\\nThis book provides a comprehensive and highly visual introduction to the\\nworld of LLMs, covering both the conceptual foundations and practical\\napplications. From word representations that preceded deep learning to the\\ncutting-edge (at the time of this writing) Transformer architecture, we will\\nexplore the history and evolution of LLMs. We delve into the inner\\nworkings of LLMs, exploring their architectures, training methods, and\\nfine-tuning techniques. We also examine various applications of LLMs in\\ntext classification, clustering, topic modeling, chatbots, search engines, and\\nmore.\\nWith its unique blend of intuition-building, applications, and illustrative\\nstyle, we hope that this book provides the ideal foundation for those looking\\nto explore the exciting world of LLMs. Whether you are a beginner or an\\nexpert, we invite you to join us on this journey to start building with LLMs.\\nAn Intuition-First Philosophy\\nThe main goal of this book is to provide an intuition into the field of LLMs.\\nThe pace of development in the Language AI field is incredibly fast and\\nfrustration can build trying to keep up with the latest technologies. Instead,\\nwe focus on the fundamentals of LLMs and intend to provide a fun and\\neasy learning process.\\nTo achieve this intuition-first philosophy we liberally make use of visual\\nlanguage. Illustrations will help give a visual identity to major concepts and\\nprocesses involved in the learning process of LLMs.1  With our illustrative', 'method of storytelling, we want to take you on a journey to this exciting\\nand potentially world-changing field.\\nThroughout the book, we make a clear distinction between representation\\nand generative language models. Representation models are LLMs that do\\nnot generate text but are commonly used for task-specific use cases, like\\nclassification, whereas generation models are LLMs that generate text, like\\nGPT models. Although generative models are typically the first thing that\\ncomes to mind when thinking about LLMs, there is still much use for\\nrepresentation models. We are also loosely using the word “large” in large\\nlanguage models and often elect to simply call them language models as\\nsize descriptions are often rather arbitrary and not always indicative of\\ncapability.\\nPrerequisites\\nThis book assumes that you have some experience programming in Python\\nand are familiar with the fundamentals of machine learning. The focus will\\nbe on building a strong intuition rather than deriving mathematical\\nequations. As such, illustrations combined with hands-on examples will\\ndrive the examples and learning through this book. This book assumes no\\nprior knowledge of popular deep learning frameworks such as PyTorch or\\nTensorFlow nor any prior knowledge of generative modeling.\\nIf you are not familiar with Python, a great place to start is Learn Python,\\nwhere you will find many tutorials on the basics of the language. To further\\nease the learning process, we made all the code available on Google Colab,\\na platform where you can run all of the code without the need to install\\nanything locally.', 'Book Structure\\nThe book is broadly divided into three parts. They are illustrated in\\nFigure P-1 to give you a full view of the book. Note that each chapter can\\nbe read independently, so feel free to skim chapters you are already familiar\\nwith.', 'Part I: Understanding Language Models\\nIn Part I of the book, we explore the inner workings of language models\\nboth small and large. We start with an overview of the field and common\\ntechniques (see Chapter 1) before moving over to two central components\\nof these models, tokenization and embeddings (see Chapter 2). We finish\\nthis part of the book with an updated and expanded version of Jay’s well-\\nknown Illustrated Transformer, which dives into the architecture of these\\nmodels (see Chapter 3). Many terms and definitions will be introduced that\\nare used throughout the book.\\nFigure P-1. All parts and chapters of the book.', '', 'Part II: Using Pretrained Language Models\\nIn Part II of the book, we explore how LLMs can be used through common\\nuse cases. We use pretrained models and demonstrate their capabilities\\nwithout the need to fine-tune them.\\nYou learn how to use language models for supervised classification (see\\nChapter 4), text clustering and topic modeling (see Chapter 5), leveraging\\nembedding models for semantic search (see Chapter 6), generating text (see\\nChapters 7 and 8), and extending the capabilities of text generation to the\\nvisual domain (see Chapter 9).\\nLearning these individual language model capabilities will equip you with\\nthe skill set to problem-solve with LLMs and build more and more\\nadvanced systems and pipelines.', 'Part III: Training and Fine-Tuning Language Models\\nIn Part III of the book, we explore advanced concepts through training and\\nfine-tuning all kinds of language models. We will explore how to create and\\nfine-tune an embedding model (see Chapter 10), review how to fine-tune\\nBERT for classification (see Chapter 11), and end the book with several\\nmethods for fine-tuning generation models (see Chapter 12).\\nHardware and Software Requirements\\nRunning generative models is generally a compute-intensive task that\\nrequires a computer with a strong GPU. Since those are not available to\\nevery reader, all examples in this book are made to run using an online\\nplatform, namely Google Colaboratory, often shortened to “Google Colab.”\\nAt the time of writing, this platform allows you to use an NVIDIA GPU\\n(T4) for free to run your code. This GPU has 16 GB of VRAM (which is\\nthe memory of your GPU), which is the minimum amount of VRAM we\\nexpect for the examples throughout the book.\\nNOTE\\nNot all chapters require a minimum of 16 GB VRAM as some examples, like training\\nand fine-tuning, are more compute-intensive than others, such as prompt engineering. In\\nthe repository, you will find the minimum GPU requirements for each chapter.\\nAll code, requirements, and additional tutorials are available in this book’s\\nrepository. If you want to run the examples locally, we recommend access\\nto an NVIDIA GPU with a minimum of 16 GB of VRAM. For a local\\ninstallation, for example with conda, you can follow this setup to create\\nyour environment:\\nconda create -n thellmbook python=3.10', 'conda activate thellmbook\\nYou can install all the necessary dependencies by forking or cloning the\\nrepository and then running the following in your newly created Python\\n3.10 environment:\\npip install -r requirements.txt\\nAPI Keys\\nWe use both open source and proprietary models throughout the examples\\nto demonstrate the advantages and disadvantages of both. For the\\nproprietary models, using OpenAI and Cohere’s offering, you will need to\\ncreate a free account:\\nOpenAI\\nClick “sign up” on the site to create a free account. This account allows\\nyou to create an API key, which can be used to access GPT-3.5. Then,\\ngo to “API keys” to create a secret key.\\nCohere\\nRegister a free account on the website. Then, go to “API keys” to create\\na secret key.\\nNote that with both accounts, rate limits apply and that these free API keys\\nonly allow for a limited number of calls per minute. Throughout all\\nexamples, we have taken that into account and provided local alternatives if\\nnecessary.\\nFor the open source models, you do not need to create an account with the\\nexception of the Llama 2 model in Chapter 2. To use that model, you will\\nneed a Hugging Face account:', 'Hugging Face\\nClick “sign up” on the Hugging Face website to create a free account.\\nThen, in “Settings” go to “Access Tokens” to create a token that you can\\nuse to download certain LLMs.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file\\nextensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to\\nprogram elements such as variable or function names, databases, data\\ntypes, environment variables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by\\nvalues determined by context.\\nTIP\\nThis element signifies a tip or suggestion.', 'NOTE\\nThis element signifies a general note.\\nUsing Code Examples\\nSupplemental material (code examples, exercises, etc.) is available for\\ndownload at https://github.com/HandsOnLLM/Hands-On-Large-Language-\\nModels.\\nIf you have a technical question or a problem using the code examples,\\nplease send email to support@oreilly.com.\\nThis book is here to help you get your job done. In general, if example code\\nis offered with this book, you may use it in your programs and\\ndocumentation. You do not need to contact us for permission unless you’re\\nreproducing a significant portion of the code. For example, writing a\\nprogram that uses several chunks of code from this book does not require\\npermission. Selling or distributing examples from O’Reilly books does\\nrequire permission. Answering a question by citing this book and quoting\\nexample code does not require permission. Incorporating a significant\\namount of example code from this book into your product’s documentation\\ndoes require permission.\\nWe appreciate, but generally do not require, attribution. An attribution\\nusually includes the title, author, publisher, and ISBN. For example:\\n“Hands-On Large Language Models by Jay Alammar and Maarten\\nGrootendorst (O’Reilly). Copyright 2024 Jay Alammar and Maarten Pieter\\nGrootendorst, 978-1-098-15096-9.”\\nIf you feel your use of code examples falls outside fair use or the\\npermission given above, feel free to contact us at permissions@oreilly.com.', 'O’Reilly Online Learning\\nNOTE\\nFor more than 40 years, O’Reilly Media has provided technology and business training,\\nknowledge, and insight to help companies succeed.\\nOur unique network of experts and innovators share their knowledge and\\nexpertise through books, articles, and our online learning platform.\\nO’Reilly’s online learning platform gives you on-demand access to live\\ntraining courses, in-depth learning paths, interactive coding environments,\\nand a vast collection of text and video from O’Reilly and 200+ other\\npublishers. For more information, visit https://oreilly.com.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the\\npublisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-889-8969 (in the United States or Canada)\\n707-827-7019 (international or local)\\n707-829-0104 (fax)\\nsupport@oreilly.com', 'https://www.oreilly.com/about/contact.html\\nWe have a web page for this book, where we list errata, examples, and any\\nadditional information. You can access this page at\\nhttps://oreil.ly/hands_on_LLMs_1e.\\nFor news and information about our books and courses, visit\\nhttps://oreilly.com.\\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media.\\nWatch us on YouTube: https://youtube.com/oreillymedia.\\nAcknowledgments\\nWriting this book has been an incredible experience, collaboration, and\\njourney for us.\\nThe field of (large) language models is one of the most dynamic areas in\\ntechnology today, and within the span of writing this book, we have\\nwitnessed extraordinary advancements. Yet, despite the rapid pace of\\nchange, the fundamental principles remain strikingly consistent which made\\nthe writing process particularly intriguing. We are grateful to have had the\\nopportunity to explore this field in-depth at such a pivotal moment.\\nWorking with our O’Reilly team was incredible! Special thanks to Michele\\nCronin for her amazing feedback, support, and enthusiasm for this book\\nfrom day one. We could not have asked for a better editor—you are\\namazing! Thank you, Nicole Butterfield, for kicking off this book and\\nhelping us maintain a structured approach throughout the writing. Thank\\nyou to Karen Montgomery for creating our wonderful cover, we love the\\nkangaroo! Big thanks to Kate Dullea for being so patient with us having to\\ngo through hundreds of illustrations many times over. The timely early\\nreleases by Clare Laylock helped us see our work grow which was a big\\nmotivator, thank you. Thanks to Ashley Stussy and Charles Roumeliotis for\\nthe development in the final stages of the book and everyone else at\\nO’Reilly who contributed.', 'Thanks to our amazing crew of technical reviewers. Invaluable feedback\\nwas given by Harm Buisman, Emir Muñoz, Luba Elliott, Guarav Chawla,\\nRafael V. Pierre, Luba Elliott, Tarun Narayanan, Nikhil Buduma, and\\nPatrick Harrison.\\nJay\\nI’d love to extend my deepest gratitude to my family for their unwavering\\nsupport and inspiration. I would like to specifically acknowledge my\\nparents, Abdullah and Mishael, and my aunts, Hussah and Aljoharah.\\nI’m grateful to the friends, colleagues, and collaborators who helped me\\nunderstand and explain the tricky concepts covered in this book as well as\\nto the Cohere folks who cultivate a supporting learning and sharing\\nenvironment. Thank you to Adrien Morisot, Aidan Gomez, Andy Toulis,\\nAnfal Alatawi, Arash Ahmadian, Bharat Venkitesh, Edward Grefenstette,\\nIvan Zhang, Joao Araújo, Luis Serrano, Matthias Gallé, Meor Amer, Nick\\nFrosst, Patrick Lewis, Phil Blunsom, Sara Hooker, and Suhas Pai.\\nI couldn’t conceive of this project getting accomplished to the level it has\\nwithout the extraordinary talent and tireless effort of Maarten, my coauthor.\\nYour ability to repeatedly nail the technical details (from the pinned version\\nof the nth import dependency to the latest in LLM quantization) while\\nweaving some of the world’s best visual narratives is absolutely\\nbreathtaking.\\nLastly, a tip of the hat to the incredible coffee shop scene of Riyadh, Saudi\\nArabia for supplying me with caffeine and a good place to focus from dawn\\nuntil midnight. It’s where I read most of these papers and worked out my\\nunderstanding (looking at you, Elixir Bunn).\\nMaarten\\nI want to begin by expressing my heartfelt appreciation to my coauthor, Jay.\\nYour insights have made this not only possible but incredibly fulfilling.\\nThis journey has been nothing short of amazing and collaborating with you\\nhas been an absolute joy.\\nI want to sincerely thank my wonderful colleagues at IKNL for their\\ncontinued support throughout this journey. A special mention goes to Harm', '—our Monday morning coffee breaks discussing this book were a constant\\nsource of encouragement.\\nThank you to my family and friends for their unwavering support, and to\\nmy parents in particular. Pap, despite the challenges you faced, you always\\nfound a way to be there for me when I needed it most, thank you. Mam, the\\nconversations we had as aspiring writers were wonderful and motivated me\\nmore than you could ever imagine. Thank you both for your endless support\\nand encouragement.\\nFinally, I am at a loss for words to adequately express my gratitude to my\\nwonderful wife, Ilse. Lieverd, your boundless enthusiasm and patience have\\nbeen legendary, especially when I droned on about the latest LLM\\ndevelopments for hours on end. You are my greatest support. My apologies\\nto my amazing daughter, Sarah. At just two years old, you already have\\nlistened to more about large language models than anyone should have to\\nendure in a lifetime! I promise we’ll make up for it with endless playtime\\nand adventures together.\\n1  J. Alammar. “Machine learning research communication via illustrated and interactive web\\narticles.” Beyond Static Papers: Rethinking How We Share Scientific Understanding in ML.\\nICLR 2021 Workshop (2021).\\nOceanofPDF.com', 'Part I. Understanding Language\\nModels\\nOceanofPDF.com', 'Chapter 1. An Introduction to\\nLarge Language Models\\nHumanity is at an inflection point. From 2012 onwards, developments in\\nbuilding AI systems (using deep neural networks) accelerated so that by the\\nend of the decade, they yielded the first software system able to write\\narticles indiscernible from those written by humans. This system was an AI\\nmodel called Generative Pre-trained Transformer 2, or GPT-2. 2022 marked\\nthe release of ChatGPT, which demonstrated how profoundly this\\ntechnology was poised to revolutionize how we interact with technology\\nand information. Reaching one million active users in five days and then\\none hundred million active users in two months, the new breed of AI\\nmodels started out as human-like chatbots but quickly evolved into a\\nmonumental shift in our approach to common tasks, like translation, text\\ngeneration, summarization, and more. It became an invaluable tool for\\nprogrammers, educators, and researchers.\\nThe success of ChatGPT was unprecedented and popularized more research\\ninto the technology behind it, namely large language models (LLMs). Both\\nproprietary and public models were being released at a steady pace, closing\\nin on, and eventually catching up to the performance of ChatGPT. It is not\\nan exaggeration to state that almost all attention was on LLMs.\\nAs a result, 2023 will always be known, at least to us, as the year that\\ndrastically changed our field, Language Artificial Intelligence (Language\\nAI), a field characterized by the development of systems capable of\\nunderstanding and generating human language.\\nHowever, LLMs have been around for a while now and smaller models are\\nstill relevant to this day. LLMs are much more than just a single model and\\nthere are many other techniques and models in the field of language AI that\\nare worth exploring.', 'In this book, we aim to give readers a solid understanding of the\\nfundamentals of both LLMs and the field of Language AI in general. This\\nchapter serves as the scaffolding for the rest of the book and will introduce\\nconcepts and terms that we will use throughout the chapters.\\nBut mostly, we intend to answer the following questions in this chapter:\\nWhat is Language AI?\\nWhat are large language models?\\nWhat are the common use cases and applications of large language\\nmodels?\\nHow can we use large language models ourselves?\\nWhat Is Language AI?\\nThe term artificial intelligence (AI) is often used to describe computer\\nsystems dedicated to performing tasks close to human intelligence, such as\\nspeech recognition, language translation, and visual perception. It is the\\nintelligence of software as opposed to the intelligence of humans.\\nHere is a more formal definition by one of the founders of the artificial\\nintelligence discipline:\\n[Artificial intelligence is] the science and engineering of making\\nintelligent machines, especially intelligent computer programs. It is\\nrelated to the similar task of using computers to understand human\\nintelligence, but AI does not have to confine itself to methods that are\\nbiologically observable.\\n—John McCarthy, 20071 \\nDue to the ever-evolving nature of AI, the term has been used to describe a\\nwide variety of systems, some of which might not truly embody intelligent\\nbehavior. For instance, characters in computer games (NPCs [nonplayable\\ncharacters]) have often been referred to as AI even though many are nothing\\nmore than if-else statements.', 'Language AI refers to a subfield of AI that focuses on developing\\ntechnologies capable of understanding, processing, and generating human\\nlanguage. The term Language AI can often be used interchangeably with\\nnatural language processing (NLP) with the continued success of machine\\nlearning methods in tackling language processing problems.\\nWe use the term Language AI to encompass technologies that technically\\nmight not be LLMs but still have a significant impact on the field, like how\\nretrieval systems can give LLMs superpowers (see Chapter 8).\\nThroughout this book, we want to focus on the models that have had a\\nmajor role in shaping the field of Language AI. This means exploring more\\nthan just LLMs in isolation. That, however, brings us to the question: what\\nare large language models? To begin answering this question in this chapter,\\nlet’s first explore the history of Language AI.\\nA Recent History of Language AI\\nThe history of Language AI encompasses many developments and models\\naiming to represent and generate language, as illustrated in Figure 1-1.\\nFigure 1-1. A peek into the history of Language AI.\\nLanguage, however, is a tricky concept for computers. Text is unstructured\\nin nature and loses its meaning when represented by zeros and ones\\n(individual characters). As a result, throughout the history of Language AI,', 'there has been a large focus on representing language in a structured\\nmanner so that it can more easily be used by computers. Examples of these\\nLanguage AI tasks are provided in Figure 1-2.\\nFigure 1-2. Language AI is capable of many tasks by processing textual input.\\nRepresenting Language as a Bag-of-Words\\nOur history of Language AI starts with a technique called bag-of-words, a\\nmethod for representing unstructured text.2  It was first mentioned around\\nthe 1950s but became popular around the 2000s.\\nBag-of-words works as follows: let’s assume that we have two sentences for\\nwhich we want to create numerical representations. The first step of the\\nbag-of-words model is tokenization, the process of splitting up the\\nsentences into individual words or subwords (tokens), as illustrated in\\nFigure 1-3.', 'Figure 1-3. Each sentence is split into words (tokens) by splitting on a whitespace.\\nThe most common method for tokenization is by splitting on a whitespace\\nto create individual words. However, this has its disadvantages as some\\nlanguages, like Mandarin, do not have whitespaces around individual\\nwords. In the next chapter, we will go in depth about tokenization and how\\nthat technique influences language models. As illustrated in Figure 1-4,\\nafter tokenization, we combine all unique words from each sentence to\\ncreate a vocabulary that we can use to represent the sentences.\\nFigure 1-4. A vocabulary is created by retaining all unique words across both sentences.\\nUsing our vocabulary, we simply count how often a word in each sentence\\nappears, quite literally creating a bag of words. As a result, a bag-of-words\\nmodel aims to create representations of text in the form of numbers, also', 'called vectors or vector representations, observed in Figure 1-5. Throughout\\nthe book, we refer to these kinds of models as representation models.\\nFigure 1-5. A bag-of-words is created by counting individual words. These values are referred to as\\nvector representations.\\nAlthough bag-of-words is a classic method, it is by no means completely\\nobsolete. In Chapter 5, we will explore how it can still be used to\\ncomplement more recent language models.\\nBetter Representations with Dense Vector Embeddings\\nBag-of-words, although an elegant approach, has a flaw. It considers\\nlanguage to be nothing more than an almost literal bag of words and ignores\\nthe semantic nature, or meaning, of text.\\nReleased in 2013, word2vec was one of the first successful attempts at\\ncapturing the meaning of text in embeddings.3  Embeddings are vector\\nrepresentations of data that attempt to capture its meaning. To do so,\\nword2vec learns semantic representations of words by training on vast\\namounts of textual data, like the entirety of Wikipedia.\\nTo generate these semantic representations, word2vec leverages neural\\nnetworks. These networks consist of interconnected layers of nodes that\\nprocess information. As illustrated in Figure 1-6, neural networks can have', 'many layers where each connection has a certain weight depending on the\\ninput. These weights are often referred to as the parameters of the model.\\nFigure 1-6. A neural network consists of interconnected layers of nodes where each connection is a\\nlinear equation.\\nUsing these neural networks, word2vec generates word embeddings by\\nlooking at which other words they tend to appear next to in a given\\nsentence. We start by assigning every word in our vocabulary with a vector\\nembedding, say of 50 values for each word initialized with random values.\\nThen in every training step, as illustrated in Figure 1-7, we take pairs of\\nwords from the training data and a model attempts to predict whether or not\\nthey are likely to be neighbors in a sentence.\\nDuring this training process, word2vec learns the relationship between\\nwords and distills that information into the embedding. If the two words\\ntend to have the same neighbors, their embeddings will be closer to one\\nanother and vice versa. In Chapter 2, we will look closer at word2vec’s\\ntraining procedure.', 'Figure 1-7. A neural network is trained to predict if two words are neighbors. During this process,\\nthe embeddings are updated to be in line with the ground truth.\\nThe resulting embeddings capture the meaning of words but what exactly\\ndoes that mean? To illustrate this phenomenon, let’s somewhat oversimplify\\nand imagine we have embeddings of several words, namely “apple” and\\n“baby.” Embeddings attempt to capture meaning by representing the\\nproperties of words. For instance, the word “baby” might score high on the\\nproperties “newborn” and “human” while the word “apple” scores low on\\nthese properties.\\nAs illustrated in Figure 1-8, embeddings can have many properties to\\nrepresent the meaning of a word. Since the size of embeddings is fixed,\\ntheir properties are chosen to create a mental representation of the word.\\nFigure 1-8. The values of embeddings represent properties that are used to represent words. We may\\noversimplify by imagining that dimensions represent concepts (which they don’t), but it helps express\\nthe idea.\\nIn practice, these properties are often quite obscure and seldom relate to a\\nsingle entity or humanly identifiable concept. However, together, these', 'properties make sense to a computer and serve as a good way to translate\\nhuman language into computer language.\\nEmbeddings are tremendously helpful as they allow us to measure the\\nsemantic similarity between two words. Using various distance metrics, we\\ncan judge how close one word is to another. As illustrated in Figure 1-9, if\\nwe were to compress these embeddings into a two-dimensional\\nrepresentation, you would notice that words with similar meaning tend to be\\ncloser. In Chapter 5, we will explore how to compress these embeddings\\ninto n-dimensional space.\\nFigure 1-9. Embeddings of words that are similar will be close to each other in dimensional space.\\nTypes of Embeddings\\nThere are many types of embeddings, like word embeddings and sentence\\nembeddings that are used to indicate different levels of abstractions (word\\nversus sentence), as illustrated in Figure 1-10.\\nBag-of-words, for instance, creates embeddings at a document level since it\\nrepresents the entire document. In contrast, word2vec generates\\nembeddings for words only.', 'Throughout the book, embeddings will take on a central role as they are\\nutilized in many use cases, such as classification (see Chapter 4), clustering\\n(see Chapter 5), and semantic search and retrieval-augmented generation\\n(see Chapter 8). In Chapter 2, we will take our first deep dive into token\\nembeddings.\\nFigure 1-10. Embeddings can be created for different types of input.\\nEncoding and Decoding Context with Attention\\nThe training process of word2vec creates static, downloadable\\nrepresentations of words. For instance, the word “bank” will always have\\nthe same embedding regardless of the context in which it is used. However,\\n“bank” can refer to both a financial bank as well as the bank of a river. Its\\nmeaning, and therefore its embeddings, should change depending on the\\ncontext.\\nA step in encoding this text was achieved through recurrent neural networks\\n(RNNs). These are variants of neural networks that can model sequences as', 'an additional input.\\nTo do so, these RNNs are used for two tasks, encoding or representing an\\ninput sentence and decoding or generating an output sentence. Figure 1-11\\nillustrates this concept by showing how a sentence like “I love llamas” gets\\ntranslated to the Dutch “Ik hou van lama’s.”\\nFigure 1-11. Two recurrent neural networks (decoder and encoder) translating an input sequence\\nfrom English to Dutch.\\nEach step in this architecture is autoregressive. When generating the next\\nword, this architecture needs to consume all previously generated words, as\\nshown in Figure 1-12.', 'Figure 1-12. Each previous output token is used as input to generate the next token.\\nThe encoding step aims to represent the input as well as possible,\\ngenerating the context in the form of an embedding, which serves as the\\ninput for the decoder. To generate this representation, it takes embeddings\\nas its inputs for words, which means we can use word2vec for the initial\\nrepresentations. In Figure 1-13, we can observe this process. Note how the\\ninputs are processed sequentially, one at a time, as well as the output.', 'Figure 1-13. Using word2vec embeddings, a context embedding is generated that represents the\\nentire sequence.\\nThis context embedding, however, makes it difficult to deal with longer\\nsentences since it is merely a single embedding representing the entire\\ninput. In 2014, a solution called attention was introduced that highly\\nimproved upon the original architecture.4  Attention allows a model to focus\\non parts of the input sequence that are relevant to one another (“attend” to\\neach other) and amplify their signal, as shown in Figure 1-14. Attention\\nselectively determines which words are most important in a given sentence.\\nFor instance, the output word “lama’s” is Dutch for “llamas,” which is why\\nthe attention between both is high. Similarly, the words “lama’s” and “I”\\nhave lower attention since they aren’t as related. In Chapter 3, we will go\\nmore in depth on the attention mechanism.', 'Figure 1-14. Attention allows a model to “attend” to certain parts of sequences that might relate\\nmore or less to one another.\\nBy adding these attention mechanisms to the decoder step, the RNN can\\ngenerate signals for each input word in the sequence related to the potential\\noutput. Instead of passing only a context embedding to the decoder, the\\nhidden states of all input words are passed. This process is demonstrated in\\nFigure 1-15.\\nFigure 1-15. After generating the words “Ik,” “hou,” and “van,” the attention mechanism of the\\ndecoder enables it to focus on the word “llamas” before it generates the Dutch translation\\n(“lama’s”).', 'As a result, during the generation of “Ik hou van lama’s,” the RNN keeps\\ntrack of the words it mostly attends to perform the translation. Compared to\\nword2vec, this architecture allows for representing the sequential nature of\\ntext and the context in which it appears by “attending” to the entire\\nsentence. This sequential nature, however, precludes parallelization during\\ntraining of the model.\\nAttention Is All You Need\\nThe true power of attention, and what drives the amazing abilities of large\\nlanguage models, was first explored in the well-known “Attention is all you\\nneed” paper released in 2017.5  The authors proposed a network architecture\\ncalled the Transformer, which was solely based on the attention mechanism\\nand removed the recurrence network that we saw previously. Compared to\\nthe recurrence network, the Transformer could be trained in parallel, which\\ntremendously sped up training.\\nIn the Transformer, encoding and decoder components are stacked on top of\\neach other, as illustrated in Figure 1-16. This architecture remains\\nautoregressive, needing to consume each generated word before creating a\\nnew word.', 'Figure 1-16. The Transformer is a combination of stacked encoder and decoder blocks where the\\ninput flows through each encoder and decoder.\\nNow, both the encoder and decoder blocks would revolve around attention\\ninstead of leveraging an RNN with attention features. The encoder block in\\nthe Transformer consists of two parts, self-attention and a feedforward\\nneural network, which are shown in Figure 1-17.', 'Figure 1-17. An encoder block revolves around self-attention to generate intermediate\\nrepresentations.\\nCompared to previous methods of attention, self-attention can attend to\\ndifferent positions within a single sequence, thereby more easily and\\naccurately representing the input sequence as illustrated in Figure 1-18.\\nInstead of processing one token at a time, it can be used to look at the entire\\nsequence in one go.', 'Figure 1-18. Self-attention attends to all parts of the input sequence so that it can “look” both\\nforward and back in a single sequence.\\nCompared to the encoder, the decoder has an additional layer that pays\\nattention to the output of the encoder (to find the relevant parts of the\\ninput). As demonstrated in Figure 1-19, this process is similar to the RNN\\nattention decoder that we discussed previously.', 'Figure 1-19. The decoder has an additional attention layer that attends to the output of the encoder.\\nAs shown in Figure 1-20, the self-attention layer in the decoder masks\\nfuture positions so it only attends to earlier positions to prevent leaking\\ninformation when generating the output.\\nFigure 1-20. Only attend to previous tokens to prevent “looking into the future.”', 'Together, these building blocks create the Transformer architecture and are\\nthe foundation of many impactful models in Language AI, such as BERT\\nand GPT-1, which we cover later in this chapter. Throughout this book,\\nmost models that we will use are Transformer-based models.\\nThere is much more to the Transformer architecture than what we explored\\nthus far. In Chapters 2 and 3, we will go through the many reasons why\\nTransformer models work so well, including multi-head attention,\\npositional embeddings, and layer normalization.\\nRepresentation Models: Encoder-Only Models\\nThe original Transformer model is an encoder-decoder architecture that\\nserves translation tasks well but cannot easily be used for other tasks, like\\ntext classification.\\nIn 2018, a new architecture called Bidirectional Encoder Representations\\nfrom Transformers (BERT) was introduced that could be leveraged for a\\nwide variety of tasks and would serve as the foundation of Language AI for\\nyears to come.6  BERT is an encoder-only architecture that focuses on\\nrepresenting language, as illustrated in Figure 1-21. This means that it only\\nuses the encoder and removes the decoder entirely.', 'Figure 1-21. The architecture of a BERT base model with 12 encoders.\\nThese encoder blocks are the same as we saw before: self-attention\\nfollowed by feedforward neural networks. The input contains an additional\\ntoken, the [CLS] or classification token, which is used as the\\nrepresentation for the entire input. Often, we use this [CLS] token as the\\ninput embedding for fine-tuning the model on specific tasks, like\\nclassification.\\nTraining these encoder stacks can be a difficult task that BERT approaches\\nby adopting a technique called masked language modeling (see Chapters 2\\nand 11). As shown in Figure 1-22, this method masks a part of the input for\\nthe model to predict. This prediction task is difficult but allows BERT to\\ncreate more accurate (intermediate) representations of the input.', 'Figure 1-22. Train a BERT model by using masked language modeling.\\nThis architecture and training procedure makes BERT and related\\narchitectures incredible at representing contextual language. BERT-like\\nmodels are commonly used for transfer learning, which involves first\\npretraining it for language modeling and then fine-tuning it for a specific\\ntask. For instance, by training BERT on the entirety of Wikipedia, it learns\\nto understand the semantic and contextual nature of text. Then, as shown in\\nFigure 1-23, we can use that pretrained model to fine-tune it for a specific\\ntask, like text classification.\\nFigure 1-23. After pretraining BERT on masked language model, we fine-tune it for specific tasks.\\nA huge benefit of pretrained models is that most of the training is already\\ndone for us. Fine-tuning on specific tasks is generally less compute-\\nintensive and requires less data. Moreover, BERT-like models generate\\nembeddings at almost every step in their architecture. This also makes', 'BERT models feature extraction machines without the need to fine-tune\\nthem on a specific task.\\nEncoder-only models, like BERT, will be used in many parts of the book.\\nFor years, they have been and are still used for common tasks, including\\nclassification tasks (see Chapter 4), clustering tasks (see Chapter 5), and\\nsemantic search (see Chapter 8).\\nThroughout the book, we will refer to encoder-only models as\\nrepresentation models to differentiate them from decoder-only, which we\\nrefer to as generative models. Note that the main distinction does not lie\\nbetween the underlying architecture and the way these models work.\\nRepresentation models mainly focus on representing language, for instance,\\nby creating embeddings, and typically do not generate text. In contrast,\\ngenerative models focus primarily on generating text and typically are not\\ntrained to generate embeddings.\\nThe distinction between representation and generative models and\\ncomponents will also be shown in most images. Representation models are\\nteal with a small vector icon (to indicate its focus on vectors and\\nembeddings) whilst generative models are pink with a small chat icon (to\\nindicate its generative capabilities).\\nGenerative Models: Decoder-Only Models\\nSimilar to the encoder-only architecture of BERT, a decoder-only\\narchitecture was proposed in 2018 to target generative tasks.7  This\\narchitecture was called a Generative Pre-trained Transformer (GPT) for its\\ngenerative capabilities (it’s now known as GPT-1 to distinguish it from later\\nversions). As shown in Figure 1-24, it stacks decoder blocks similar to the\\nencoder-stacked architecture of BERT.\\nGPT-1 was trained on a corpus of 7,000 books and Common Crawl, a large\\ndataset of web pages. The resulting model consisted of 117 million\\nparameters. Each parameter is a numerical value that represents the model’s\\nunderstanding of language.', 'If everything remains the same, we expect more parameters to greatly\\ninfluence the capabilities and performance of language models. Keeping\\nthis in mind, we saw larger and larger models being released at a steady\\npace. As illustrated in Figure 1-25, GPT-2 had 1.5 billion parameters8  and\\nGPT-3 used 175 billion parameters9  quickly followed.\\nFigure 1-24. The architecture of a GPT-1. It uses a decoder-only architecture and removes the\\nencoder-attention block.', 'Figure 1-25. GPT models quickly grew in size with each iteration.\\nThese generative decoder-only models, especially the “larger” models, are\\ncommonly referred to as large language models (LLMs). As we will discuss\\nlater in this chapter, the term LLM is not only reserved for generative\\nmodels (decoder-only) but also representation models (encoder-only).\\nGenerative LLMs, as sequence-to-sequence machines, take in some text and\\nattempt to autocomplete it. Although a handy feature, their true power\\nshone from being trained as a chatbot. Instead of completing a text, what if\\nthey could be trained to answer questions? By fine-tuning these models, we\\ncan create instruct or chat models that can follow directions.\\nAs illustrated in Figure 1-26, the resulting model could take in a user query\\n(prompt) and output a response that would most likely follow that prompt.\\nAs such, you will often hear that generative models are completion models.', 'Figure 1-26. Generative LLMs take in some input and try to complete it. With instruct models, this is\\nmore than just autocomplete and attempts to answer the question.\\nA vital part of these completion models is something called the context\\nlength or context window. The context length represents the maximum\\nnumber of tokens the model can process, as shown in Figure 1-27. A large\\ncontext window allows entire documents to be passed to the LLM. Note\\nthat due to the autoregressive nature of these models, the current context\\nlength will increase as new tokens are generated.', 'Figure 1-27. The context length is the maximum context an LLM can handle.\\nThe Year of Generative AI\\nLLMs had a tremendous impact on the field and led some to call 2023 The\\nYear of Generative AI with the release, adoption, and media coverage of\\nChatGPT (GPT-3.5). When we refer to ChatGPT, we are actually talking\\nabout the product and not the underlying model. When it was first released,\\nit was powered by the GPT-3.5 LLM and has since then grown to include\\nseveral more performant variants, such as GPT-4.10 \\nGPT-3.5 was not the only model that made its impact in the Year of\\nGenerative AI. As illustrated in Figure 1-28, both open source and\\nproprietary LLMs have made their way to the people at an incredible pace.\\nThese open source base models are often referred to as foundation models\\nand can be fine-tuned for specific tasks, like following instructions.', 'Figure 1-28. A comprehensive view into the Year of Generative AI. Note that many models are still\\nmissing from this overview!\\nApart from the widely popular Transformer architecture, new promising\\narchitectures have emerged such as Mamba11 ,12  and RWKV.13  These novel\\narchitectures attempt to reach Transformer-level performance with\\nadditional advantages, like larger context windows or faster inference.\\nThese developments exemplify the evolution of the field and showcase\\n2023 as a truly hectic year for AI. It took all we had to just keep up with the\\nmany developments, both within and outside of Language AI.\\nAs such, this book explores more than just the latest LLMs. We will explore\\nhow other models, such as embedding models, encoder-only models, and\\neven bag-of-words can be used to empower LLMs.\\nThe Moving Definition of a “Large Language\\nModel”\\nIn our travels through the recent history of Language AI, we observed that\\nprimarily generative decoder-only (Transformer) models are commonly', 'referred to as large language models. Especially if they are considered to be\\n“large.” In practice, this seems like a rather constrained description!\\nWhat if we create a model with the same capabilities as GPT-3 but 10 times\\nsmaller? Would such a model fall outside the “large” language model\\ncategorization?\\nSimilarly, what if we released a model as big as GPT-4 that can perform\\naccurate text classification but does not have any generative capabilities?\\nWould it still qualify as a large “language model” if its primary function is\\nnot language generation, even though it still represents text?\\nThe problem with these kinds of definitions is that we exclude capable\\nmodels. What name we give one model or the other does not change how it\\nbehaves.\\nSince the definition of the term “large language model” tends to evolve with\\nthe release of new models, we want to be explicit in what it means for this\\nbook. “Large” is arbitrary and what might be considered a large model\\ntoday could be small tomorrow. There are currently many names for the\\nsame thing and to us, “large language models” are also models that do not\\ngenerate text and can be run on consumer hardware.\\nAs such, aside from covering generative models, this book will also cover\\nmodels with fewer than 1 billion parameters that do not generate text. We\\nwill explore how other models, such as embedding models, representation\\nmodels, and even bag-of-words can be used to empower LLMs.\\nThe Training Paradigm of Large Language\\nModels\\nTraditional machine learning generally involves training a model for a\\nspecific task, like classification. As shown in Figure 1-29, we consider this\\nto be a one-step process.', 'Figure 1-29. Traditional machine learning involves a single step: training a model for a specific\\ntarget task, like classification or regression.\\nCreating LLMs, in contrast, typically consists of at least two steps:\\nLanguage modeling\\nThe first step, called pretraining, takes the majority of computation and\\ntraining time. An LLM is trained on a vast corpus of internet text\\nallowing the model to learn grammar, context, and language patterns.\\nThis broad training phase is not yet directed toward specific tasks or\\napplications beyond predicting the next word. The resulting model is\\noften referred to as a foundation model or base model. These models\\ngenerally do not follow instructions.\\nFine-tuning\\nThe second step, fine-tuning or sometimes post-training, involves using\\nthe previously trained model and further training it on a narrower task.\\nThis allows the LLM to adapt to specific tasks or to exhibit desired\\nbehavior. For example, we could fine-tune a base model to perform well\\non a classification task or to follow instructions. It saves massive\\namounts of resources because the pretraining phase is quite costly and\\ngenerally requires data and computing resources that are out of the\\nreach of most people and organizations. For instance, Llama 2 has been\\ntrained on a dataset containing 2 trillion tokens.14  Imagine the compute', 'necessary to create that model! In Chapter 12, we will go over several\\nmethods for fine-tuning foundation models on your dataset.\\nAny model that goes through the first step, pretraining, we consider a\\npretrained model, which also includes fine-tuned models. This two-step\\napproach of training is visualized in Figure 1-30.\\nFigure 1-30. Compared to traditional machine learning, LLM training takes a multistep approach.\\nAdditional fine-tuning steps can be added to further align the model with\\nthe user’s preferences, as we will explore in Chapter 12.\\nLarge Language Model Applications: What\\nMakes Them So Useful?\\nThe nature of LLMs makes them suitable for a wide range of tasks. With\\ntext generation and prompting, it almost seems as if your imagination is the\\nlimit. To illustrate, let’s explore some common tasks and techniques:\\nDetecting whether a review left by a customer is positive or negative\\nThis is (supervised) classification and can be handled with both\\nencoder- and decoder-only models either with pretrained models (see\\nChapter 4) or by fine-tuning models (see Chapter 11).\\nDeveloping a system for finding common topics in ticket issues', 'This is (unsupervised) classification for which we have no predefined\\nlabels. We can leverage encoder-only models to perform the\\nclassification itself and decoder-only models for labeling the topics (see\\nChapter 5).\\nBuilding a system for retrieval and inspection of relevant documents\\nA major component of language model systems is their ability to add\\nexternal resources of information. Using semantic search, we can build\\nsystems that allow us to easily access and find information for an LLM\\nto use (see Chapter 8). Improve your system by creating or fine-tuning a\\ncustom embedding model (see Chapter 12).\\nConstructing an LLM chatbot that can leverage external resources, such as\\ntools and documents\\nThis is a combination of techniques that demonstrates how the true\\npower of LLMs can be found through additional components. Methods\\nsuch as prompt engineering (see Chapter 6), retrieval-augmented\\ngeneration (see Chapter 8), and fine-tuning an LLM (see Chapter 12)\\nare all pieces of the LLM puzzle.\\nConstructing an LLM capable of writing recipes based on a picture\\nshowing the products in your fridge\\nThis is a multimodal task where the LLM takes in an image and reasons\\nabout what it sees (see Chapter 9). LLMs are being adapted to other\\nmodalities, such as Vision, which opens a wide variety of interesting use\\ncases.', 'LLM applications are incredibly satisfying to create since they are partially\\nbounded by the things you can imagine. As these models grow more\\naccurate, using them in practice for creative use cases such as role-playing\\nand writing children’s books simply becomes more and more fun.\\nResponsible LLM Development and Usage\\nThe impact of LLMs has been and likely continues to be significant due to\\ntheir widespread adoption. As we explore the incredible capabilities of\\nLLMs it is important to keep their societal and ethical implications in mind.\\nSeveral key points to consider:\\nBias and fairness\\nLLMs are trained on large amounts of data that might contain biases.\\nLLMs might learn from these biases, start to reproduce them, and\\npotentially amplify them. Since the data on which LLMs are trained are\\nseldom shared, it remains unclear what potential biases they might\\ncontain unless you try them out.\\nTransparency and accountability\\nDue to LLMs’ incredible capabilities, it is not always clear when you\\nare talking with a human or an LLM. As such, the usage of LLMs when\\ninteracting with humans can have unintended consequences when there\\nis no human in the loop. For instance, LLM-based applications used in\\nthe medical field might be regulated as medical devices since they could\\naffect a patient’s well-being.\\nGenerating harmful content\\nAn LLM does not necessarily generate ground-truth content and might\\nconfidently output incorrect text. Moreover, they can be used to', 'generate fake news, articles, and other misleading sources of\\ninformation.\\nIntellectual property\\nIs the output of an LLM your intellectual property or that of the LLM’s\\ncreator? When the output is similar to a phrase in the training data, does\\nthe intellectual property belong to the author of that phrase? Without\\naccess to the training data it remains unclear when copyrighted material\\nis being used by the LLM.\\nRegulation\\nDue to the enormous impact of LLMs, governments are starting to\\nregulate commercial applications. An example is the European AI Act,\\nwhich regulates the development and deployment of foundation models\\nincluding LLMs.\\nAs you develop and use LLMs, we want to stress the importance of ethical\\nconsiderations and urge you to learn more about the safe and responsible\\nuse of LLMs and AI systems in general.\\nLimited Resources Are All You Need\\nThe compute resources that we have referenced several times thus far\\ngenerally relate to the GPU(s) you have available on your system. A\\npowerful GPU (graphics card) will make both training and using LLMs\\nmuch more efficient and faster.\\nIn choosing a GPU, an important component is the amount of VRAM\\n(video random-access memory) you have available. This refers to the\\namount of memory you have available on your GPU. In practice, the more', 'VRAM you have the better. The reason for this is that some models simply\\ncannot be used at all if you do not have sufficient VRAM.\\nBecause training and fine-tuning LLMs can be an expensive process, GPU-\\nwise, those without a powerful GPU have often been referred to as the\\nGPU-poor. This illustrates the battle for computing resources to train these\\nhuge models. To create the Llama 2 family of models, for example, Meta\\nused A100-80 GB GPUs. Assuming renting such a GPU would cost\\n$1.50/hr, the total costs of creating these models would exceed\\n$5,000,000!15 \\nUnfortunately, there is no single rule to determine exactly how much\\nVRAM you need for a specific model. It depends on the model’s\\narchitecture and size, compression technique, context size, backend for\\nrunning the model, etc.\\nThis book is for the GPU-poor! We will use models that users can run\\nwithout the most expensive GPU(s) available or a big budget. To do so, we\\nwill make all the code available in Google Colab instances. At the time of\\nwriting, a free instance of Google Colab will net you a T4 GPU with 16 GB\\nVRAM, which is the minimum amount of VRAM that we suggest.\\nInterfacing with Large Language Models\\nInterfacing with LLMs is a vital component of not only using them but also\\ndeveloping an understanding of their inner workings. Due to the many\\ndevelopments in the field, there has been an abundance of techniques,\\nmethods, and packages for communicating with LLMs. Throughout the\\nbook, we intend to explore the most common techniques for doing so,\\nincluding using both proprietary (closed source) and publicly available open\\nmodels.\\nProprietary, Private Models\\nClosed source LLMs are models that do not have their weights and\\narchitecture shared with the public. They are developed by specific', 'organizations with their underlying code being kept secret. Examples of\\nsuch models include OpenAI’s GPT-4 and Anthropic’s Claude. These\\nproprietary models are generally backed by significant commercial support\\nand have been developed and integrated within their services.\\nYou can access these models through an interface that communicates with\\nthe LLM, called an API (application programming interface), as illustrated\\nin Figure 1-31. For instance, to use ChatGPT in Python you can use\\nOpenAI’s package to interface with the service without directly accessing it.\\nFigure 1-31. Closed source LLMs are accessed by an interface (API). As a result, details of the LLM\\nitself, including its code and architecture are not shared with the user.\\nA huge benefit of proprietary models is that the user does not need to have a\\nstrong GPU to use the LLM. The provider takes care of hosting and running\\nthe model and generally has more computing available. There is no\\nexpertise necessary concerning hosting and using the model, which lowers\\nthe barrier to entry significantly. Moreover, these models tend to be more\\nperformant than their open source counterparts due to the significant\\ninvestment from these organizations.\\nA downside to this is that it can be a costly service. The provider manages\\nthe risk and costs of hosting the LLM, which often translates to a paid\\nservice. Moreover, since there is no direct access to the model, there is no\\nmethod to fine-tune it yourself. Lastly, your data is shared with the provider,\\nwhich is not desirable in many common use cases, such as sharing patient\\ndata.', 'Open Models\\nOpen LLMs are models that share their weights and architecture with the\\npublic to use. They are still developed by specific organizations but often\\nshare their code for creating or running the model locally—with varying\\nlevels of licensing that may or may not allow commercial usage of the\\nmodel. Cohere’s Command R, the Mistral models, Microsoft’s Phi, and\\nMeta’s Llama models are all examples of open models.\\nNOTE\\nThere are ongoing discussions as to what truly represents an open source model. For\\ninstance, some publicly shared models have a permissive commercial license, which\\nmeans that the model cannot be used for commercial purposes. For many, this is not the\\ntrue definition of open source, which states that using these models should not have any\\nrestrictions. Similarly, the data on which a model is trained as well as its source code are\\nseldom shared.\\nYou can download these models and use them on your device as long as\\nyou have a powerful GPU that can handle these kinds of models, as shown\\nin Figure 1-32.\\nFigure 1-32. Open source LLMs are directly by the user. As a result, details of the LLM itself\\nincluding its code and architecture are shared with the user.\\nA major advantage of these local models is that you, the user, have\\ncomplete control over the model. You can use the model without depending\\non the API connection, fine-tune it, and run sensitive data through it. You\\nare not dependent on any service and have complete transparency of the', 'processes that lead to the output of the model. This benefit is enhanced by\\nthe large communities that enable these processes, such as Hugging Face,\\ndemonstrating the possibilities of collaborative efforts.\\nA downside is that you need powerful hardware to run these models and\\neven more when training or fine-tuning them. Moreover, it requires specific\\nknowledge to set up and use these models (which we will cover throughout\\nthis book).\\nWe generally prefer using open source models wherever we can. The\\nfreedom this gives to play around with options, explore the inner workings,\\nand use the model locally arguably provides more benefits than using\\nproprietary LLMs.\\nOpen Source Frameworks\\nCompared to closed source LLMs, open source LLMs require you to use\\ncertain packages to run them. In 2023, many different packages and\\nframeworks were released that, each in their own way, interact with and\\nmake use of LLMs. Wading through hundreds upon hundreds of potentially\\nworthwhile frameworks is not the most enjoyable experience.\\nAs a result, you might even miss your favorite framework in this book!\\nInstead of attempting to cover every LLM framework in existence (there are\\ntoo many, and they continue to grow in number), we aim to provide you\\nwith a solid foundation for leveraging LLMs. The idea is that after reading\\nthis book, you can easily pick up most other frameworks as they all work in\\na very similar manner.\\nThe intuition that we attempt to realize is an important component of this. If\\nyou have an intuitive understanding of not only LLMs but also using them\\nin practice with common frameworks, branching out to others should be a\\nstraightforward task.\\nMore specifically, we focus on backend packages. These are packages\\nwithout a GUI (graphical user interface) that are created for efficiently', 'loading and running any LLM on your device, such as llama.cpp,\\nLangChain, and the core of many frameworks, Hugging Face Transformers.\\nTIP\\nWe will mostly cover frameworks for interacting with large language models through\\ncode. Although it helps you learn the fundamentals of these frameworks, sometimes you\\njust want a ChatGPT-like interface with a local LLM. Fortunately, there are many\\nincredible frameworks that allow for this. A few examples include text-generation-\\nwebui, KoboldCpp, and LM Studio.\\nGenerating Your First Text\\nAn important component of using language models is selecting them. The\\nmain source for finding and downloading LLMs is the Hugging Face Hub.\\nHugging Face is the organization behind the well-known Transformers\\npackage, which for years has driven the development of language models in\\ngeneral. As the name implies, the package was built on top of the\\ntransformers framework that we discussed in “A Recent History of\\nLanguage AI”.\\nAt the time of writing, you will find more than 800,000 models on Hugging\\nFace’s platform for many different purposes, from LLMs and computer\\nvision models to models that work with audio and tabular data. Here, you\\ncan find almost any open source LLM.\\nAlthough we will explore all kinds of models throughout this book, let’s\\nstart our first lines of code with a generative model. The main generative\\nmodel we use throughout the book is Phi-3-mini, which is a relatively small\\n(3.8 billion parameters) but quite performant model.16  Due to its small size,\\nthe model can be run on devices with less than 8 GB of VRAM. If you\\nperform quantization, a type of compression that we will further discuss in\\nChapters 7 and 12, you can use even less than 6 GB of VRAM. Moreover,\\nthe model is licensed under the MIT license, which allows the model to be\\nused for commercial purposes without constraints!', 'Keep in mind that new and improved LLMs are frequently released. To\\nensure this book remains current, most examples are designed to work with\\nany LLM. We’ll also highlight different models in the repository associated\\nwith this book for you to try out.\\nLet’s get started! When you use an LLM, two models are loaded:\\nThe generative model itself\\nIts underlying tokenizer\\nThe tokenizer is in charge of splitting the input text into tokens before\\nfeeding it to the generative model. You can find the tokenizer and model on\\nthe Hugging Face site and only need the corresponding IDs to be passed. In\\nthis case, we use “microsoft/Phi-3-mini-4k-instruct” as the main path to the\\nmodel.\\nWe can use transformers to load both the tokenizer and model. Note\\nthat we assume you have an NVIDIA GPU (device_map=\"cuda\") but\\nyou can choose a different device instead. If you do not have access to a\\nGPU you can use the free Google Colab notebooks we made available in\\nthe repository of this book:\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n# Load model and tokenizer\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    device_map=\"cuda\",\\n    torch_dtype=\"auto\",\\n    trust_remote_code=True,\\n)\\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-\\n4k-instruct\")\\nRunning the code will start downloading the model and depending on your\\ninternet connection can take a couple of minutes.', 'Although we now have enough to start generating text, there is a nice trick\\nin transformers that simplifies the process, namely\\ntransformers.pipeline. It encapsulates the model, tokenizer, and\\ntext generation process into a single function:\\nfrom transformers import pipeline\\n# Create a pipeline\\ngenerator = pipeline(\\n    \"text-generation\",\\n    model=model,\\n    tokenizer=tokenizer,\\n    return_full_text=False,\\n    max_new_tokens=500,\\n    do_sample=False\\n)\\nThe following parameters are worth mentioning:\\nreturn_full_text\\nBy setting this to False, the prompt will not be returned but merely\\nthe output of the model.\\nmax_new_tokens\\nThe maximum number of tokens the model will generate. By setting a\\nlimit, we prevent long and unwieldy output as some models might\\ncontinue generating output until they reach their context window.\\ndo_sample\\nWhether the model uses a sampling strategy to choose the next token.\\nBy setting this to False, the model will always select the next most\\nprobable token. In Chapter 6, we explore several sampling parameters\\nthat invoke some creativity in the model’s output.', 'To generate our first text, let’s instruct the model to tell a joke about\\nchickens. To do so, we format the prompt in a list of dictionaries where\\neach dictionary relates to an entity in the conversation. Our role is that of\\n“user” and we use the “content” key to define our prompt:\\n# The prompt (user input / query)\\nmessages = [\\n    {\"role\": \"user\", \"content\": \"Create a funny joke about \\nchickens.\"}\\n]\\n# Generate output\\noutput = generator(messages)\\nprint(output[0][\"generated_text\"])\\nWhy don\\'t chickens like to go to the gym? Because they can\\'t \\ncrack the egg-sistence of it!\\nAnd that is it! The first text generated in this book was a decent joke about\\nchickens.\\nSummary\\nIn this first chapter of the book, we delved into the revolutionary impact\\nLLMs have had on the Language AI field. It has significantly changed our\\napproach to tasks such as translation, classification, summarization, and\\nmore. Through a recent history of Language AI, we explored the\\nfundamentals of several types of LLMs, from a simple bag-of-words\\nrepresentation to more complex representations using neural networks.\\nWe discussed the attention mechanism as a step toward encoding context\\nwithin models, a vital component of what makes LLMs so capable. We\\ntouched on two main categories of models that use this incredible\\nmechanism: representation models (encoder-only) like BERT and\\ngenerative models (decoder-only) like the GPT family of models. Both\\ncategories are considered large language models throughout this book.', 'Overall, the chapter provided an overview of the landscape of Language AI,\\nincluding its applications, societal and ethical implications, and the\\nresources needed to run such models. We ended by generating our first text\\nusing Phi-3, a model that will be used throughout the book.\\nIn the next two chapters, you will learn about some underlying processes.\\nWe start by exploring tokenization and embeddings in Chapter 2, two often\\nunderestimated but vital components of the Language AI field. What\\nfollows in Chapter 3 is an in-depth look into language models where you\\nwill discover the precise methods used for generating text.\\n1  J. McCarthy (2007). “What is artificial intelligence?” Retrieved from https://oreil.ly/C7sja\\nand https://oreil.ly/n9X8O.\\n2  Fabrizio Sebastiani. “Machine learning in automated text categorization.” ACM Computing\\nSurveys (CSUR) 34.1 (2002): 1–47.\\n3  Tomas Mikolov et al. “Efficient estimation of word representations in vector space.” arXiv\\npreprint arXiv:1301.3781 (2013).\\n4  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. “Neural machine translation by\\njointly learning to align and translate.” arXiv preprint arXiv:1409.0473 (2014).\\n5  Ashish Vaswani et al. “Attention is all you need.” Advances in Neural Information Processing\\nSystems 30 (2017).\\n6  Jacob Devlin et al. “BERT: Pre-training of deep bidirectional transformers for language\\nunderstanding.” arXiv preprint arXiv:1810.04805 (2018).\\n7  Alec Radford et al. “Improving language understanding by generative pre-training”, (2018).\\n8  Alec Radford et al. “Language models are unsupervised multitask learners.” OpenAI Blog 1.8\\n(2019): 9.\\n9  Tom Brown et al. “Language models are few-shot learners.” Advances in Neural Information\\nProcessing Systems 33 (2020): 1877–1901.\\n10  OpenAI, “Gpt-4 technical report.” arXiv preprint arXiv:2303.08774 (2023).\\n11  Albert Gu and Tri Dao. “Mamba: Linear-time sequence modeling with selective state spaces.”\\narXiv preprint arXiv:2312.00752 (2023).\\n12  See “A Visual Guide to Mamba and State Space Models” for an illustrated and visual guide to\\nMamba as an alternative to the Transformer architecture.', '13  Bo Peng et al. “RWKV: Reinventing RNNs for the transformer era.” arXiv preprint\\narXiv:2305.13048 (2023).\\n14  Hugo Touvron et al. “Llama 2: Open foundation and fine-tuned chat models.” arXiv preprint\\narXiv:2307.09288 (2023).\\n15  The models were trained for 3,311,616 GPU hours, which refers to the amount of time it takes\\nto train a model on a GPU, multiplied by the number of GPUs available.\\n16  Marah Abdin et al. “Phi-3 technical report: A highly capable language model locally on your\\nphone.” arXiv preprint arXiv:2404.14219 (2024).\\nOceanofPDF.com', 'Chapter 2. Tokens and\\nEmbeddings\\nTokens and embeddings are two of the central concepts of using large\\nlanguage models (LLMs). As we’ve seen in the first chapter, they’re not\\nonly important to understanding the history of Language AI, but we cannot\\nhave a clear sense of how LLMs work, how they’re built, and where they\\nwill go in the future without a good sense of tokens and embeddings, as we\\ncan see in Figure 2-1.\\nFigure 2-1. Language models deal with text in small chunks called tokens. For the language model to\\ncompute language, it needs to turn tokens into numeric representations called embeddings.\\nIn this chapter, we look more closely at what tokens are and the\\ntokenization methods used to power LLMs. We will then dive into the\\nfamous word2vec embedding method that preceded modern-day LLMs and\\nsee how it’s extending the concept of token embeddings to build', 'commercial recommendation systems that power a lot of the apps you use.\\nFinally, we go from token embeddings into sentence or text embeddings,\\nwhere a whole sentence or document can have one vector that represents it\\n—enabling applications like semantic search and topic modeling that we\\nsee in Part II of this book.\\nLLM Tokenization\\nThe way the majority of people interact with language models, at the time\\nof this writing, is through a web playground that presents a chat interface\\nbetween the user and a language model. You may notice that a model does\\nnot produce its output response all at once; it actually generates one token at\\na time.\\nBut tokens aren’t only the output of a model, they’re also the way in which\\nthe model sees its inputs. A text prompt sent to the model is first broken\\ndown into tokens, as we’ll now see.\\nHow Tokenizers Prepare the Inputs to the Language\\nModel\\nViewed from the outside, generative LLMs take an input prompt and\\ngenerate a response, as we can see in Figure 2-2.\\nFigure 2-2. High-level view of a language model and its input prompt.', 'Before the prompt is presented to the language model, however, it first has\\nto go through a tokenizer that breaks it into pieces. You can find an example\\nshowing the tokenizer of GPT-4 on the OpenAI Platform. If we feed it the\\ninput text, it shows the output in Figure 2-3, where each token is shown in a\\ndifferent color.\\nFigure 2-3. A tokenizer breaks down text into words or parts of words before the model processes the\\ntext. It does so according to a specific method and training procedure (from https://oreil.ly/ovUWO).\\nLet’s look at a code example and interact with these tokens ourselves. Here\\nwe’ll be downloading an LLM and seeing how to tokenize the input before\\ngenerating text with the LLM.\\nDownloading and Running an LLM\\nLet’s start by loading our model and its tokenizer as we’ve done in\\nChapter 1:', 'from transformers import AutoModelForCausalLM, AutoTokenizer\\n# Load model and tokenizer\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    device_map=\"cuda\",\\n    torch_dtype=\"auto\",\\n    trust_remote_code=True,\\n)\\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-\\n4k-instruct\")\\nWe can then proceed to the actual generation. We first declare our prompt,\\nthen tokenize it, then pass those tokens to the model, which generates its\\noutput. In this case, we’re asking the model to only generate 20 new tokens:\\nprompt = \"Write an email apologizing to Sarah for the tragic \\ngardening mishap. Explain how it happened.<|assistant|>\"\\n# Tokenize the input prompt\\ninput_ids = tokenizer(prompt, \\nreturn_tensors=\"pt\").input_ids.to(\"cuda\")\\n# Generate the text\\ngeneration_output = model.generate(\\n  input_ids=input_ids,\\n  max_new_tokens=20\\n)\\n# Print the output\\nprint(tokenizer.decode(generation_output[0]))\\nOutput:\\n<s> Write an email apologizing to Sarah for the tragic \\ngardening mishap. Explain how it happened.<|assistant|> Subject: \\nMy Sincere Apologies for the Gardening Mishap\\nDear', \"The text in bold is the 20 tokens generated by the model.\\nLooking at the code, we can see that the model does not in fact receive the\\ntext prompt. Instead, the tokenizers processed the input prompt, and\\nreturned the information the model needed in the variable input_ids,\\nwhich the model used as its input.\\nLet’s print input_ids to see what it holds inside:\\ntensor([[ 1, 14350, 385, 4876, 27746, 5281, 304, 19235, 363, \\n278, 25305, 293, 16423, 292, 286, 728, 481, 29889, 12027, 7420, \\n920, 372, 9559, 29889, 32001]], device='cuda:0')\\nThis reveals the inputs that LLMs respond to, a series of integers as shown\\nin Figure 2-4. Each one is the unique ID for a specific token (character,\\nword, or part of a word). These IDs reference a table inside the tokenizer\\ncontaining all the tokens it knows.\\nFigure 2-4. A tokenizer processes the input prompt and prepares the actual input into the language\\nmodel: a list of token IDs. The specific token IDs in the figure are just demonstrative.\\nIf we want to inspect those IDs, we can use the tokenizer’s decode method\\nto translate the IDs back into text that we can read:\\nfor id in input_ids[0]:\\n   print(tokenizer.decode(id))\", 'This prints (each token is on a separate line):\\n<s>\\nWrite\\nan\\nemail\\napolog\\nizing\\nto\\nSarah\\nfor\\nthe\\ntrag\\nic\\ngarden\\ning\\nm\\nish\\nap\\n.\\nExp\\nlain\\nhow\\nit', \"happened\\n.\\n<|assistant|>\\nThis is how the tokenizer broke down our input prompt. Notice the\\nfollowing:\\nThe first token is ID 1 (<s>), a special token indicating the\\nbeginning of the text.\\nSome tokens are complete words (e.g., Write, an, email).\\nSome tokens are parts of words (e.g., apolog, izing, trag, ic).\\nPunctuation characters are their own token.\\nNotice how the space character does not have its own token. Instead, partial\\ntokens (like “izing” and “ic”) have a special hidden character at their\\nbeginning that indicates that they’re connected with the token that precedes\\nthem in the text. Tokens without that special character are assumed to have\\na space before them.\\nOn the output side, we can also inspect the tokens generated by the model\\nby printing the generation_output variable. This shows the input\\ntokens as well as the output tokens (we’ll highlight the new tokens in bold):\\ntensor([[ 1, 14350, 385, 4876, 27746, 5281, 304, 19235, 363, \\n278,\\n25305, 293, 16423, 292, 286, 728, 481, 29889, 12027, 7420,\\n920, 372, 9559, 29889, 32001, 3323, 622, 29901, 1619, 317,\\n3742, 406, 6225, 11763, 363, 278, 19906, 292, 341, 728,\\n481, 13, 13, 29928, 799]], device='cuda:0')\", \"This shows us the model generated the token 3323, 'Sub', followed by\\ntoken 622, 'ject'. Together they formed the word 'Subject'. They\\nwere then followed by token 29901, which is the colon ':'...and so on.\\nJust like on the input side, we need the tokenizer on the output side to\\ntranslate the token ID into the actual text. We do that using the tokenizer’s\\ndecode method. We can pass it an individual token ID or a list of them:\\nprint(tokenizer.decode(3323))\\nprint(tokenizer.decode(622))\\nprint(tokenizer.decode([3323, 622]))\\nprint(tokenizer.decode(29901))\\nThis outputs:\\nSub\\nject\\nSubject\\n:\\nHow Does the Tokenizer Break Down Text?\\nThere are three major factors that dictate how a tokenizer breaks down an\\ninput prompt.\\nFirst, at model design time, the creator of the model chooses a tokenization\\nmethod. Popular methods include byte pair encoding (BPE) (widely used by\\nGPT models) and WordPiece (used by BERT). These methods are similar in\\nthat they aim to optimize an efficient set of tokens to represent a text\\ndataset, but they arrive at it in different ways.\\nSecond, after choosing the method, we need to make a number of tokenizer\\ndesign choices like vocabulary size and what special tokens to use. More on\\nthis in “Comparing Trained LLM Tokenizers”.\", 'Third, the tokenizer needs to be trained on a specific dataset to establish the\\nbest vocabulary it can use to represent that dataset. Even if we set the same\\nmethods and parameters, a tokenizer trained on an English text dataset will\\nbe different from another trained on a code dataset or a multilingual text\\ndataset.\\nIn addition to being used to process the input text into a language model,\\ntokenizers are used on the output of the language model to turn the resulting\\ntoken ID into the output word or token associated with it, as Figure 2-5\\nshows.\\nFigure 2-5. Tokenizers are also used to process the output of the model by converting the output\\ntoken ID into the word or token associated with that ID.\\nWord Versus Subword Versus Character Versus Byte\\nTokens\\nThe tokenization scheme we just discussed is called subword tokenization.\\nIt’s the most commonly used tokenization scheme but not the only one. The', 'four notable ways to tokenize are shown in Figure 2-6. Let’s go over them:\\nWord tokens\\nThis approach was common with earlier methods like word2vec but is\\nbeing used less and less in NLP. Its usefulness, however, led it to be\\nused outside of NLP for use cases such as recommendation systems, as\\nwe’ll see later in the chapter.\\nOne challenge with word tokenization is that the tokenizer may be\\nunable to deal with new words that enter the dataset after the tokenizer\\nwas trained. This also results in a vocabulary that has a lot of tokens\\nwith minimal differences between them (e.g., apology, apologize,\\napologetic, apologist). This latter challenge is resolved by subword\\ntokenization as it has a token for apolog, and then suffix tokens (e.g., -y,\\n-ize, -etic, -ist) that are common with many other tokens, resulting in a\\nmore expressive vocabulary.\\nSubword tokens\\nThis method contains full and partial words. In addition to the\\nvocabulary expressivity mentioned earlier, another benefit of the\\napproach is its ability to represent new words by breaking down the new\\ntoken into smaller characters, which tend to be a part of the vocabulary.', 'Figure 2-6. There are multiple methods of tokenization that break down the text to different sizes\\nof components (words, subwords, characters, and bytes).\\nCharacter tokens\\nThis is another method that can deal successfully with new words\\nbecause it has the raw letters to fall back on. While that makes the\\nrepresentation easier to tokenize, it makes the modeling more difficult.\\nWhere a model with subword tokenization can represent “play” as one\\ntoken, a model using character-level tokens needs to model the\\ninformation to spell out “p-l-a-y” in addition to modeling the rest of the\\nsequence.\\nSubword tokens present an advantage over character tokens in the\\nability to fit more text within the limited context length of a\\nTransformer model. So with a model with a context length of 1,024, you\\nmay be able to fit about three times as much text using subword', 'tokenization than using character tokens (subword tokens often average\\nthree characters per token).\\nByte tokens\\nOne additional tokenization method breaks down tokens into the\\nindividual bytes that are used to represent unicode characters. Papers\\nlike “CANINE: Pre-training an efficient tokenization-free encoder for\\nlanguage representation” outline methods like this, which are also called\\n“tokenization-free encoding.” Other works like “ByT5: Towards a\\ntoken-free future with pre-trained byte-to-byte models” show that this\\ncan be a competitive method, especially in multilingual scenarios.\\nOne distinction to highlight here: some subword tokenizers also include\\nbytes as tokens in their vocabulary as the final building block to fall back to\\nwhen they encounter characters they can’t otherwise represent. The GPT-2\\nand RoBERTa tokenizers do this, for example. This doesn’t make them\\ntokenization-free byte-level tokenizers, because they don’t use these bytes\\nto represent everything, only a subset, as we’ll see in the next section.\\nIf you want to go deeper into tokenizers, they are discussed in more detail\\nin Designing Large Language Model Applications.\\nComparing Trained LLM Tokenizers\\nWe’ve pointed out earlier three major factors that dictate the tokens that\\nappear within a tokenizer: the tokenization method, the parameters and\\nspecial tokens we use to initialize the tokenizer, and the dataset the\\ntokenizer is trained on. Let’s compare and contrast a number of actual,\\ntrained tokenizers to see how these choices change their behavior. This\\ncomparison will show us that newer tokenizers have changed their behavior\\nto improve model performance, and we’ll also see how specialized models\\n(like code generation models, for example) often need specialized\\ntokenizers.', 'We’ll use a number of tokenizers to encode the following text:\\ntext = \"\"\"\\nEnglish and CAPITALIZATION\\n\\x00\\x00\\nshow_tokens False None elif == >= else: two tabs:\" \" Three tabs: \\n\"   \"\\n12.0*50=600\\n\"\"\"\\nThis will allow us to see how each tokenizer deals with a number of\\ndifferent kinds of tokens:\\nCapitalization.\\nLanguages other than English.\\nEmojis.\\nProgramming code with keywords and whitespaces often used for\\nindentation (in languages like Python for example).\\nNumbers and digits.\\nSpecial tokens. These are unique tokens that have a role other than\\nrepresenting text. They include tokens that indicate the beginning\\nof the text, or the end of the text (which is the way the model\\nsignals to the system that it has completed this generation), or other\\nfunctions as we’ll see.\\nLet’s go from older to newer tokenizers to see how they tokenize this text\\nand what that might say about the language model. We’ll tokenize the text,\\nand then print each token with a color background color using this function:\\ncolors_list = [', \"    '102;194;165', '252;141;98', '141;160;203', \\n    '231;138;195', '166;216;84', '255;217;47'\\n]\\ndef show_tokens(sentence, tokenizer_name):\\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\\n    token_ids = tokenizer(sentence).input_ids\\n    for idx, t in enumerate(token_ids):\\n        print(\\n            f'\\\\x1b[0;30;48;2;{colors_list[idx % \\nlen(colors_list)]}m' + \\n            tokenizer.decode(t) + \\n            '\\\\x1b[0m', \\n            end=' '\\n        )\\nBERT base model (uncased) (2018)\\nLink to the model on the HuggingFace model hub\\nTokenization method: WordPiece, introduced in “Japanese and Korean\\nvoice search”:\\nVocabulary size: 30,522\\nSpecial tokens:\\nunk_token [UNK]\\nAn unknown token that the tokenizer has no specific encoding for.\\nsep_token [SEP]\\nA separator that enables certain tasks that require giving the model two\\ntexts (in these cases, the model is called a cross-encoder). One example\\nis reranking, as we’ll see in Chapter 8.\\npad_token [PAD]\\nA padding token used to pad unused positions in the model’s input (as\\nthe model expects a certain length of input, its context-size).\\ncls_token [CLS]\", 'A special classification token for classification tasks, as we’ll see in\\nChapter 4.\\nmask_token [MASK]\\nA masking token used to hide tokens during the training process.\\nTokenized text:\\n[CLS] english and capital ##ization [UNK] [UNK] show _\\ntoken ##s false none eli ##f = = > = else : two tab ##s : \"\\n\" three tab ##s : \" \" 12 . 0 * 50 = 600 [SEP]\\nBERT was released in two major flavors: cased (where the capitalization is\\nkept) and uncased (where all capital letters are first turned into small cap\\nletters). With the uncased (and more popular) version of the BERT\\ntokenizer, we notice the following:\\nThe newline breaks are gone, which makes the model blind to\\ninformation encoded in newlines (e.g., a chat log when each turn is\\nin a new line).\\nAll the text is in lowercase.\\nThe word “capitalization” is encoded as two subtokens: capital\\n##ization. The ## characters are used to indicate this token is a\\npartial token connected to the token that precedes it. This is also a\\nmethod to indicate where the spaces are, as it is assumed tokens\\nwithout ## in front have a space before them.\\nThe emoji and Chinese characters are gone and replaced with the\\n[UNK] special token indicating an “unknown token.”\\nBERT base model (cased) (2018)\\nLink to the model on the HuggingFace model hub\\nTokenization method: WordPiece', 'Vocabulary size: 28,996\\nSpecial tokens: Same as the uncased version\\nTokenized text:\\n[CLS] English and CA ##PI ##TA ##L ##I ##Z ##AT ##ION\\n[UNK] [UNK] show _ token ##s F ##als ##e None el ##if = = >\\n= else : two ta ##bs : \" \" Three ta ##bs : \" \" 12 . 0 * 50 =\\n600 [SEP]\\nThe cased version of the BERT tokenizer differs mainly in including\\nuppercase tokens.\\nNotice how “CAPITALIZATION” is now represented as eight\\ntokens: CA ##PI ##TA ##L ##I ##Z ##AT ##ION.\\nBoth BERT tokenizers wrap the input within a starting [CLS]\\ntoken and a closing [SEP] token. [CLS] and [SEP] are utility\\ntokens used to wrap the input text and they serve their own\\npurposes. [CLS] stands for classification as it’s a token used at\\ntimes for sentence classification. [SEP] stands for separator, as\\nit’s used to separate sentences in some applications that require\\npassing two sentences to a model (For example, in Chapter 8, we\\nwill use a [SEP] token to separate the text of the query and a\\ncandidate result.)\\nGPT-2 (2019)\\nLink to the model on the HuggingFace model hub\\nTokenization method: Byte pair encoding (BPE), introduced in “Neural\\nmachine translation of rare words with subword units”.\\nVocabulary size: 50,257\\nSpecial tokens: <|endoftext|>\\nEnglish and CAP ITAL IZ ATION\\n�  �  �  �  �  � ', 'show _ t ok ens False None el if == >= else : two tabs :\" \"\\nThree tabs : \" \"\\n12 . 0 * 50 = 600\\nWith the GPT-2 tokenizer, we notice the following:\\nThe newline breaks are represented in the tokenizer.\\nCapitalization is preserved, and the word “CAPITALIZATION” is\\nrepresented in four tokens.\\nThe \\x00\\x00 characters are now represented by multiple tokens each.\\nWhile we see these tokens printed as the �  character, they actually\\nstand for different tokens. For example, the \\x00 emoji is broken down\\ninto the tokens with token IDs 8582, 236, and 113. The tokenizer is\\nsuccessful in reconstructing the original character from these\\ntokens. We can see that by printing\\ntokenizer.decode([8582, 236, 113]), which prints\\nout \\x00.\\nThe two tabs are represented as two tokens (token number 197 in\\nthat vocabulary) and the four spaces are represented as three tokens\\n(number 220) with the final space being a part of the token for the\\nclosing quote character.\\nThe two tabs are represented as two tokens (token number 197 in\\nthat vocabulary) and the four spaces are represented as three tokens\\n(number 220) with the final space being a part of the token for the\\nclosing quote character.', 'NOTE\\nWhat is the significance of whitespace characters? These are important for models to\\nunderstand or generate code. A model that uses a single token to represent four\\nconsecutive whitespace characters is more tuned to a Python code dataset. While a\\nmodel can live with representing it as four different tokens, it does make the modeling\\nmore difficult as the model needs to keep track of the indentation level, which often\\nleads to worse performance. This is an example of where tokenization choices can help\\nthe model improve on a certain task.\\nFlan-T5 (2022)\\nTokenization method: Flan-T5 uses a tokenizer implementation called\\nSentencePiece, introduced in “SentencePiece: A simple and language\\nindependent subword tokenizer and detokenizer for neural text processing”,\\nwhich supports BPE and the unigram language model (described in\\n“Subword regularization: Improving neural network translation models with\\nmultiple subword candidates”).\\nVocabulary size: 32,100\\nSpecial tokens:\\nunk_token <unk>\\npad_token <pad>\\nTokenized text:\\nEnglish and CA PI TAL IZ ATION <unk> <unk> show _ to ken s\\nFal s e None e l if = = > = else : two tab s : \" \" Three tab s\\n: \" \" 12. 0 * 50 = 600 </s>\\nThe Flan-T5 family of models use  the SentencePiece method. We notice\\nthe following:\\nNo newline or whitespace tokens; this would make it challenging\\nfor the model to work with code.', 'The emoji and Chinese characters are both replaced by the <unk>\\ntoken, making the model completely blind to them.\\nGPT-4 (2023)\\nTokenization method: BPE\\nVocabulary size: A little over 100,000\\nSpecial tokens:\\n<|endoftext|>\\nFill in the middle tokens. These three tokens enable the LLM to\\ngenerate a completion given not only the text before it but also\\nconsidering the text after it. This method is explained in more\\ndetail in the paper “Efficient training of language models to fill in\\nthe middle”; its exact details are beyond the scope of this book.\\nThese special tokens are:\\n<|fim_prefix|>\\n<|fim_middle|>\\n<|fim_suffix|>\\nTokenized text:\\nEnglish and CAPITAL IZATION\\n�  �  �  �  �  � \\nshow _tokens False None elif == >= else : two tabs :\"  \"\\nThree tabs : \"  \"\\n12 . 0 * 50 = 600\\nThe GPT-4 tokenizer behaves similarly to its ancestor, the GPT-2 tokenizer.\\nSome differences are:\\nThe GPT-4 tokenizer represents the four spaces as a single token.\\nIn fact, it has a specific token for every sequence of whitespaces up', 'to a list of 83 whitespaces.\\nThe Python keyword elif has its own token in GPT-4. Both this\\nand the previous point stem from the model’s focus on code in\\naddition to natural language.\\nThe GPT-4 tokenizer uses fewer tokens to represent most words.\\nExamples here include “CAPITALIZATION” (two tokens versus\\nfour) and “tokens” (one token versus three).\\nRefer back to what we said about the GPT-2 tokenizer with regards\\nto the Ł tokens.\\nStarCoder2 (2024)\\nStarCoder2 is a 15-billion parameter model focused on generating code\\ndescribed in the paper “StarCoder 2 and the stack v2: The next generation”,\\nwhich continues the work from the original StarCoder described in\\n“StarCoder: May the source be with you!”.\\nTokenization method: Byte pair encoding (BPE)\\nVocabulary size: 49,152\\nExample special tokens:\\n<|endoftext|>\\nFill in the middle tokens:\\n<fim_prefix>\\n<fim_middle>\\n<fim_suffix>\\n<fim_pad>\\nWhen representing code, managing the context is important. One\\nfile might make a function call to a function that is defined in a\\ndifferent file. So the model needs some way of being able to', 'identify code that is in different files in the same code repository,\\nwhile making a distinction between code in different repos. That’s\\nwhy StarCoder2 uses special tokens for the name of the repository\\nand the filename:\\n<filename>\\n<reponame>\\n<gh_stars>\\nTokenized text:\\nEnglish and CAPITAL IZATION\\n�  �  �  �  � \\nshow _ tokens False None elif == >= else : two tabs :\"  \"\\nThree tabs : \"  \"\\n1 2 . 0 * 5 0 = 6 0 0\\nThis is an encoder that focuses on code generation:\\nSimilar to GPT-4, it encodes the list of whitespaces as a single\\ntoken.\\nA major difference here to everything we’ve seen so far is that each\\ndigit is assigned its own token (so 600 becomes 6 0 0). The\\nhypothesis here is that this would lead to better representation of\\nnumbers and mathematics. In GPT-2, for example, the number 870\\nis represented as a single token. But 871 is represented as two\\ntokens (8 and 71). You can intuitively see how that might be\\nconfusing to the model and how it represents numbers.\\nGalactica\\nThe Galactica model described in “Galactica: A large language model for\\nscience” is focused on scientific knowledge and is trained on many\\nscientific papers, reference materials, and knowledge bases. It pays extra', 'attention to tokenization that makes it more sensitive to the nuances of the\\ndataset it’s representing. For example, it includes special tokens for\\ncitations, reasoning, mathematics, amino acid sequences, and DNA\\nsequences.\\nTokenization method: Byte pair encoding (BPE)\\nVocabulary size: 50,000\\nSpecial tokens:\\n<s>\\n<pad>\\n</s>\\n<unk>\\nReferences: Citations are wrapped within the two special tokens:\\n[START_REF]\\n[END_REF]\\nOne example of usage from the paper is: Recurrent\\nneural networks, long short-term memory\\n[START_REF]Long Short-Term Memory,\\nHochreiter[END_REF]\\nStep-by-step reasoning:\\n<work> is an interesting token that the model uses for\\nchain-of-thought reasoning.\\nTokenized text:\\nEnglish and CAP ITAL IZATION\\n�  �  �  �  �  �  � ', 'show _ tokens False None elif == > = else : two t abs : \"  \"\\nThree t abs : \"  \"\\n1 2 . 0 * 5 0 = 6 0 0\\nThe Galactica tokenizer behaves similar to StarCoder2 in that it has code in\\nmind. It also encodes whitespaces in the same way: assigning a single token\\nto sequences of whitespace of different lengths. It differs in that it also does\\nthat for tabs, though. So from all the tokenizers we’ve seen so far, it’s the\\nonly one that assigns a single token to the string made up of two tabs\\n(\\'\\\\t\\\\t\\').\\nPhi-3 (and Llama 2)\\nThe Phi-3 model we look at in this book reuses the tokenizer of Llama 2 yet\\nadds a number of special tokens.\\nTokenization method: Byte pair encoding (BPE)\\nVocabulary size: 32,000\\nSpecial tokens:\\n<|endoftext|>\\nChat tokens: As chat LLMs rose to popularity in 2023, the\\nconversational nature of LLMs started to be a leading use case.\\nTokenizers have been adapted to this direction by the addition of\\ntokens that indicate the turns in a conversation and the roles of\\neach speaker. These special tokens include:\\n<|user|>\\n<|assistant|>\\n<|system|>\\nWe can now recap our tour by looking at all these examples side by side:', 'BERT base\\nmodel (uncased)\\n[CLS] english and capital ##ization [UNK] [UNK] sh\\now _ token ##s false none eli ##f = = > = else : two\\ntab ##s : \" \" three tab ##s : \" \" 12 . 0 * 50 = 600 [SE\\nP]\\nBERT base\\nmodel (cased)\\n[CLS] English and CA ##PI ##TA ##L ##I ##Z ##AT ##I\\nON [UNK] [UNK] show _ token ##s F ##als ##e None el\\n##if = = > = else : two ta ##bs : \" \" Three ta ##bs :\\n\" \" 12 . 0 * 50 = 600 [SEP]\\nGPT-2 English and CAP ITAL IZ ATION\\n�  �  �  �  �  � \\nshow _ t ok ens False None el if == >= else : two tab\\ns :\" \" Three tabs : \" \"\\n12 . 0 * 50 = 600\\nFLAN-T5 English and CA PI TAL IZ ATION <unk> <unk> show _ to\\nken s Fal s e None e l if = = > = else : two tab s : \" \"\\nThree tab s : \" \" 12. 0 * 50 = 600 </s>\\nGPT-4 English and CAPITAL IZATION\\n�  �  �  �  �  � \\nshow _tokens False None elif == >= else : two tabs\\n:\"  \" Three tabs : \"  \"\\n12 . 0 * 50 = 600\\nStarCoder English and CAPITAL IZATION\\n�  �  �  �  � \\nshow _ tokens False None elif == >= else : two tabs\\n:\"  \" Three tabs : \"  \"', '1 2 . 0 * 5 0 = 6 0 0\\nGalactica English and CAP ITAL IZATION\\n�  �  �  �  �  �  � \\nshow _ tokens False None elif == > = else : two t abs\\n: \" \" Three t abs : \" \"\\n1 2 . 0 * 5 0 = 6 0 0\\nPhi-3 and Llama\\n2\\n<s>\\nEnglish and C AP IT AL IZ ATION\\n�  �  �  �  �  �  � \\nshow _ to kens False None elif == >= else : two tabs\\n:\"  \" Three tabs : \"  \"\\n1 2 . 0 * 5 0 = 6 0 0\\nTokenizer Properties\\nThe preceding guided tour of trained tokenizers showed a number of ways\\nin which actual tokenizers differ from each other. But what determines their\\ntokenization behavior? There are three major groups of design choices that\\ndetermine how the tokenizer will break down text: the tokenization method,\\nthe initialization parameters, and the domain of the data the tokenizer\\ntargets.\\nTokenization methods\\nAs we’ve seen, there are a number of tokenization methods with byte pair\\nencoding (BPE) being the more popular one. Each of these methods\\noutlines an algorithm for how to choose an appropriate set of tokens to\\nrepresent a dataset. You can find a great overview of all these methods on\\nthe Hugging Face page that summarizes tokenizers.', 'Tokenizer parameters\\nAfter choosing a tokenization method, an LLM designer needs to make\\nsome decisions about the parameters of the tokenizer. These include:\\nVocabulary size\\nHow many tokens to keep in the tokenizer’s vocabulary? (30K and 50K\\nare often used as vocabulary size values, but more and more we’re\\nseeing larger sizes like 100K.)\\nSpecial tokens\\nWhat special tokens do we want the model to keep track of? We can add\\nas many of these as we want, especially if we want to build an LLM for\\nspecial use cases. Common choices include:\\nBeginning of text token (e.g., <s>)\\nEnd of text token\\nPadding token\\nUnknown token\\nCLS token\\nMasking token', 'Aside from these, the LLM designer can add tokens that help better\\nmodel the domain of the problem they’re trying to focus on, as we’ve\\nseen with Galactica’s <work> and [START_REF] tokens.\\nCapitalization\\nIn languages such as English, how do we want to deal with\\ncapitalization? Should we convert everything to lowercase? (Name\\ncapitalization often carries useful information, but do we want to waste\\ntoken vocabulary space on all-caps versions of words?)\\nThe domain of the data\\nEven if we select the same method and parameters, tokenizer behavior will\\nbe different based on the dataset it was trained on (before we even start\\nmodel training). The tokenization methods mentioned previously work by\\noptimizing the vocabulary to represent a specific dataset. From our guided\\ntour we’ve seen how that has an impact on datasets like code and\\nmultilingual text.\\nFor code, for example, we’ve seen that a text-focused tokenizer may\\ntokenize the indentation spaces like this (we’ll highlight some tokens in\\ncolor):\\ndef add_numbers(a, b):\\n....\"\"\"Add the two numbers `a` and `b`.\"\"\"\\n....return a + b\\nThis may be suboptimal for a code-focused model. Code-focused models\\nare often improved by making different tokenization choices:\\ndef add_numbers(a, b):\\n....\"\"\"Add the two numbers `a` and `b`.\"\"\"\\n....return a + b', 'These tokenization choices make the model’s job easier and thus its\\nperformance has a higher probability of improving.\\nYou can find a more detailed tutorial on training tokenizers in the\\nTokenizers section of the Hugging Face course and in Natural Language\\nProcessing with Transformers, Revised Edition.\\nToken Embeddings\\nNow that we understand tokenization, we have solved one part of the\\nproblem of representing language to a language model. In this sense,\\nlanguage is a sequence of tokens. And if we train a good-enough model on a\\nlarge-enough set of tokens, it starts to capture the complex patterns that\\nappear in its training dataset:\\nIf the training data contains a lot of English text, that pattern\\nreveals itself as a model capable of representing and generating the\\nEnglish language.\\nIf the training data contains factual information (Wikipedia, for\\nexample), the model would have the ability to generate some\\nfactual information (see the following note).\\nThe next piece of the puzzle is finding the best numerical representation for\\nthese tokens that the model can use to calculate and properly model the\\npatterns in the text. These patterns reveal themselves to us as a model’s\\ncoherence in a specific language, or capability to code, or any of the\\ngrowing list of capabilities we expect from language models.\\nAs we’ve seen in Chapter 1, that is what embeddings are. They are the\\nnumeric representation space utilized to capture the meanings and patterns\\nin language.', 'NOTE\\nOops: Achieving a good threshold of language coherence and better-than-average\\nfactual generation, however, starts to present a new problem. Some users start to trust\\nthe model’s fact generation ability (e.g., at the beginning of 2023 some language models\\nwere being dubbed “Google killers”). It didn’t take long for advanced users to recognize\\nthat generation models alone aren’t reliable search engines. This led to the rise of\\nretrieval-augmented generation (RAG), which combines search and LLMs. We cover\\nRAG in more detail in Chapter 8.\\nA Language Model Holds Embeddings for the\\nVocabulary of Its Tokenizer\\nAfter a tokenizer is initialized and trained, it is then used in the training\\nprocess of its associated language model. This is why a pretrained language\\nmodel is linked with its tokenizer and can’t use a different tokenizer without\\ntraining.\\nThe language model holds an embedding vector for each token in the\\ntokenizer’s vocabulary, as we can see in Figure 2-7. When we download a\\npretrained language model, a portion of the model is this embeddings\\nmatrix holding all of these vectors.\\nBefore the beginning of the training process, these vectors are randomly\\ninitialized like the rest of the model’s weights, but the training process\\nassigns them the values that enable the useful behavior they’re trained to\\nperform.', 'Figure 2-7. A language model holds an embedding vector associated with each token in its tokenizer.\\nCreating Contextualized Word Embeddings with\\nLanguage Models\\nNow that we’ve covered token embeddings as the input to a language\\nmodel, let’s look at how language models can create better token\\nembeddings. This is one of the primary ways to use language models for\\ntext representation. This empowers applications like named-entity\\nrecognition or extractive text summarization (which summarizes a long text\\nby highlighting the most important parts of it, instead of generating new\\ntext as a summary).\\nInstead of representing each token or word with a static vector, language\\nmodels create contextualized word embeddings (shown in Figure 2-8) that\\nrepresent a word with a different token based on its context. These vectors\\ncan then be used by other systems for a variety of tasks. In addition to the\\ntext applications we mentioned in the previous paragraph, these\\ncontextualized vectors, for example, are what powers AI image generation\\nsystems like DALL·E, Midjourney, and Stable Diffusion, for example.', 'Figure 2-8. Language models produce contextualized token embeddings that improve on raw, static\\ntoken embeddings.\\nLet’s look at how we can generate contextualized word embeddings; the\\nmajority of this code should be familiar to you by now:\\nfrom transformers import AutoModel, AutoTokenizer\\n# Load a tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-\\nbase\")\\n# Load a language model\\nmodel = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\\n# Tokenize the sentence\\ntokens = tokenizer(\\'Hello world\\', return_tensors=\\'pt\\')\\n# Process the tokens\\noutput = model(**tokens)[0]\\nThe model we’re using here is called DeBERTa v3, which at the time of\\nwriting is one of the best-performing language models for token\\nembeddings while being small and highly efficient. It is described in the\\npaper “DeBERTaV3: Improving DeBERTa using ELECTRA-style pre-\\ntraining gradient-disentangled embedding sharing”.', \"This code downloads a pretrained tokenizer and model, then uses them to\\nprocess the string “Hello world”. The output of the model is then saved in\\nthe output variable. Let’s inspect that variable by first printing its\\ndimensions (we expect it to be a multidimensional array):\\noutput.shape\\nThis prints out:\\ntorch.Size([1, 4, 384])\\nSkipping the first dimension, we can read this as four tokens, each one\\nembedded in a vector of 384 values. The first dimension is the batch\\ndimension used in cases (like training) when we want to send multiple input\\nsentences to the model at the same time (they’re processed at the same time,\\nwhich speeds up the process).\\nBut what are these four vectors? Did the tokenizer break the two words into\\nfour tokens, or is something else happening here? We can use what we’ve\\nlearned about tokenizers to inspect them:\\nfor token in tokens['input_ids'][0]:\\n    print(tokenizer.decode(token))\\nThis prints out:\\n[CLS]\\nHello\\nworld\\n[SEP]\\nThis particular tokenizer and model operate by adding the [CLS] and\\n[SEP] tokens to the beginning and end of a string.\", 'Our language model has now processed the text input. The result of its\\noutput is the following:\\ntensor([[\\n[-3.3060, -0.0507, -0.1098, ..., -0.1704, -0.1618, 0.6932],\\n[ 0.8918, 0.0740, -0.1583, ..., 0.1869, 1.4760, 0.0751],\\n[ 0.0871, 0.6364, -0.3050, ..., 0.4729, -0.1829, 1.0157],\\n[-3.1624, -0.1436, -0.0941, ..., -0.0290, -0.1265, 0.7954]\\n]], grad_fn=<NativeLayerNormBackward0>)\\nThis is the raw output of a language model. The applications of large\\nlanguage models build on top of outputs like this.\\nWe recap the input tokenization and resulting outputs of a language model\\nin Figure 2-9. Technically, the switch from token IDs into raw embeddings\\nis the first step that occurs inside a language model.', 'Figure 2-9. A language model operates on raw, static embeddings as its input and produces\\ncontextual text embeddings.\\nA visual like this is essential for the next chapter when we start to look at\\nhow Transformer-based LLMs work.\\nText Embeddings (for Sentences and Whole\\nDocuments)\\nWhile token embeddings are key to how LLMs operate, a number of LLM\\napplications require operating on entire sentences, paragraphs, or even text\\ndocuments. This has led to special language models that produce text', 'embeddings—a single vector that represents a piece of text longer than just\\none token.\\nWe can think of text embedding models as taking a piece of text and\\nultimately producing a single vector that represents that text and captures its\\nmeaning in some useful form. Figure 2-10 shows that process.\\nFigure 2-10. In step 1, we use the embedding model to extract the features and convert the input text\\nto embeddings.\\nThere are multiple ways of producing a text embedding vector. One of the\\nmost common ways is to average the values of all the token embeddings\\nproduced by the model. Yet high-quality text embedding models tend to be\\ntrained specifically for text embedding tasks.\\nWe can produce text embeddings with sentence-transformers, a\\npopular package for leveraging pretrained embedding models.1  The\\npackage, like transformers in the previous chapter, can be used to load\\npublicly available models. To illustrate creating embeddings, we use the all-', 'mpnet-base-v2 model. Note that in Chapter 4, we will further explore how\\nyou can choose an embedding model for your task.\\nfrom sentence_transformers import SentenceTransformer\\n# Load model\\nmodel = SentenceTransformer(\"sentence-transformers/all-mpnet-\\nbase-v2\")\\n# Convert text to text embeddings\\nvector = model.encode(\"Best movie ever!\")\\nThe number of values, or the dimensions, of the embedding vector depends\\non the underlying embedding model. Let’s explore that for our model:\\nvector.shape\\n(768,)\\nThis sentence is now encoded in this one vector with a dimension of 768\\nnumerical values. In Part II of this book, once we start looking at\\napplications, we’ll start to see the immense usefulness of these text\\nembeddings vectors in powering everything from categorization to semantic\\nsearch to RAG.\\nWord Embeddings Beyond LLMs\\nEmbeddings are useful even outside of text and language generation.\\nEmbeddings, or assigning meaningful vector representations to objects,\\nturns out to be useful in many domains, including recommender engines\\nand robotics. In this section, we’ll look at how to use pretrained word2vec\\nembeddings and touch on how the method creates word embeddings.\\nSeeing how word2vec is trained will prime you to learn about contrastive\\ntraining in Chapter 10. Then in the following section, we’ll see how those\\nembeddings can be used for recommendation systems.', 'Using pretrained Word Embeddings\\nLet’s look at how we can download pretrained word embeddings (like\\nword2vec or GloVe) using the Gensim library:\\nimport gensim.downloader as api\\n# Download embeddings (66MB, glove, trained on wikipedia, vector \\nsize: 50)\\n# Other options include \"word2vec-google-news-300\"\\n# More options at https://github.com/RaRe-Technologies/gensim-\\ndata\\nmodel = api.load(\"glove-wiki-gigaword-50\")\\nHere, we’ve downloaded the embeddings of a large number of words\\ntrained on Wikipedia. We can then explore the embedding space by seeing\\nthe nearest neighbors of a specific word, “king” for example:\\nmodel.most_similar([model[\\'king\\']], topn=11)\\nThis outputs:\\n[(\\'king\\', 1.0000001192092896),\\n(\\'prince\\', 0.8236179351806641),\\n(\\'queen\\', 0.7839043140411377),\\n(\\'ii\\', 0.7746230363845825),\\n(\\'emperor\\', 0.7736247777938843),\\n(\\'son\\', 0.766719400882721),\\n(\\'uncle\\', 0.7627150416374207),\\n(\\'kingdom\\', 0.7542161345481873),\\n(\\'throne\\', 0.7539914846420288),\\n(\\'brother\\', 0.7492411136627197),\\n(\\'ruler\\', 0.7434253692626953)]\\nThe Word2vec Algorithm and Contrastive Training\\nThe word2vec algorithm described in the paper “Efficient estimation of\\nword representations in vector space” is described in detail in The', 'Illustrated Word2vec. The central ideas are condensed here as we build on\\nthem when discussing one method for creating embeddings for\\nrecommendation engines in the following section.\\nJust like LLMs, word2vec is trained on examples generated from text. Let’s\\nsay, for example, we have the text “Thou shalt not make a machine in the\\nlikeness of a human mind” from the Dune novels by Frank Herbert. The\\nalgorithm uses a sliding window to generate training examples. We can, for\\nexample, have a window size two, meaning that we consider two neighbors\\non each side of a central word.\\nThe embeddings are generated from a classification task. This task is used\\nto train a neural network to predict if words commonly appear in the same\\ncontext or not (context here means in many sentences in the training dataset\\nwe’re modeling). We can think of this as a neural network that takes two\\nwords and outputs 1 if they tend to appear in the same context, and 0 if they\\ndo not.\\nIn the first position for the sliding window, we can generate four training\\nexamples, as we can see in Figure 2-11.\\nFigure 2-11. A sliding window is used to generate training examples for the word2vec algorithm to\\nlater predict if two words are neighbors or not.\\nIn each of the produced training examples, the word in the center is used as\\none input, and each of its neighbors is a distinct second input in each\\ntraining example. We expect the final trained model to be able to classify\\nthis neighbor relationship and output 1 if the two input words it receives are\\nindeed neighbors. These training examples are visualized in Figure 2-12.', 'Figure 2-12. Each generated training example shows a pair of neighboring words.\\nIf, however, we have a dataset of only a target value of 1, then a model can\\ncheat and ace it by outputting 1 all the time. To get around this, we need to\\nenrich our training dataset with examples of words that are not typically\\nneighbors. These are called negative examples and are shown in Figure 2-\\n13.\\nFigure 2-13. We need to present our models with negative examples: words that are not usually\\nneighbors. A better model is able to better distinguish between the positive and negative examples.\\nIt turns out that we don’t have to be too scientific in how we choose the\\nnegative examples. A lot of useful models result from the simple ability to\\ndetect positive examples from randomly generated examples (inspired by an', 'important idea called noise-contrastive estimation and described in “Noise-\\ncontrastive estimation: A new estimation principle for unnormalized\\nstatistical models”). So in this case, we get random words and add them to\\nthe dataset and indicate that they are not neighbors (and thus the model\\nshould output 0 when it sees them).\\nWith this, we’ve seen two of the main concepts of word2vec (Figure 2-14):\\nskip-gram, the method of selecting neighboring words, and negative\\nsampling, adding negative examples by random sampling from the dataset.\\nFigure 2-14. Skip-gram and negative sampling are two of the main ideas behind the word2vec\\nalgorithm and are useful in many other problems that can be formulated as token sequence problems.\\nWe can generate millions and even billions of training examples like this\\nfrom running text. Before proceeding to train a neural network on this\\ndataset, we need to make a couple of tokenization decisions, which, just like\\nwe’ve seen with LLM tokenizers, include how to deal with capitalization\\nand punctuation and how many tokens we want in our vocabulary.\\nWe then create an embedding vector for each token, and randomly initialize\\nthem, as can be seen in Figure 2-15. In practice, this is a matrix of\\ndimensions vocab_size x embedding_dimensions.', 'Figure 2-15. A vocabulary of words and their starting, random, uninitialized embedding vectors.\\nA model is then trained on each example to take in two embedding vectors\\nand predict if they’re related or not. We can see what this looks like in\\nFigure 2-16.\\nFigure 2-16. A neural network is trained to predict if two words are neighbors. It updates the\\nembeddings in the training process to produce the final, trained embeddings.\\nBased on whether its prediction was correct or not, the typical machine\\nlearning training step updates the embeddings so that the next time the\\nmodel is presented with those two vectors, it has a better chance of being', 'more correct. And by the end of the training process, we have better\\nembeddings for all the tokens in our vocabulary.\\nThis idea of a model that takes two vectors and predicts if they have a\\ncertain relation is one of the most powerful ideas in machine learning, and\\ntime after time has proven to work very well with language models. This is\\nwhy we’re dedicating Chapter 10 to this concept and how it optimizes\\nlanguage models for specific tasks (like sentence embeddings and retrieval).\\nThe same idea is also central to bridging modalities like text and images,\\nwhich is key to AI image generation models, as we’ll see in Chapter 9 on\\nmultimodal models. In that formulation, a model is presented with an image\\nand a caption, and it should predict whether that caption describes the\\nimage or not.\\nEmbeddings for Recommendation Systems\\nAs we’ve mentioned, the concept of embeddings is useful in so many other\\ndomains. In industry, it’s widely used for recommendation systems, for\\nexample.\\nRecommending Songs by Embeddings\\nIn this section we’ll use the word2vec algorithm to embed songs using\\nhuman-made music playlists. Imagine if we treated each song as we would\\na word or token, and we treated each playlist like a sentence. These\\nembeddings can then be used to recommend similar songs that often appear\\ntogether in playlists.\\nThe dataset we’ll use was collected by Shuo Chen from Cornell University.\\nIt contains playlists from hundreds of radio stations around the US.\\nFigure 2-17 demonstrates this dataset.', 'Figure 2-17. For song embeddings that capture song similarity we’ll use a dataset made up of a\\ncollection of playlists, each containing a list of songs.\\nLet’s demonstrate the end product before we look at how it’s built. So let’s\\ngive it a few songs and see what it recommends in response.\\nLet’s start by giving it Michael Jackson’s “Billie Jean,” the song with ID\\n3822:\\n# We will define and explore this function in detail below\\nprint_recommendations(3822)\\nid Title artist\\n4181 Kiss Prince & The\\nRevolution\\n12749 Wanna Be Startin’\\nSomethin’\\nMichael Jackson\\n1506 The Way You Make Me\\nFeel\\nMichael Jackson\\n3396 Holiday Madonna\\n500 Don’t Stop ‘Til You Get\\nEnough\\nMichael Jackson', 'That looks reasonable. Madonna, Prince, and other Michael Jackson songs\\nare the nearest neighbors.\\nLet’s step away from pop and into rap, and see the neighbors of 2Pac’s\\n“California Love”:\\nprint_recommendations(842)\\nid Title artist\\n413 If I Ruled the World\\n(Imagine That) (w\\\\/ Lauryn\\nHill)\\nNas\\n196 I’ll Be Missing You Puff Daddy & The\\nFamily\\n330 Hate It or Love It (w\\\\/ 50\\nCent)\\nThe Game\\n211 Hypnotize The Notorious\\nB.I.G.\\n5788 Drop It Like It’s Hot (w\\\\/\\nPharrell)\\nSnoop Dogg\\nAnother quite reasonable list! Now that we know it works, let’s see how to\\nbuild such a system.\\nTraining a Song Embedding Model\\nWe’ll start by loading the dataset containing the song playlists as well as\\neach song’s metadata, such as its title and artist:', 'import pandas as pd\\nfrom urllib import request\\n# Get the playlist dataset file\\ndata = request.urlopen(\\'https://storage.googleapis.com/maps-\\npremium/dataset/yes_complete/train.txt\\')\\n# Parse the playlist dataset file. Skip the first two lines as\\n# they only contain metadata\\nlines = data.read().decode(\"utf-8\").split(\\'\\\\n\\')[2:]\\n# Remove playlists with only one song\\nplaylists = [s.rstrip().split() for s in lines if len(s.split()) \\n> 1]\\n# Load song metadata\\nsongs_file = \\nrequest.urlopen(\\'https://storage.googleapis.com/maps-\\npremium/dataset/yes_complete/song_hash.txt\\')\\nsongs_file = songs_file.read().decode(\"utf-8\").split(\\'\\\\n\\')\\nsongs = [s.rstrip().split(\\'\\\\t\\') for s in songs_file]\\nsongs_df = pd.DataFrame(data=songs, columns = [\\'id\\', \\'title\\', \\n\\'artist\\'])\\nsongs_df = songs_df.set_index(\\'id\\')\\nNow that we’ve saved them, let’s inspect the playlists list. Each\\nelement inside it is a playlist containing a list of song IDs:\\nprint( \\'Playlist #1:\\\\n \\', playlists[0], \\'\\\\n\\')\\nprint( \\'Playlist #2:\\\\n \\', playlists[1])\\nPlaylist #1: [\\'0\\', \\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', ..., \\'43\\']\\nPlaylist #2: [\\'78\\', \\'79\\', \\'80\\', \\'3\\', \\'62\\', ..., \\'210\\']\\nLet’s train the model:\\nfrom gensim.models import Word2Vec\\n# Train our Word2Vec model\\nmodel = Word2Vec(\\n    playlists, vector_size=32, window=20, negative=50, ', \"min_count=1, workers=4\\n)\\nThat takes a minute or two to train and results in embeddings being\\ncalculated for each song that we have. Now we can use those embeddings\\nto find similar songs exactly as we did earlier with words:\\nsong_id = 2172\\n# Ask the model for songs similar to song #2172\\nmodel.wv.most_similar(positive=str(song_id))\\nThis outputs:\\n[('2976', 0.9977465271949768),\\n ('3167', 0.9977430701255798),\\n ('3094', 0.9975950717926025),\\n ('2640', 0.9966474175453186),\\n ('2849', 0.9963167905807495)]\\nThat is the list of the songs whose embeddings are most similar to song\\n2172.\\nIn this case, the song is:\\nprint(songs_df.iloc[2172])\\ntitle Fade To Black\\nartist Metallica\\nName: 2172 , dtype: object\\nThis results in recommendations that are all in the same heavy metal and\\nhard rock genre:\\nimport numpy as np\", 'def print_recommendations(song_id):\\n    similar_songs = np.array(\\n        model.wv.most_similar(positive=str(song_id),topn=5)\\n    )[:,0]\\n    return  songs_df.iloc[similar_songs]\\n# Extract recommendations\\nprint_recommendations(2172)\\nid Title artist\\n11473 Little Guitars Van Halen\\n3167 Unchained Van Halen\\n5586 The Last in LineDio\\n5634 Mr. BrownstoneGuns N’ Roses\\n3094 Breaking the LawJudas Priest\\nSummary\\nIn this chapter, we have covered LLM tokens, tokenizers, and useful\\napproaches to using token embeddings. This prepares us to start looking\\ncloser at language models in the next chapter, and also opens the door to\\nlearn about how embeddings are used beyond language models.\\nWe explored how tokenizers are the first step in processing input to an\\nLLM, transforming raw textual input into token IDs. Common tokenization\\nschemes include breaking text down into words, subword tokens,\\ncharacters, or bytes, depending on the specific requirements of a given\\napplication.', 'A tour of real-world pretrained tokenizers (from BERT to GPT-2, GPT-4,\\nand other models) showed us areas where some tokenizers are better (e.g.,\\npreserving information like capitalization, newlines, or tokens in other\\nlanguages) and other areas where tokenizers are just different from each\\nother (e.g., how they break down certain words).\\nThree of the major tokenizer design decisions are the tokenizer algorithm\\n(e.g., BPE, WordPiece, SentencePiece), tokenization parameters (including\\nvocabulary size, special tokens, capitalization, treatment of capitalization\\nand different languages), and the dataset the tokenizer is trained on.\\nLanguage models are also creators of high-quality contextualized token\\nembeddings that improve on raw static embeddings. Those contextualized\\ntoken embeddings are what’s used for tasks including named-entity\\nrecognition (NER), extractive text summarization, and text classification. In\\naddition to producing token embeddings, language models can produce text\\nembeddings that cover entire sentences or even documents. This empowers\\nplenty of applications that will be shown in Part II of this book covering\\nlanguage model applications\\nBefore LLMs, word embedding methods like word2vec, GloVe, and\\nfastText were popular. In language processing, this has largely been\\nreplaced with contextualized word embeddings produced by language\\nmodels. The word2vec algorithm relies on two main ideas: skip-gram and\\nnegative sampling. It also uses contrastive training similar to the type we’ll\\nsee in Chapter 10.\\nEmbeddings are useful for creating and improving recommender systems as\\nwe discussed in the music recommender we built from curated song\\nplaylists.\\nIn the next chapter, we will take a deep dive into the process after\\ntokenization: how does an LLM process these tokens and generate text? We\\nwill look at some of the main intuitions of how LLMs that use the\\nTransformer architecture work.', '1  Nils Reimers and Iryna Gurevych. “Sentence-BERT: Sentence embeddings using Siamese\\nBERT-networks.” arXiv preprint arXiv:1908.10084 (2019).\\nOceanofPDF.com', 'Chapter 3. Looking Inside Large\\nLanguage Models\\nNow that we have a sense of tokenization and embeddings, we’re ready to\\ndive deeper into the language model and see how it works. In this chapter,\\nwe’ll look at some of the main intuitions of how Transformer language\\nmodels work. Our focus will be on text generation models so we get a\\ndeeper sense for generative LLMs in particular.\\nWe’ll be looking at both the concepts and some code examples that\\ndemonstrate them. Let’s start by loading a language model and getting it\\nready for generation by declaring a pipeline. In your first read, feel free to\\nskip the code and focus on grasping the concepts involved. Then in a\\nsecond read, the code will get you to start applying these concepts.\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, \\npipeline\\n# Load model and tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-\\n4k-instruct\")\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    device_map=\"cuda\",\\n    torch_dtype=\"auto\",\\n    trust_remote_code=True,\\n)\\n# Create a pipeline\\ngenerator = pipeline(\\n    \"text-generation\",\\n    model=model,\\n    tokenizer=tokenizer,\\n    return_full_text=False,', '    max_new_tokens=50,\\n    do_sample=False,\\n)\\nAn Overview of Transformer Models\\nLet’s begin our exploration with a high-level overview of the model, and\\nthen we’ll see how later work has improved upon the Transformer model\\nsince its introduction in 2017.\\nThe Inputs and Outputs of a Trained Transformer LLM\\nThe most common picture of understanding the behavior of a Transformer\\nLLM is to think of it as a software system that takes in text and generates\\ntext in response. Once a large enough text-in-text-out model is trained on a\\nlarge enough high-quality dataset, it becomes able to generate impressive\\nand useful outputs. Figure 3-1 shows one such model used to author an\\nemail.', 'Figure 3-1. At a high level of abstraction, Transformer LLMs take a text prompt and output\\ngenerated text.\\nThe model does not generate the text all in one operation; it actually\\ngenerates one token at a time. Figure 3-2 shows four steps of token\\ngeneration in response to the input prompt. Each token generation step is\\none forward pass through the model (that’s machine-learning speak for the\\ninputs going into the neural network and flowing through the computations\\nit needs to produce an output on the other end of the computation graph).', 'Figure 3-2. Transformer LLMs generate one token at a time, not the entire text at once.\\nAfter each token generation, we tweak the input prompt for the next\\ngeneration step by appending the output token to the end of the input\\nprompt. We can see this in Figure 3-3.', 'Figure 3-3. An output token is appended to the prompt, then this new text is presented to the model\\nagain for another forward pass to generate the next token.\\nThis gives us a more accurate picture of the model as it is simply predicting\\nthe next token based on an input prompt. Software around the neural\\nnetwork basically runs it in a loop to sequentially expand the generated text\\nuntil completion.\\nThere’s a specific word used in machine learning to describe models that\\nconsume their earlier predictions to make later predictions (e.g., the model’s\\nfirst generated token is used to generate the second token). They’re called\\nautoregressive models. That is why you’ll hear text generation LLMs being\\ncalled autoregressive models. This is often used to differentiate text\\ngeneration models from text representation models like BERT, which are\\nnot autoregressive.\\nThis autoregressive, token-by-token generation is what happens under the\\nhood when we generate text with the LLM like we see here:\\nprompt = \"Write an email apologizing to Sarah for the tragic \\ngardening mishap. Explain how it happened.\"\\noutput = generator(prompt)\\nprint(output[0][\\'generated_text\\'])\\nThis generates the text:\\nSolution 1:', 'Subject: My Sincere Apologies for the Gardening Mishap\\nDear Sarah,\\nI hope this message finds you well. I am writing to express my \\ndeep\\nWe can see the model begin to write the email starting with the subject. It\\nstopped abruptly because it reached the token limit we established by\\nsetting max_new_tokens to 50 tokens. If we increase that, it will\\ncontinue until concluding the email.\\nThe Components of the Forward Pass\\nIn addition to the loop, two key internal components are the tokenizer and\\nthe language modeling head (LM head). Figure 3-4 shows where these\\ncomponents lie in the system. We saw in the previous chapter how\\ntokenizers break down the text into a sequence of token IDs that then\\nbecome the input to the model.\\nThe tokenizer is followed by the neural network: a stack of Transformer\\nblocks that do all of the processing. That stack is then followed by the LM\\nhead, which translates the output of the stack into probability scores for\\nwhat the most likely next token is.\\nFigure 3-4. A Transformer LLM is made up of a tokenizer, a stack of Transformer blocks, and a\\nlanguage modeling head.', 'Recall from Chapter 2 that the tokenizer contains a table of tokens—the\\ntokenizer’s vocabulary. The model has a vector representation associated\\nwith each of these tokens in the vocabulary (token embeddings). Figure 3-5\\nshows both the vocabulary and associated token embeddings for a model\\nwith a vocabulary of 50,000 tokens.\\nFigure 3-5. The tokenizer has a vocabulary of 50,000 tokens. The model has token embeddings\\nassociated with those embeddings.\\nThe flow of the computation follows the direction of the arrow from top to\\nbottom. For each generated token, the process flows once through each of\\nthe Transformer blocks in the stack in order, then to the LM head, which\\nfinally outputs the probability distribution for the next token, seen in\\nFigure 3-6.', 'Figure 3-6. At the end of the forward pass, the model predicts a probability score for each token in\\nthe vocabulary.\\nThe LM head is a simple neural network layer itself. It is one of multiple\\npossible “heads” to attach to a stack of Transformer blocks to build different\\nkinds of systems. Other kinds of Transformer heads include sequence\\nclassification heads and token classification heads.\\nWe can display the order of the layers by simply printing out the model\\nvariable. For this model, we have:\\nPhi3ForCausalLM(\\n  (model): Phi3Model(\\n    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\\n    (embed_dropout): Dropout(p=0.0, inplace=False)\\n    (layers): ModuleList(\\n      (0-31): 32 x Phi3DecoderLayer(\\n        (self_attn): Phi3Attention(\\n          (o_proj): Linear(in_features=3072, out_features=3072, \\nbias=False)\\n          (qkv_proj): Linear(in_features=3072, \\nout_features=9216, bias=False)\\n          (rotary_emb): Phi3RotaryEmbedding()\\n        )\\n        (mlp): Phi3MLP(\\n          (gate_up_proj): Linear(in_features=3072, \\nout_features=16384, bias=False)\\n          (down_proj): Linear(in_features=8192, ', 'out_features=3072, bias=False)\\n          (activation_fn): SiLU()\\n        )\\n        (input_layernorm): Phi3RMSNorm()\\n        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\\n        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\\n        (post_attention_layernorm): Phi3RMSNorm()\\n      )\\n    )\\n    (norm): Phi3RMSNorm()\\n  )\\n  (lm_head): Linear(in_features=3072, out_features=32064, \\nbias=False)\\n)\\nLooking at this structure, we can notice the following highlights:\\nThis shows us the various nested layers of the model. The majority\\nof the model is labeled model, followed by lm_head.\\nInside the Phi3Model model, we see the embeddings matrix\\nembed_tokens and its dimensions. It has 32,064 tokens each\\nwith a vector size of 3,072.\\nSkipping the dropout layer for now, we can see the next major\\ncomponent is the stack of Transformer decoder layers. It contains\\n32 blocks of type Phi3DecoderLayer.\\nEach of these Transformer blocks includes an attention layer and a\\nfeedforward neural network (also known as an mlp or multilevel\\nperceptron). We’ll cover these in more detail later in the chapter.\\nFinally, we see the lm_head taking a vector of size 3,072 and\\noutputting a vector equivalent to the number of tokens the model\\nknows. That output is the probability score for each token that\\nhelps us select the output token.\\nChoosing a Single Token from the Probability', 'Distribution (Sampling/Decoding)\\nAt the end of processing, the output of the model is a probability score for\\neach token in the vocabulary, as we saw previously in Figure 3-6. The\\nmethod of choosing a single token from the probability distribution is called\\nthe decoding strategy. Figure 3-7 shows how this leads to picking the token\\n“Dear” in one example.\\nThe easiest decoding strategy would be to always pick the token with the\\nhighest probability score. In practice, this doesn’t tend to lead to the best\\noutputs for most use cases. A better approach is to add some randomness\\nand sometimes choose the second or third highest probability token. The\\nidea here is to basically sample from the probability distribution based on\\nthe probability score, as the statisticians would say.\\nWhat this means for the example in Figure 3-7 is that if the token “Dear”\\nhas a 40% probability of being the next token, then it has a 40% chance of\\nbeing picked (instead of greedy search, which would pick it directly for\\nhaving the highest score). So with this method, all the other tokens have a\\nchance of being picked according to their score.\\nFigure 3-7. The tokens with the highest probability after the model’s forward pass. Our decoding\\nstrategy decides which of the tokens to output by sampling based on the probabilities.\\nChoosing the highest scoring token every time is called greedy decoding.\\nIt’s what happens if you set the temperature parameter to zero in an LLM.', 'We cover the concept of temperature in Chapter 6.\\nLet’s look more closely at the code that demonstrates this process. In this\\ncode block, we pass the input tokens through the model, and then\\nlm_head:\\nprompt = \"The capital of France is\"\\n# Tokenize the input prompt\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\n# Tokenize the input prompt\\ninput_ids = input_ids.to(\"cuda\")\\n# Get the output of the model before the lm_head\\nmodel_output = model.model(input_ids)\\n# Get the output of the lm_head\\nlm_head_output = model.lm_head(model_output[0])\\nNow, lm_head_output is of the shape [1, 6, 32064]. We can access the\\ntoken probability scores for the last generated token using\\nlm_head_output[0,-1], which uses the index 0 across the batch\\ndimension; the index –1 gets us the last token in the sequence. This is now a\\nlist of probability scores for all 32,064 tokens. We can get the top scoring\\ntoken ID, and then decode it to arrive at the text of the generated output\\ntoken:\\ntoken_id = lm_head_output[0,-1].argmax(-1)\\ntokenizer.decode(token_id)\\nIn this case this turns out to be:\\nParis', 'Parallel Token Processing and Context Size\\nOne of the most compelling features of Transformers is that they lend\\nthemselves better to parallel computing than previous neural network\\narchitectures in language processing. In text generation, we get a first\\nglance at this when looking at how each token is processed. We know from\\nthe previous chapter that the tokenizer will break down the text into tokens.\\nEach of these input tokens then flows through its own computation path\\n(that’s a good first intuition, at least). We can see these individual\\nprocessing tracks or streams in Figure 3-8.\\nFigure 3-8. Each token is processed through its own stream of computation (with some interaction\\nbetween them in attention steps, as we’ll later see).', 'Current Transformer models have a limit for how many tokens they can\\nprocess at once. That limit is called the model’s context length. A model\\nwith 4K context length can only process 4K tokens and would only have 4K\\nof these streams.\\nEach of the token streams starts with an input vector (the embedding vector\\nand some positional information; we’ll discuss positional embeddings later\\nin the chapter). At the end of the stream, another vector emerges as the\\nresult of the model’s processing, as shown in Figure 3-9.\\nFigure 3-9. Each processing stream takes a vector as input and produces a final resulting vector of\\nthe same size (often referred to as the model dimension).\\nFor text generation, only the output result of the last stream is used to\\npredict the next token. That output vector is the only input into the LM head\\nas it calculates the probabilities of the next token.', 'You may wonder why we go through the trouble of calculating all the token\\nstreams if we’re discarding the outputs of all but the last token. The answer\\nis that the calculations of the previous streams are required and used in\\ncalculating the final stream. Yes, we’re not using their final output vector,\\nbut we use earlier outputs (in each Transformer block) in the Transformer\\nblock’s attention mechanism.\\nIf you’re following along with the code examples, recall that the output of\\nlm_head was of the shape [1, 6, 32064]. That was because the input to it\\nwas of the shape [1, 6, 3072], which is a batch of one input string,\\ncontaining six tokens, each of them represented by a vector of size 3,072\\ncorresponding to the output vectors after the stack of Transformer blocks.\\nWe can access these matrices and view their dimensions by printing:\\nmodel_output[0].shape\\nThis outputs:\\ntorch.Size([1, 6, 3072])\\nSimilarly, we can print the output of the LM head:\\nlm_head_output.shape\\nThis outputs:\\ntorch.Size([1, 6, 32064])\\nSpeeding Up Generation by Caching Keys and Values\\nRecall that when generating the second token, we simply append the output\\ntoken to the input and do another forward pass through the model. If we', 'give the model the ability to cache the results of the previous calculation\\n(especially some of the specific vectors in the attention mechanism), we no\\nlonger need to repeat the calculations of the previous streams. This time the\\nonly needed calculation is for the last stream. This is an optimization\\ntechnique called the keys and values (kv) cache and it provides a significant\\nspeedup of the generation process. Keys and values are some of the central\\ncomponents of the attention mechanism, as we’ll see later in this chapter.\\nFigure 3-10 shows how when generating the second token, only one\\nprocessing stream is active as we cache the results of the previous streams.\\nFigure 3-10. When generating text, it’s important to cache the computation results of previous tokens\\ninstead of repeating the same calculation over and over again.\\nIn Hugging Face Transformers, cache is enabled by default. We can disable\\nit by setting use_cache to False. We can see the difference in speed by', 'asking for a long generation, and timing the generation with and without\\ncaching:\\nprompt = \"Write a very long email apologizing to Sarah for the \\ntragic gardening mishap. Explain how it happened.\"\\n# Tokenize the input prompt\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\ninput_ids = input_ids.to(\"cuda\")\\nThen we time how long it takes to generate 100 tokens with caching. We\\ncan use the %%timeit magic command in Jupyter or Colab to time how\\nlong the execution takes (it runs the command several times and gets the\\naverage):\\n%%timeit -n 1\\n# Generate the text\\ngeneration_output = model.generate(\\n  input_ids=input_ids,\\n  max_new_tokens=100,\\n  use_cache=True\\n)\\nOn a Colab with a T4 GPU, this comes to 4.5 seconds. How long would that\\ntake if we disable the cache, however?\\n%%timeit -n 1\\n# Generate the text\\ngeneration_output = model.generate(\\n  input_ids=input_ids,\\n  max_new_tokens=100,\\n  use_cache=False\\n)\\nThis comes out to 21.8 seconds. A dramatic difference. In fact, from a user\\nexperience standpoint, even the four-second generation time tends to be a\\nlong time to wait for a user that’s staring at a screen and waiting for an\\noutput from the model. This is one reason why LLM APIs stream the output', 'tokens as the model generates them instead of waiting for the entire\\ngeneration to be completed.\\nInside the Transformer Block\\nWe can now talk about where the vast majority of processing happens: the\\nTransformer blocks. As Figure 3-11 shows, Transformer LLMs are\\ncomposed of a series Transformer blocks (often in the range of six in the\\noriginal Transformer paper, to over a hundred in many large LLMs). Each\\nblock processes its inputs, then passes the results of its processing to the\\nnext block.\\nFigure 3-11. The bulk of the Transformer LLM processing happens inside a series of Transformer\\nblocks, each handing the result of its processing as input to the subsequent block.\\nA Transformer block (Figure 3-12) is made up of two successive\\ncomponents:', '1. The attention layer is mainly concerned with incorporating\\nrelevant information from other input tokens and positions\\n2. The feedforward layer houses the majority of the model’s\\nprocessing capacity\\nFigure 3-12. A Transformer block is made up of a self-attention layer and a feedforward neural\\nnetwork.\\nThe feedforward neural network at a glance\\nA simple example giving the intuition of the feedforward neural network\\nwould be if we pass the simple input “The Shawshank” to a language\\nmodel, with the expectation that it will generate “Redemption” as the most\\nprobable next word (in reference to the film from 1994).\\nThe feedforward neural network (collectively in all the model layers) is the\\nsource of this information, as Figure 3-13 shows. When the model was\\nsuccessfully trained to model a massive text archive (which included many\\nmentions of “The Shawshank Redemption”), it learned and stored the\\ninformation (and behaviors) that make it succeed at this task.', 'Figure 3-13. The feedforward neural network component of a Transformer block likely does the\\nmajority of the model’s memorization and interpolation.\\nFor an LLM to be successfully trained, it needs to memorize a lot of\\ninformation. But it is not simply a large database. Memorization is only one\\ningredient in the recipe of impressive text generation. The model is able to\\nuse this same machinery to interpolate between data points and more\\ncomplex patterns to be able to generalize—which means doing well on\\ninputs it hadn’t seen in the past and were not in its training dataset.', 'NOTE\\nWhen you use a modern commercial LLM, the outputs you get are not the ones\\nmentioned earlier in the strict meaning of a “language model.” Passing “The\\nShawshank” to a chat LLM like GPT-4 produces an output:\\n\"The Shawshank Redemption\" is a 1994 film directed by \\nFrank Darabont and is based on the novella \"Rita \\nHayworth and Shawshank Redemption\" written by Stephen \\nKing. ...etc.\\nThis is because raw language models (like GPT-3) are difficult for people to properly\\nutilize. This is why the language model is then trained on instruction-tuning and human\\npreference and feedback fine-tuning to match people’s expectations of what the model\\nshould output.\\nThe attention layer at a glance\\nContext is vital in order to properly model language. Simple memorization\\nand interpolation based on the previous token can only take us so far. We\\nknow that because this was one of the leading approaches to build language\\nmodels before neural networks (see Chapter 3, “N-gram Language Models”\\nof Speech and Language Processing by Daniel Jurafsky and James H.\\nMartin).\\nAttention is a mechanism that helps the model incorporate context as it’s\\nprocessing a specific token. Think of the following prompt:\\n“The dog chased the squirrel because it ”\\nFor the model to predict what comes after “it,” it needs to know what “it”\\nrefers to. Does it refer to the dog or the squirrel?\\nIn a trained Transformer LLM, the attention mechanism makes that\\ndetermination. Attention adds information from the context into the\\nrepresentation of the “it” token. We can see a simple version of that in\\nFigure 3-14.', 'Figure 3-14. The self-attention layer incorporates relevant information from previous positions that\\nhelp process the current token.\\nThe model does that based on the patterns seen and learned from the\\ntraining dataset. Perhaps previous sentences also give more clues, like, for\\nexample, referring to the dog as “she” thus making it clear that “it” refers to\\nthe squirrel.\\nAttention is all you need\\nIt is worth diving deeper into the attention mechanism. The most stripped-\\ndown version of the mechanism is shown in Figure 3-15. It shows multiple\\ntoken positions going into the attention layer; the final one is the one being\\ncurrently processed (the pink arrow). The attention mechanism operates on\\nthe input vector at that position. It incorporates relevant information from\\nthe context into the vector it produces as the output for that position.', 'Figure 3-15. A simplified framing of attention: an input sequence and a current position being\\nprocessed. As we’re mainly concerned with this position, the figure shows an input vector and an\\noutput vector that incorporates information from the previous elements in the sequence according to\\nthe attention mechanism.\\nTwo main steps are involved in the attention mechanism:\\n1. A way to score how relevant each of the previous input tokens are\\nto the current token being processed (in the pink arrow).\\n2. Using those scores, we combine the information from the various\\npositions into a single output vector.\\nFigure 3-16 shows these two steps.\\nFigure 3-16. Attention is made up of two major steps: relevance scoring for each position, then a step\\nwhere we combine the information based on those scores.', 'To give the Transformer more extensive attention capability, the attention\\nmechanism is duplicated and executed multiple times in parallel. Each of\\nthese parallel applications of attention is conducted into an attention head.\\nThis increases the model’s capacity to model complex patterns in the input\\nsequence that require paying attention to different patterns at once.\\nFigure 3-17 shows the intuition of how attention heads run in parallel with a\\npreceding step of splitting information and a later step of combining the\\nresults of all the heads.\\nFigure 3-17. We get better LLMs by doing attention multiple times in parallel, increasing the model’s\\ncapacity to attend to different types of information.\\nHow attention is calculated\\nLet’s look at how attention is calculated inside a single attention head.\\nBefore we start the calculation, let’s observe the following as the starting\\nposition:\\nThe attention layer (of a generative LLM) is processing attention\\nfor a single position.\\nThe inputs to the layer are:\\nThe vector representation of the current position or token\\nThe vector representations of the previous tokens', 'The goal is to produce a new representation of the current position\\nthat incorporates relevant information from the previous tokens:\\nFor example, if we’re processing the last position in the\\nsentence “Sarah fed the cat because it,” we want “it” to\\nrepresent the cat—so attention bakes in “cat information”\\nfrom the cat token.\\nThe training process produces three projection matrices that\\nproduce the components that interact in this calculation:\\nA query projection matrix\\nA key projection matrix\\nA value projection matrix\\nFigure 3-18 shows the starting position for all of these components before\\nthe attention calculations start. For simplicity, let’s look at only one\\nattention head because the other heads have identical calculations but with\\ntheir individual projection matrices.', 'Figure 3-18. Before starting the self-attention calculation, we have the inputs to the layer and\\nprojection matrices for queries, keys, and values.\\nAttention starts by multiplying the inputs by the projection matrices to\\ncreate three new matrices. These are called the queries, keys, and values\\nmatrices. These matrices contain the information of the input tokens\\nprojected to three different spaces that help carry out the two steps of\\nattention:\\n1. Relevance scoring\\n2. Combining information\\nFigure 3-19 shows these three new matrices, and how the bottom row of all\\nthree matrices is associated with the current position while the rows above\\nit are associated with the previous positions.', 'Figure 3-19. Attention is carried out by the interaction of the queries, keys, and values matrices.\\nThose are produced by multiplying the layer’s inputs with the projection matrices.\\nSelf-attention: Relevance scoring\\nIn a generative Transformer, we’re generating one token at a time. This\\nmeans we’re processing one position at a time. So the attention mechanism\\nhere is only concerned with this one position, and how information from\\nother positions can be pulled in to inform this position.\\nThe relevance scoring step of attention is conducted by multiplying the\\nquery vector of the current position with the keys matrix. This produces a\\nscore stating how relevant each previous token is. Passing that by a softmax\\noperation normalizes these scores so they sum up to 1. Figure 3-20 shows\\nthe relevance score resulting from this calculation.', 'Figure 3-20. Scoring the relevance of previous tokens is accomplished by multiplying the query\\nassociated with the current position with the keys matrix.\\nSelf-attention: Combining information\\nNow that we have the relevance scores, we multiply the value vector\\nassociated with each token by that token’s score. Summing up those\\nresulting vectors produces the output of this attention step, as we see in\\nFigure 3-21.', 'Figure 3-21. Attention combines the relevant information of previous positions by multiplying their\\nrelevance scores by their respective value vectors.\\nRecent Improvements to the Transformer\\nArchitecture\\nSince the release of the Transformer architecture, much work has been done\\nto improve it and create better models. This spans training on larger datasets\\nand optimizations for the training process and learning rates to use, but it\\nalso extends to the architecture itself. At the time of writing, a lot of the\\nideas of the original Transformer stand unchanged. There are a few\\narchitectural ideas that have proved to be valuable. They contribute to the\\nperformance of more recent Transformer models like Llama 2. In this final\\nsection of the chapter, we go over a number of the important recent\\ndevelopments of the Transformer architecture.', 'More Efficient Attention\\nThe area that gets the most focus from the research community is the\\nattention layer of the Transformer. This is because the attention calculation\\nis the most computationally expensive part of the process.\\nLocal/sparse attention\\nAs Transformers started getting larger, ideas like sparse attention\\n(“Generating long sequences with sparse transformers”) and sliding\\nwindow attention (“Longformer: The long-document transformer”)\\nprovided improvements for the efficiency of the attention calculation.\\nSparse attention limits the context of previous tokens that the model can\\nattend to, as we can see in Figure 3-22.\\nFigure 3-22. Local attention boosts performance by only paying attention to a small number of\\nprevious positions.\\nOne model that incorporates such a mechanism is GPT-3. But it does not\\nuse that for all the Transformer blocks—the quality of the generation would\\nvastly degrade if the model could only see a small number of previous\\ntokens. The GPT-3 architecture interweaved full-attention and efficient-\\nattention Transformer blocks. So the Transformer blocks alternate between\\nfull attention (e.g., blocks 1 and 3) and sparse attention (e.g., blocks 2 and\\n4).\\nTo demonstrate different kinds of attention, review Figure 3-23, which\\nshows how different attention mechanisms work. Each figure shows which\\nprevious tokens (light blue) can be attended to when processing the current\\ntoken (in dark blue).', 'Figure 3-23. Full attention versus sparse attention. Figure 3-24 explains the coloring. (Source:\\n“Generating long sequences with sparse transformers”.)\\nEach row corresponds to a token being processed. The color coding\\nindicates which tokens the model is able to pay attention to while it’s\\nprocessing the token in the dark blue cell. Figure 3-24 describes this with\\nmore clarity.', 'Figure 3-24. Attention figures show which token is being processed, and which previous tokens an\\nattention mechanism allows it to attend to.\\nThis figure also shows the autoregressive nature of decoder Transformer\\nblocks (which make up most text generation models); they can only pay\\nattention to previous tokens. Contrast this to BERT, which can pay attention\\nto both sides (hence the B in BERT stands for bidirectional).\\nMulti-query and grouped-query attention\\nA more recent efficient attention tweak to the Transformer is grouped-query\\nattention (“GQA: Training generalized multi-query transformer models\\nfrom multi-head checkpoints”), which is used by models like Llama 2 and', '3. Figure 3-25 shows these different types of attention, and the next section\\ncontinues to explain them.\\nFigure 3-25. A comparison of different kinds of attention: the original multi-head, grouped-query\\nattention, and multi-query attention (source: “Fast transformer decoding: One write-head is all you\\nneed”).\\nGrouped-query attention builds on multi-query attention (“Fast transformer\\ndecoding: One write-head is all you need”). These methods improve\\ninference scalability of larger models by reducing the size of the matrices\\ninvolved.\\nOptimizing attention: From multi-head to multi-query to\\ngrouped query\\nEarlier in the chapter we showed how the Transformer paper described\\nmulti-headed attention. The Illustrated Transformer discusses in detail how\\nthe queries, keys, and values matrices are used to conduct the attention\\noperation. Figure 3-26 shows how each “attention head” has its own distinct\\nquery, key, and value matrices calculated for a given input.\\nThe way that multi-query attention optimizes this is to share the keys and\\nvalues matrices between all the heads. So the only unique matrices for each\\nhead would be the queries matrices, as we can see in Figure 3-27.', 'Figure 3-26. Attention is conducted using matrices of queries, keys, and values. In multi-head\\nattention, each head has a distinct version of each of these matrices.', 'Figure 3-27. Multi-query attention presents a more efficient attention mechanism by sharing the keys\\nand values matrices across all the attention heads.\\nAs model sizes grow, however, this optimization can be too punishing and\\nwe can afford to use a little more memory to improve the quality of the\\nmodels. This is where grouped-query attention comes in. Instead of cutting\\nthe number of keys and values matrices to one of each, it allows us to use\\nmore (but less than the number of heads). Figure 3-28 shows these groups\\nand how each group of attention heads shares keys and values matrices.', 'Figure 3-28. Grouped-query attention sacrifices a little bit of the efficiency of multi-query attention\\nin return for a large improvement in quality by allowing multiple groups of shared key/value\\nmatrices; each group has its respective set of attention heads.\\nFlash Attention\\nFlash Attention is a popular method and implementation that provides\\nsignificant speedups for both training and inference of Transformer LLMs\\non GPUs. It speeds up the attention calculation by optimizing what values\\nare loaded and moved between a GPU’s shared memory (SRAM) and high\\nbandwidth memory (HBM). It is described in detail in the papers\\n“FlashAttention: Fast and memory-efficient exact attention with IO-\\nawareness” and the subsequent “FlashAttention-2: Faster attention with\\nbetter parallelism and work partitioning”.\\nThe Transformer Block\\nRecall that the two major components of a Transformer block are an\\nattention layer and a feedforward neural network. A more detailed view of\\nthe block would also reveal the residual connections and layer-\\nnormalization operations that we can see in Figure 3-29.', 'Figure 3-29. A Transformer block from the original Transformer paper.\\nThe latest Transformer models at the time of this writing still retain the\\nmajor components, yet make a number of tweaks as we can see in Figure 3-\\n30.\\nOne of the differences we see in this version of the Transformer block is\\nthat normalization happens prior to attention and the feedforward layers.\\nThis has been reported to reduce the required training time (read: “On layer\\nnormalization in the Transformer architecture”). Another improvement in', 'normalization here is using RMSNorm, which is simpler and more efficient\\nthan the LayerNorm used in the original Transformer (read: “Root mean\\nsquare layer normalization”). Lastly, instead of the original Transformer’s\\nReLU activation function, newer variants like SwiGLU (described in “GLU\\nVariants Improve Transformer”) are now more common.\\nFigure 3-30. The Transformer block of a 2024-era Transformer like Llama 3 features some tweaks\\nlike pre-normalization and an attention optimized with grouped-query attention and rotary\\nembeddings.', 'Positional Embeddings (RoPE)\\nPositional embeddings have been a key component since the original\\nTransformer. They enable the model to keep track of the order of\\ntokens/words in a sequence/sentence, which is an indispensable source of\\ninformation in language. From the many positional encoding schemes\\nproposed in the past years, rotary positional embeddings (or “RoPE,”\\nintroduced in “RoFormer: Enhanced Transformer with rotary position\\nembedding”) is especially important to point out.\\nThe original Transformer paper and some of the early variants had absolute\\npositional embeddings that, in essence, marked the first token as position 1,\\nthe second as position 2...etc. These could either be static methods (where\\nthe positional vectors are generated using geometric functions) or learned\\n(where the model training assigns them their values during the learning\\nprocess). Some challenges arise from such methods when we scale up\\nmodels, which requires us to find ways to improve their efficiency.\\nFor example, one challenge in efficiently training models with large context\\nis that a lot of documents in the training set are much shorter than that\\ncontext. It would be inefficient to allocate the entire, say, 4K context to a\\nshort 10-word sentence. So during model training, documents are packed\\ntogether into each context in the training batch, as Figure 3-31 shows.', 'Figure 3-31. Packing is the process of efficiently organizing short training documents into the\\ncontext. It includes grouping multiple documents in a single context while minimizing the padding at\\nthe end of the context.\\nLearn more about packing by reading “Efficient sequence packing without\\ncross-contamination: Accelerating large language models without impacting\\nperformance” and watching the great visuals in “Introducing packed BERT\\nfor 2X training speed-up in natural language processing”.\\nPositional embedding methods have to adapt to this and other practical\\nconsiderations. If Document 50, for example, starts at position 50, then\\nwe’d be misinforming the model if we tell it that that first token is number\\n50 and that would affect its performance (because it would assume there’s\\nprevious context while in reality the earlier tokens belong to a different and\\nunrelated document the model should ignore).\\nInstead of the static, absolute embeddings that are added in the beginning of\\nthe forward pass, rotary embeddings are a method to encode positional\\ninformation in a way that captures absolute and relative token position\\ninformation. It is based on the idea of rotating vectors in their embeddings\\nspace. In the forward pass, they are added in the attention step, as Figure 3-\\n32 shows.', 'Figure 3-32. Rotary embeddings are applied in the attention step, not at the start of the forward pass.\\nDuring the attention process, the positional information is mixed in\\nspecifically to the queries and keys matrices just before we multiply them\\nfor relevance scoring, as we can see in Figure 3-33.', 'Figure 3-33. Rotary positional embeddings are added to the representation of tokens just before the\\nrelevance scoring step in self-attention.\\nOther Architectural Experiments and Improvements\\nMany tweaks of the Transformer are proposed and researched on a\\ncontinuous basis. “A Survey of Transformers” highlights a few of the main\\ndirections. Transformer architectures are also constantly adapted to domains\\nbeyond LLMs. Computer vision is an area where a lot of Transformer\\narchitecture research is happening (see: “Transformers in vision: A survey”\\nand “A survey on vision transformer”). Other domains include robotics (see\\n“Open X-Embodiment: Robotic learning datasets and RT-X models”) and\\ntime series (see “Transformers in time series: A survey”).', 'Summary\\nIn this chapter we discussed the main intuitions of Transformers and recent\\ndevelopments that enable the latest Transformer LLMs. We went over many\\nnew concepts, so let’s break down the key concepts that we discussed in\\nthis chapter:\\nA Transformer LLM generates one token at a time.\\nThat output token is appended to the prompt, then this updated\\nprompt is presented to the model again for another forward pass to\\ngenerate the next token.\\nThe three major components of the Transformer LLM are the\\ntokenizer, a stack of Transformer blocks, and a language modeling\\nhead.\\nThe tokenizer contains the token vocabulary for the model. The\\nmodel has token embeddings associated with those tokens.\\nBreaking the text into tokens and then using the embeddings of\\nthese tokens is the first step in the token generation process.\\nThe forward pass flows through all the stages once, one by one.\\nNear the end of the process, the LM head scores the probabilities\\nof the next possible token. Decoding strategies inform which actual\\ntoken to pick as the output for this generation step (sometimes it’s\\nthe most probable next token, but not always).\\nOne reason the Transformer excels is its ability to process tokens in\\nparallel. Each of the input tokens flow into their individual tracks\\nor streams of processing. The number of streams is the model’s\\n“context size” and this represents the max number of tokens the\\nmodel can operate on.\\nBecause Transformer LLMs loop to generate the text one token at a\\ntime, it’s a good idea to cache the processing results of each step so', 'we don’t duplicate the processing effort (these results are stored as\\nvarious matrices within the layers).\\nThe majority of processing happens within Transformer blocks.\\nThese are made up of two components. One of them is the\\nfeedforward neural network, which is able to store information and\\nmake predictions and interpolations from data it was trained on.\\nThe second major component of a Transformer block is the\\nattention layer. Attention incorporates contextual information to\\nallow the model to better capture the nuance of language.\\nAttention happens in two major steps: (1) scoring relevance and (2)\\ncombining information.\\nA Transformer attention layer conducts several attention operations\\nin parallel, each occurring inside an attention head, and their\\noutputs are aggregated to make up the output of the attention layer.\\nAttention can be accelerated via sharing the keys and values\\nmatrices between all heads, or groups of heads (grouped-query\\nattention).\\nMethods like Flash Attention speed up the attention calculation by\\noptimizing how the operation is done on the different memory\\nsystems of a GPU.\\nTransformers continue to see new developments and proposed tweaks to\\nimprove them in different scenarios, including language models and other\\ndomains and applications.\\nIn Part II of the book, we will cover some of these practical applications of\\nLLMs. In Chapter 4, we start with text classification, a common task in\\nLanguage AI. This next chapter serves as an introduction to applying both\\ngenerative and representation models.\\nOceanofPDF.com', 'Part II. Using Pretrained\\nLanguage Models\\nOceanofPDF.com', 'Chapter 4. Text Classification\\nA common task in natural language processing is classification. The goal of\\nthe task is to train a model to assign a label or class to some input text (see\\nFigure 4-1). Classifying text is used across the world for a wide range of\\napplications, from sentiment analysis and intent detection to extracting\\nentities and detecting language. The impact of language models, both\\nrepresentative and generative, on classification cannot be understated.\\nFigure 4-1. Using a language model to classify text.\\nIn this chapter, we will discuss several ways to use language models for\\nclassifying text. It will serve as an accessible introduction to using language\\nmodels that already have been trained. Due to the broad field of text\\nclassification, we will discuss several techniques and use them to explore\\nthe field of language models:\\n“Text Classification with Representation Models” demonstrates the\\nflexibility of nongenerative models for classification. We will\\ncover both task-specific models and embedding models.\\n“Text Classification with Generative Models” is an introduction to\\ngenerative language models as most of them can be used for\\nclassification. We will cover both an open source as well as a\\nclosed source language model.\\nIn this chapter, we will focus on leveraging pretrained language models,\\nmodels that already have been trained on large amounts of data that can be', 'used for classifying text. As illustrated in Figure 4-2, we will examine both\\nrepresentation and language models and explore their differences.\\nFigure 4-2. Although both representation and generative models can be used for classification, their\\napproaches differ.\\nThis chapter serves as an introduction to a variety of language models, both\\ngenerative and nongenerative. We will encounter common packages for\\nloading and using these models.\\nTIP\\nAlthough this book focuses on LLMs, it is highly advised to compare these examples\\nagainst classic, but strong baselines such as representing text with TF-IDF and training a\\nlogistic regression classifier on top of that.\\nThe Sentiment of Movie Reviews\\nYou can find the data we use to explore techniques for classifying text on\\nthe Hugging Face Hub, a platform for hosting models but also data. We will\\nuse the well-known “rotten_tomatoes” dataset to train and evaluate our\\nmodels.1  It contains 5,331 positive and 5,331 negative movie reviews from\\nRotten Tomatoes.', 'To load this data, we make use of the datasets package, which will be\\nused throughout the book:\\nfrom datasets import load_dataset\\n# Load our data\\ndata = load_dataset(\"rotten_tomatoes\")\\ndata\\nDatasetDict({\\n    train: Dataset({\\n        features: [\\'text\\', \\'label\\'],\\n        num_rows: 8530\\n    })\\n    validation: Dataset({\\n        features: [\\'text\\', \\'label\\'],\\n        num_rows: 1066\\n    })\\n    test: Dataset({\\n        features: [\\'text\\', \\'label\\'],\\n        num_rows: 1066\\n    })\\n})\\nThe data is split up into train, test, and validation splits. Throughout this\\nchapter, we will use the train split when we train a model and the test split\\nfor validating the results. Note that the additional validation split can be\\nused to further validate generalization if you used the train and test splits to\\nperform hyperparameter tuning.\\nLet’s take a look at some examples in our train split:\\ndata[\"train\"][0, -1]\\n{\\'text\\': [\\'the rock is destined to be the 21st century\\\\\\'s new \" \\nconan \" and that he\\\\\\'s going to make a splash even greater than \\narnold schwarzenegger , jean-claud van damme or steven segal \\n.\\',\\n  \\'things really get weird , though not particularly scary : \\nthe movie is all portent and no content .\\'],\\n \\'label\\': [1, 0]}', 'These short reviews are either labeled as positive (1) or negative (0). This\\nmeans that we will focus on binary sentiment classification.\\nText Classification with Representation\\nModels\\nClassification with pretrained representation models generally comes in two\\nflavors, either using a task-specific model or an embedding model. As we\\nexplored in the previous chapter, these models are created by fine-tuning a\\nfoundation model, like BERT, on a specific downstream task as illustrated\\nin Figure 4-3.\\nFigure 4-3. A foundation model is fine-tuned for specific tasks; for instance, to perform classification\\nor generate general-purpose embeddings.\\nA task-specific model is a representation model, such as BERT, trained for a\\nspecific task, like sentiment analysis. As we explored in Chapter 1, an\\nembedding model generates general-purpose embeddings that can be used\\nfor a variety of tasks not limited to classification, like semantic search (see\\nChapter 8).\\nThe process of fine-tuning a BERT model for classification is covered in\\nChapter 11 while creating an embedding model is covered in Chapter 10. In\\nthis chapter, we keep both models frozen (nontrainable) and only use their\\noutput as shown in Figure 4-4.', 'Figure 4-4. Perform classification directly with a task-specific model or indirectly with general-\\npurpose embeddings.\\nWe will leverage pretrained models that others have already fine-tuned for\\nus and explore how they can be used to classify our selected movie reviews.\\nModel Selection\\nChoosing the right models is not as straightforward as you might think with\\nover 60,000 models on the Hugging Face Hub for text classification and\\nmore than 8,000 models that generate embeddings at the moment of\\nwriting. Moreover, it’s crucial to select a model that fits your use case and\\nconsider its language compatibility, the underlying architecture, size, and\\nperformance.\\nLet’s start with the underlying architecture. As we explored in Chapter 1,\\nBERT, a well-known encoder-only architecture, is a popular choice for\\ncreating task-specific and embedding models. While generative models, like\\nthe GPT family, are incredible models, encoder-only models similarly excel\\nin task-specific use cases and tend to be significantly smaller in size.\\nOver the years, many variations of BERT have been developed, including\\nRoBERTa,2  DistilBERT,3  ALBERT,4  and DeBERTa,5  each trained in', 'various contexts. You can find an overview of some well-known BERT-like\\nmodels in Figure 4-5.\\nFigure 4-5. A timeline of common BERT-like model releases. These are considered foundation models\\nand are mostly intended to be fine-tuned on a downstream task.\\nSelecting the right model for the job can be a form of art in itself. Trying\\nthousands of pretrained models that can be found on Hugging Face’s Hub is\\nnot feasible so we need to be efficient with the models that we choose.\\nHaving said that, several models are great starting points and give you an\\nidea of the base performance of these kinds of models. Consider them solid\\nbaselines:\\nBERT base model (uncased)\\nRoBERTa base model\\nDistilBERT base model (uncased)\\nDeBERTa base model\\nbert-tiny\\nALBERT base v2\\nFor the task-specific model, we are choosing the Twitter-RoBERTa-base for\\nSentiment Analysis model. This is a RoBERTa model fine-tuned on tweets\\nfor sentiment analysis. Although this was not trained specifically for movie\\nreviews, it is interesting to explore how this model generalizes.', 'When selecting models to generate embeddings from, the MTEB\\nleaderboard is a great place to start. It contains open and closed source\\nmodels benchmarked across several tasks. Make sure to not only take\\nperformance into account. The importance of inference speed should not be\\nunderestimated in real-life solutions. As such, we will use sentence-\\ntransformers/all-mpnet-base-v2 as the embedding throughout this section. It\\nis a small but performant model.\\nUsing a Task-Specific Model\\nNow that we have selected our task-specific representation model, let’s start\\nby loading our model:\\nfrom transformers import pipeline\\n# Path to our HF model\\nmodel_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\\n# Load model into pipeline\\npipe = pipeline(\\n    model=model_path,\\n    tokenizer=model_path,\\n    return_all_scores=True,\\n    device=\"cuda:0\"\\n)\\nAs we load our model, we also load the tokenizer, which is responsible for\\nconverting input text into individual tokens, as illustrated in Figure 4-6.\\nAlthough that parameter is not needed as it is loaded automatically, it\\nillustrates what is happening under the hood.', 'Figure 4-6. An input sentence is first fed to a tokenizer before it can be processed by the task-specific\\nmodel.\\nThese tokens are at the core of most language models, as explored in depth\\nin Chapter 2. A major benefit of these tokens is that they can be combined\\nto generate representations even if they were not in the training data, as\\nshown in Figure 4-7.', 'Figure 4-7. By breaking down an unknown word into tokens, word embeddings can still be generated.\\nAfter loading all the necessary components, we can go ahead and use our\\nmodel on the test split of our data:\\nimport numpy as np\\nfrom tqdm import tqdm\\nfrom transformers.pipelines.pt_utils import KeyDataset\\n# Run inference\\ny_pred = []\\nfor output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), \\ntotal=len(data[\"test\"])):\\n    negative_score = output[0][\"score\"]\\n    positive_score = output[2][\"score\"]\\n    assignment = np.argmax([negative_score, positive_score])\\n    y_pred.append(assignment)\\nNow that we have generated our predictions, all that is left is evaluation. We\\ncreate a small function that we can easily use throughout this chapter:', 'from sklearn.metrics import classification_report\\ndef evaluate_performance(y_true, y_pred):\\n    \"\"\"Create and print the classification report\"\"\"\\n    performance = classification_report(\\n        y_true, y_pred,\\n        target_names=[\"Negative Review\", \"Positive Review\"]\\n    )\\n    print(performance)\\nNext, let’s create our classification report:\\nevaluate_performance(data[\"test\"][\"label\"], y_pred)\\n                precision    recall  f1-score   support\\nNegative Review       0.76      0.88      0.81       533\\nPositive Review       0.86      0.72      0.78       533\\n       accuracy                           0.80      1066\\n      macro avg       0.81      0.80      0.80      1066\\n   weighted avg       0.81      0.80      0.80      1066\\nTo read the resulting classification report, let’s first start by exploring how\\nwe can identify correct and incorrect predictions. There are four\\ncombinations depending on whether we predict something correctly (True)\\nversus incorrectly (False) and whether we predict the correct class\\n(Positive) versus incorrect class (Negative). We can illustrate these\\ncombinations as a matrix, commonly referred to as a confusion matrix, in\\nFigure 4-8.', 'Figure 4-8. The confusion matrix describes four types of predictions we can make.\\nUsing the confusion matrix, we can derive several formulas to describe the\\nquality of the model. In the previously generated classification report we\\ncan see four such methods, namely precision, recall, accuracy, and the F1\\nscore:\\nPrecision measures how many of the items found are relevant,\\nwhich indicates the accuracy of the relevant results.\\nRecall refers to how many relevant classes were found, which\\nindicates its ability to find all relevant results.\\nAccuracy refers to how many correct predictions the model makes\\nout of all predictions, which indicates the overall correctness of the\\nmodel.\\nThe F1 score balances both precision and recall to create a model’s\\noverall performance.\\nThese four metrics are illustrated in Figure 4-9, which describes them using\\nthe aforementioned classification report.', 'Figure 4-9. The classification report describes several metrics for evaluating a model’s performance.\\nWe will consider the weighted average of the F1 score throughout the\\nexamples in this book to make sure each class is treated equally. Our\\npretrained BERT model gives us an F1 score of 0.80 (we are reading this\\nfrom the weighted avg row and the f1-score column), which is great for a\\nmodel not trained specifically on our domain data!\\nTo improve the performance of our selected model, we could do a few\\ndifferent things including selecting a model trained on our domain data,\\nmovie reviews in this case, like DistilBERT base uncased finetuned SST-2.\\nWe could also shift our focus to another flavor of representation models,\\nnamely embedding models.\\nClassification Tasks That Leverage\\nEmbeddings\\nIn the previous example, we used a pretrained task-specific model for\\nsentiment analysis. However, what if we cannot find a model that was\\npretrained for this specific task? Do we need to fine-tune a representation\\nmodel ourselves? The answer is no!', 'There might be times when you want to fine-tune the model yourself if you\\nhave sufficient computing available (see Chapter 11). However, not\\neveryone has access to extensive computing. This is where general-purpose\\nembedding models come in.\\nSupervised Classification\\nUnlike the previous example, we can perform part of the training process\\nourselves by approaching it from a more classical perspective. Instead of\\ndirectly using the representation model for classification, we will use an\\nembedding model for generating features. Those features can then be fed\\ninto a classifier, thereby creating a two-step approach as shown in Figure 4-\\n10.\\nFigure 4-10. The feature extraction step and classification steps are separated.\\nA major benefit of this separation is that we do not need to fine-tune our\\nembedding model, which can be costly. In contrast, we can train a classifier,\\nlike a logistic regression, on the CPU instead.\\nIn the first step, we convert our textual input to embeddings using the\\nembedding model as shown in Figure 4-11. Note that this model is similarly\\nkept frozen and is not updated during the training process.', 'Figure 4-11. In step 1, we use the embedding model to extract the features and convert the input text\\nto embeddings.\\nWe can perform this step with sentence-transformer, a popular\\npackage for leveraging pretrained embedding models.6  Creating the\\nembeddings is straightforward:\\nfrom sentence_transformers import SentenceTransformer\\n# Load model\\nmodel = SentenceTransformer(\"sentence-transformers/all-mpnet-\\nbase-v2\")\\n# Convert text to embeddings\\ntrain_embeddings = model.encode(data[\"train\"][\"text\"], \\nshow_progress_bar=True)\\ntest_embeddings = model.encode(data[\"test\"][\"text\"], \\nshow_progress_bar=True)\\nAs we covered in Chapter 1, these embeddings are numerical\\nrepresentations of the input text. The number of values, or dimension, of the\\nembedding depends on the underlying embedding model. Let’s explore that\\nfor our model:', 'train_embeddings.shape\\n(8530, 768)\\nThis shows that each of our 8,530 input documents has an embedding\\ndimension of 768 and therefore each embedding contains 768 numerical\\nvalues.\\nIn the second step, these embeddings serve as the input features to the\\nclassifier illustrated in Figure 4-12. The classifier is trainable and not\\nlimited to logistic regression and can take on any form as long as it\\nperforms classification.\\nFigure 4-12. Using the embeddings as our features, we train a logistic regression model on our\\ntraining data.\\nWe will keep this step straightforward and use a logistic regression as the\\nclassifier. To train it, we only need to use the generated embeddings', 'together with our labels:\\nfrom sklearn.linear_model import LogisticRegression\\n# Train a logistic regression on our train embeddings\\nclf = LogisticRegression(random_state=42)\\nclf.fit(train_embeddings, data[\"train\"][\"label\"])\\nNext, let’s evaluate our model:\\n# Predict previously unseen instances\\ny_pred = clf.predict(test_embeddings)\\nevaluate_performance(data[\"test\"][\"label\"], y_pred)\\n              precision    recall  f1-score   support\\nNegative Review       0.85      0.86      0.85       533\\nPositive Review       0.86      0.85      0.85       533\\n       accuracy                           0.85      1066\\n      macro avg       0.85      0.85      0.85      1066\\n   weighted avg       0.85      0.85      0.85      1066\\nBy training a classifier on top of our embeddings, we managed to get an F1\\nscore of 0.85! This demonstrates the possibilities of training a lightweight\\nclassifier while keeping the underlying embedding model frozen.\\nTIP\\nIn this example, we used sentence-transformers to extract our embeddings,\\nwhich benefits from a GPU to speed up inference. However, we can remove this GPU\\ndependency by using an external API to create the embeddings. Popular choices for\\ngenerating embeddings are Cohere’s and OpenAI’s offerings. As a result, this would\\nallow the pipeline to run entirely on the CPU.', 'What If We Do Not Have Labeled Data?\\nIn our previous example, we had labeled data that we could leverage, but\\nthis might not always be the case in practice. Getting labeled data is a\\nresource-intensive task that can require significant human labor. Moreover,\\nis it actually worthwhile to collect these labels?\\nTo test this, we can perform zero-shot classification, where we have no\\nlabeled data to explore whether the task seems feasible. Although we know\\nthe definition of the labels (their names), we do not have labeled data to\\nsupport them. Zero-shot classification attempts to predict the labels of input\\ntext even though it was not trained on them, as shown in Figure 4-13.\\nFigure 4-13. In zero-shot classification, we have no labeled data, only the labels themselves. The\\nzero-shot model decides how the input is related to the candidate labels.', 'To perform zero-shot classification with embeddings, there is a neat trick\\nthat we can use. We can describe our labels based on what they should\\nrepresent. For example, a negative label for movie reviews can be described\\nas “This is a negative movie review.” By describing and embedding the\\nlabels and documents, we have data that we can work with. This process, as\\nillustrated in Figure 4-14, allows us to generate our own target labels\\nwithout the need to actually have any labeled data.\\nFigure 4-14. To embed the labels, we first need to give them a description, such as “a negative movie\\nreview.” This can then be embedded through sentence-transformers.\\nWe can create these label embeddings using the .encode function as we\\ndid earlier:\\n# Create embeddings for our labels\\nlabel_embeddings = model.encode([\"A negative review\",  \"A \\npositive review\"])\\nTo assign labels to documents, we can apply cosine similarity to the\\ndocument label pairs. This is the cosine of the angle between vectors, which', 'is calculated through the dot product of the embeddings and divided by the\\nproduct of their lengths, as illustrated in Figure 4-15.\\nFigure 4-15. The cosine similarity is the angle between two vectors or embeddings. In this example,\\nwe calculate the similarity between a document and the two possible labels, positive and negative.\\nWe can use cosine similarity to check how similar a given document is to\\nthe description of the candidate labels. The label with the highest similarity\\nto the document is chosen as illustrated in Figure 4-16.\\nFigure 4-16. After embedding the label descriptions and the documents, we can use cosine similarity\\nfor each label document pair.', 'To perform cosine similarity on the embeddings, we only need to compare\\nthe document embeddings with the label embeddings and get the best\\nmatching pairs:\\nfrom sklearn.metrics.pairwise import cosine_similarity\\n# Find the best matching label for each document\\nsim_matrix = cosine_similarity(test_embeddings, label_embeddings)\\ny_pred = np.argmax(sim_matrix, axis=1)\\nAnd that is it! We only needed to come up with names for our labels to\\nperform our classification tasks. Let’s see how well this method works:\\nevaluate_performance(data[\"test\"][\"label\"], y_pred)\\n                precision    recall  f1-score   support\\nNegative Review       0.78      0.77      0.78       533\\nPositive Review       0.77      0.79      0.78       533\\n       accuracy                           0.78      1066\\n      macro avg       0.78      0.78      0.78      1066\\n   weighted avg       0.78      0.78      0.78      1066\\nNOTE\\nIf you are familiar with zero-shot classification with Transformer-based models, you\\nmight wonder why we choose to illustrate this with embeddings instead. Although\\nnatural language inference models are amazing for zero-shot classification, the example\\nhere demonstrates the flexibility of embeddings for a variety of tasks. As you will see\\nthroughout the book, embeddings can be found in most Language AI use cases and are\\noften an underestimated but incredibly vital component.\\nAn F1 score of 0.78 is quite impressive considering we did not use any\\nlabeled data at all! This just shows how versatile and useful embeddings\\nare, especially if you are a bit creative with how they are used.', 'TIP\\nLet’s put that creativity to the test. We decided upon “A negative/positive review” as the\\nname of our labels but that can be improved. Instead, we can make them a bit more\\nconcrete and specific toward our data by using “A very negative/positive movie review”\\ninstead. This way, the embedding will capture that it is a movie review and will focus a\\nbit more on the extremes of the two labels. Try it out and explore how it affects the\\nresults.\\nText Classification with Generative Models\\nClassification with generative language models, such as OpenAI’s GPT\\nmodels, works a bit differently from what we have done thus far. These\\nmodels take as input some text and generative text and are thereby aptly\\nnamed sequence-to-sequence models. This is in stark contrast to our task-\\nspecific model, which outputs a class instead, as illustrated in Figure 4-17.\\nFigure 4-17. A task-specific model generates numerical values from sequences of tokens while a\\ngenerative model generates sequences of tokens from sequences of tokens.\\nThese generative models are generally trained on a wide variety of tasks\\nand usually do not perform your use case out of the box. For instance, if we\\ngive a generative model a movie review without any context, it has no idea\\nwhat to do with it.\\nInstead, we need to help it understand the context and guide it toward the\\nanswers that we are looking for. As demonstrated in Figure 4-18, this\\nguiding process is done mainly through the instruction, or prompt, that you', 'give such a model. Iteratively improving your prompt to get your preferred\\noutput is called prompt engineering.\\nFigure 4-18. Prompt engineering allows prompts to be updated to improve the output generated by\\nthe model.\\nIn this section, we will demonstrate how we can leverage different types of\\ngenerative models to perform classification without our Rotten Tomatoes\\ndataset.\\nUsing the Text-to-Text Transfer Transformer\\nThroughout this book, we will explore mostly encoder-only (representation)\\nmodels like BERT and decoder-only (generative) models like ChatGPT.\\nHowever, as discussed in Chapter 1, the original Transformer architecture\\nactually consists of an encoder-decoder architecture. Like the decoder-only\\nmodels, these encoder-decoder models are sequence-to-sequence models\\nand generally fall in the category of generative models.', 'An interesting family of models that leverage this architecture is the Text-\\nto-Text Transfer Transformer or T5 model. Illustrated in Figure 4-19, its\\narchitecture is similar to the original Transformer where 12 decoders and 12\\nencoders are stacked together.7 \\nFigure 4-19. The T5 architecture is similar to the original Transformer model, a decoder-encoder\\narchitecture.\\nWith this architecture, these models were first pretrained using masked\\nlanguage modeling. In the first step of training, illustrated in Figure 4-20,\\ninstead of masking individual tokens, sets of tokens (or token spans) were\\nmasked during pretraining.', 'Figure 4-20. In the first step of training, namely pretraining, the T5 model needs to predict masks\\nthat could contain multiple tokens.\\nThe second step of training, namely fine-tuning the base model, is where\\nthe real magic happens. Instead of fine-tuning the model for one specific\\ntask, each task is converted to a sequence-to-sequence task and trained\\nsimultaneously. As illustrated in Figure 4-21, this allows the model to be\\ntrained on a wide variety of tasks.\\nFigure 4-21. By converting specific tasks to textual instructions, the T5 model can be trained on a\\nvariety of tasks during fine-tuning.', 'This method of fine-tuning was extended in the paper “Scaling instruction-\\nfinetuned language models”, which introduced more than a thousand tasks\\nduring fine-tuning that more closely follow instructions as we know them\\nfrom GPT models.8  This resulted in the Flan-T5 family of models that\\nbenefit from this large variety of tasks.\\nTo use this pretrained Flan-T5 model for classification, we will start by\\nloading it through the \"text2text-generation\" task, which is\\ngenerally reserved for these encoder-decoder models:\\n# Load our model\\npipe = pipeline(\\n    \"text2text-generation\", \\n    model=\"google/flan-t5-small\", \\n    device=\"cuda:0\"\\n)\\nThe Flan-T5 model comes in various sizes (flan-t5-small/base/large/xl/xxl)\\nand we will use the smallest to speed things up a bit. However, feel free to\\nplay around with larger models to see if you can improve the results.\\nCompared to our task-specific model, we cannot just give the model some\\ntext and hope it will output the sentiment. Instead, we will have to instruct\\nthe model to do so.\\nThus, we prefix each document with the prompt “Is the following sentence\\npositive or negative?”:\\n# Prepare our data\\nprompt = \"Is the following sentence positive or negative? \"\\ndata = data.map(lambda example: {\"t5\": prompt + example[\\'text\\']})\\ndata\\nDatasetDict({\\n    train: Dataset({\\n        features: [\\'text\\', \\'label\\', \\'t5\\'],\\n        num_rows: 8530\\n    })\\n    validation: Dataset({', '        features: [\\'text\\', \\'label\\', \\'t5\\'],\\n        num_rows: 1066\\n    })\\n    test: Dataset({\\n        features: [\\'text\\', \\'label\\', \\'t5\\'],\\n        num_rows: 1066\\n    })\\n})\\nAfter creating our updated data, we can run the pipeline similar to the task-\\nspecific example:\\n# Run inference\\ny_pred = []\\nfor output in tqdm(pipe(KeyDataset(data[\"test\"], \"t5\")), \\ntotal=len(data[\"test\"])):\\n    text = output[0][\"generated_text\"]\\n    y_pred.append(0 if text == \"negative\" else 1)\\nSince this model generates text, we did need to convert the textual output to\\nnumerical values. The output word “negative” was mapped to 0 whereas\\n“positive” was mapped to 1.\\nThese numerical values now allow us to test the quality of the model in the\\nsame way we have done before:\\nevaluate_performance(data[\"test\"][\"label\"], y_pred)\\n                precision    recall  f1-score   support\\nNegative Review       0.83      0.85      0.84       533\\nPositive Review       0.85      0.83      0.84       533\\n       accuracy                           0.84      1066\\n      macro avg       0.84      0.84      0.84      1066\\n   weighted avg       0.84      0.84      0.84      1066\\nWith an F1 score of 0.84, it is clear this Flan-T5 model is an amazing first\\nlook into the capabilities of generative models.', 'ChatGPT for Classification\\nAlthough we focus throughout the book on open source models, another\\nmajor component of the Language AI field is closed sourced models; in\\nparticular, ChatGPT.\\nAlthough the underlying architecture of the original ChatGPT model (GPT-\\n3.5) is not shared, we can assume from its name that it is based on the\\ndecoder-only architecture that we have seen in the GPT models thus far.\\nFortunately, OpenAI shared an overview of the training procedure that\\ninvolved an important component, namely preference tuning. As illustrated\\nin Figure 4-22, OpenAI first manually created the desired output to an input\\nprompt (instruction data) and used that data to create a first variant of its\\nmodel.\\nFigure 4-22. Manually labeled data consisting of an instruction (prompt) and output was used to\\nperform fine-tuning (instruction-tuning).\\nOpenAI used the resulting model to generate multiple outputs that were\\nmanually ranked from best to worst. As shown in Figure 4-23, this ranking\\ndemonstrates a preference for certain outputs (preference data) and was\\nused to create its final model, ChatGPT.', 'Figure 4-23. Manually ranked preference data was used to generate the final model, ChatGPT.\\nA major benefit of using preference data over instruction data is the nuance\\nit represents. By demonstrating the difference between a good and better\\noutput the generative model learns to generate text that resembles human\\npreference. In Chapter 12, we will explore how these fine-tuning and\\npreference-tuning methodologies work and how you can perform them\\nyourself.\\nThe process of using a closed sourced model is quite different from the\\nopen sourced examples we have seen thus far. Instead of loading the model,\\nwe can access the model through OpenAI’s API.\\nBefore we go into the classification example, you will first need to create a\\nfree account on https://oreil.ly/AEXvA and create an API key here:\\nhttps://oreil.ly/lrTXl. After doing so, you can use your API to communicate\\nwith OpenAI’s servers.\\nWe can use this key to create a client:\\nimport openai\\n# Create client\\nclient = openai.OpenAI(api_key=\"YOUR_KEY_HERE\")\\nUsing this client, we create the chatgpt_generation function, which\\nallows us to generate some text based on a specific prompt, input document,\\nand the selected model:', 'def chatgpt_generation(prompt, document, model=\"gpt-3.5-turbo-\\n0125\"):\\n    \"\"\"Generate an output based on a prompt and an input \\ndocument.\"\"\"\\n    messages=[\\n        {\\n            \"role\": \"system\",\\n            \"content\": \"You are a helpful assistant.\"\\n            },\\n        {\\n            \"role\": \"user\",\\n            \"content\":   prompt.replace(\"[DOCUMENT]\", document)\\n            }\\n    ]\\n    chat_completion = client.chat.completions.create(\\n      messages=messages,\\n      model=model,\\n      temperature=0\\n    )\\n    return chat_completion.choices[0].message.content\\nNext, we will need to create a template to ask the model to perform the\\nclassification:\\n# Define a prompt template as a base\\nprompt = \"\"\"Predict whether the following document is a positive \\nor negative movie review:\\n[DOCUMENT]\\nIf it is positive return 1 and if it is negative return 0. Do not \\ngive any other answers.\\n\"\"\"\\n# Predict the target using GPT\\ndocument = \"unpretentious , charming , quirky , original\"\\nchatgpt_generation(prompt, document)\\nThis template is merely an example and can be changed however you want.\\nFor now, we kept it as simple as possible to illustrate how to use such a\\ntemplate.', 'Before you use this over a potentially large dataset, it is important to always\\nkeep track of your usage. External APIs such as OpenAI’s offering can\\nquickly become costly if you perform many requests. At the time of writing,\\nrunning our test dataset using the “gpt-3.5-turbo-0125” model costs 3 cents,\\nwhich is covered by the free account, but this might change in the future.\\nTIP\\nWhen dealing with external APIs, you might run into rate limit errors. These appear\\nwhen you call the API too often as some APIs might limit the rate with which you can\\nuse it per minute or hour.\\nTo prevent these errors, we can implement several methods for retrying the request,\\nincluding something referred to as exponential backoff. It performs a short sleep each\\ntime we hit a rate limit error and then retries the unsuccessful request. Whenever it is\\nunsuccessful again, the sleep length is increased until the request is successful or we hit\\na maximum number of retries.\\nTo use it with OpenAI, there is a great guide that can help you get started.\\nNext, we can run this for all reviews in the test dataset to get its predictions.\\nYou can skip this if you want to save your (free) credits for other tasks.\\n# You can skip this if you want to save your (free) credits\\npredictions = [\\n    chatgpt_generation(prompt, doc) for doc in tqdm(data[\"test\"]\\n[\"text\"])\\n]\\nLike the previous example, we need to convert the output from strings to\\nintegers to evaluate its performance:\\n# Extract predictions\\ny_pred = [int(pred) for pred in predictions]', '# Evaluate performance\\nevaluate_performance(data[\"test\"][\"label\"], y_pred)\\n                precision    recall  f1-score   support\\nNegative Review       0.87      0.97      0.92       533\\nPositive Review       0.96      0.86      0.91       533\\n       accuracy                           0.91      1066\\n      macro avg       0.92      0.91      0.91      1066\\n   weighted avg       0.92      0.91      0.91      1066\\nThe F1 score of 0.91 already gives a glimpse into the performance of the\\nmodel that brought generative AI to the masses. However, since we do not\\nknow what data the model was trained on, we cannot easily use these kinds\\nof metrics for evaluating the model. For all we know, it might have actually\\nbeen trained on our dataset!\\nIn Chapter 12, we will explore how we can evaluate both open source and\\nclosed source models on more generalized tasks.\\nSummary\\nIn this chapter, we discussed many different techniques for performing a\\nwide variety of classification tasks, from fine-tuning your entire model to\\nno tuning at all! Classifying textual data is not as straightforward as it may\\nseem on the surface and there is an incredible amount of creative techniques\\nfor doing so.\\nIn this chapter, we explored text classification using both generative and\\nrepresentation language models. Our goal was to assign a label or class to\\ninput text for the classification of a review’s sentiment.\\nWe explored two types of representation models, a task-specific model and\\nan embedding model. The task-specific model was pretrained on a large\\ndataset specifically for sentiment analysis and showed us that pretrained\\nmodels are a great technique for classifying documents. The embedding', 'model was used to generate multipurpose embeddings that we used as the\\ninput to train a classifier.\\nSimilarly, we explored two types of generative models, an open source\\nencoder-decoder model (Flan-T5) and a closed source decoder-only model\\n(GPT-3.5). We used these generative models in text classification without\\nrequiring specific (additional) training on domain data or labeled datasets.\\nIn the next chapter, we will continue with classification but focus instead on\\nunsupervised classification. What can we do if we have textual data without\\nany labels? What information can we extract? We will focus on clustering\\nour data as well as naming the clusters with topic modeling techniques.\\n1  Bo Pang and Lillian Lee. “Seeing stars: Exploiting class relationships for sentiment\\ncategorization with respect to rating scales.” arXiv preprint cs/0506075 (2005).\\n2  Yinhan Liuet et al. “RoBERTa: A robustly optimized BERT pretraining approach.” arXiv\\npreprint arXiv:1907.11692 (2019).\\n3  Victor Sanh et al. “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\\nlighter.” arXiv preprint arXiv:1910.01108 (2019).\\n4  Zhenzhong Lan et al. “ALBERT: A lite BERT for self-supervised learning of language\\nrepresentations.” arXiv preprint arXiv:1909.11942 (2019).\\n5  Pengcheng He et al. “DeBERTa: Decoding-enhanced BERT with disentangled attention.”\\narXiv preprint arXiv:2006.03654 (2020).\\n6  Nils Reimers and Iryna Gurevych. “Sentence-BERT: Sentence embeddings using Siamese\\nBERT-networks.” arXiv preprint arXiv:1908.10084 (2019).\\n7  Colin Raffel et al. “Exploring the limits of transfer learning with a unified text-to-text\\ntransformer.” The Journal of Machine Learning Research 21.1 (2020): 5485–5551.\\n8  Hyung Won Chung et al. “Scaling instruction-finetuned language models.” arXiv preprint\\narXiv:2210.11416 (2022).\\nOceanofPDF.com', 'Chapter 5. Text Clustering and\\nTopic Modeling\\nAlthough supervised techniques, such as classification, have reigned\\nsupreme over the last few years in the industry, the potential of\\nunsupervised techniques such as text clustering cannot be understated.\\nText clustering aims to group similar texts based on their semantic content,\\nmeaning, and relationships. As illustrated in Figure 5-1, the resulting\\nclusters of semantically similar documents not only facilitate efficient\\ncategorization of large volumes of unstructured text but also allow for quick\\nexploratory data analysis.\\nFigure 5-1. Clustering unstructured textual data.\\nThe recent evolution of language models, which enable contextual and\\nsemantic representations of text, has enhanced the effectiveness of text\\nclustering. Language is more than a bag of words, and recent language\\nmodels have proved to be quite capable of capturing that notion. Text', 'clustering, unbound by supervision, allows for creative solutions and\\ndiverse applications, such as finding outliers, speedup labeling, and finding\\nincorrectly labeled data.\\nText clustering has also found itself in the realm of topic modeling, where\\nwe want to discover (abstract) topics that appear in large collections of\\ntextual data. As shown in Figure 5-2, we generally describe a topic using\\nkeywords or keyphrases and, ideally, have a single overarching label.\\nFigure 5-2. Topic modeling is a way to give meaning to clusters of textual documents.\\nIn this chapter, we will first explore how to perform clustering with\\nembedding models and then transition to a text-clustering-inspired method\\nof topic modeling, namely BERTopic.\\nText clustering and topic modeling have an important role in this book as\\nthey explore creative ways to combine a variety of different language\\nmodels. We will explore how combining encoder-only (embeddings),\\ndecoder-only (generative), and even classical methods (bag-of-words) can\\nresult in amazing new techniques and pipelines.\\nArXiv’s Articles: Computation and Language\\nThroughout this chapter, we will be running clustering and topic modeling\\nalgorithms on ArXiv articles. ArXiv is an open-access platform for\\nscholarly articles, mostly in the fields of computer science, mathematics,', 'and physics. We will explore articles in the field of Computation and\\nLanguage to keep with the theme of this book. The dataset contains 44,949\\nabstracts between 1991 and 2024 from ArXiv’s cs.CL (Computation and\\nLanguage) section.\\nWe load the data and create separate variables for the abstracts, titles, and\\nyears of each article:\\n# Load data from Hugging Face\\nfrom datasets import load_dataset\\ndataset = load_dataset(\"maartengr/arxiv_nlp\")[\"train\"]\\n# Extract metadata\\nabstracts = dataset[\"Abstracts\"]\\ntitles = dataset[\"Titles\"]\\nA Common Pipeline for Text Clustering\\nText clustering allows for discovering patterns in data that you may or may\\nnot be familiar with. It allows for getting an intuitive understanding of the\\ntask, for example, a classification task, but also of its complexity. As a\\nresult, text clustering can become more than just a quick method for\\nexploratory data analysis.\\nAlthough there are many methods for text clustering, from graph-based\\nneural networks to centroid-based clustering techniques, a common pipeline\\nthat has gained popularity involves three steps and algorithms:\\n1. Convert the input documents to embeddings with an embedding\\nmodel.\\n2. Reduce the dimensionality of embeddings with a dimensionality\\nreduction model.\\n3. Find groups of semantically similar documents with a cluster\\nmodel.', 'Embedding Documents\\nThe first step is to convert our textual data to embeddings, as illustrated in\\nFigure 5-3. Recall from previous chapters that embeddings are numerical\\nrepresentations of text that attempt to capture its meaning.\\nFigure 5-3. Step 1: We convert documents to embeddings using an embedding model.\\nChoosing embedding models optimized for semantic similarity tasks is\\nespecially important for clustering as we attempt to find groups of\\nsemantically similar documents. Fortunately, most embedding models at the\\ntime of writing focus on just that, semantic similarity.\\nAs we did in the previous chapter, we will use the MTEB leaderboard to\\nselect an embedding model. We will need an embedding model that has a\\ndecent score on clustering tasks but also is small enough to run quickly.\\nInstead of using the “sentence-transformers/all-mpnet-base-v2” model we\\nused in the previous chapter, we use the “thenlper/gte-small” model instead.\\nIt is a more recent model that outperforms the previous model on clustering\\ntasks and due to its small size is even faster for inference. However, feel\\nfree to play around with newer models that have been released since!\\nfrom sentence_transformers import SentenceTransformer\\n# Create an embedding for each abstract\\nembedding_model = SentenceTransformer(\"thenlper/gte-small\")\\nembeddings = embedding_model.encode(abstracts, \\nshow_progress_bar=True)', 'Let’s check how many values each document embedding contains:\\n# Check the dimensions of the resulting embeddings\\nembeddings.shape\\n(44949, 384)\\nEach embedding has 384 values that together represent the semantic\\nrepresentation of the document. You can view these embeddings as the\\nfeatures that we want to cluster.\\nReducing the Dimensionality of Embeddings\\nBefore we cluster the embeddings, we will first need to take their high\\ndimensionality into account. As the number of dimensions increases, there\\nis an exponential growth in the number of possible values within each\\ndimension. Finding all subspaces within each dimension becomes\\nincreasingly complex.\\nAs a result, high-dimensional data can be troublesome for many clustering\\ntechniques as it gets more difficult to identify meaningful clusters. Instead,\\nwe can make use of dimensionality reduction. As illustrated in Figure 5-4,\\nthis technique allows us to reduce the size of the dimensional space and\\nrepresent the same data with fewer dimensions. Dimensionality reduction\\ntechniques aim to preserve the global structure of high-dimensional data by\\nfinding low-dimensional representations.', 'Figure 5-4. Dimensionality reduction allows data in high-dimensional space to be compressed to a\\nlower-dimensional representation.\\nNote that this is a compression technique and that the underlying algorithm\\nis not arbitrarily removing dimensions. To help the cluster model create\\nmeaningful clusters, the second step in our clustering pipeline is therefore\\ndimensionality reduction, as shown in Figure 5-5.\\nFigure 5-5. Step 2: The embeddings are reduced to a lower-dimensional space using dimensionality\\nreduction.', \"Well-known methods for dimensionality reduction are Principal Component\\nAnalysis (PCA)1  and Uniform Manifold Approximation and Projection\\n(UMAP).2  For this pipeline, we are going with UMAP as it tends to handle\\nnonlinear relationships and structures a bit better than PCA.\\nNOTE\\nDimensionality reduction techniques, however, are not flawless. They do not perfectly\\ncapture high-dimensional data in a lower-dimensional representation. Information will\\nalways be lost with this procedure. There is a balance between reducing dimensionality\\nand keeping as much information as possible.\\nTo perform dimensionality reduction, we need to instantiate our UMAP\\nclass and pass the generated embeddings to it:\\nfrom umap import UMAP\\n# We reduce the input embeddings from 384 dimensions to 5 \\ndimensions\\numap_model = UMAP(\\n    n_components=5, min_dist=0.0, metric='cosine', \\nrandom_state=42\\n)\\nreduced_embeddings = umap_model.fit_transform(embeddings)\\nWe can use the n_components parameter to decide the shape of the\\nlower-dimensional space, namely 5 dimensions. Generally, values between\\n5 and 10 work well to capture high-dimensional global structures.\\nThe min_dist parameter is the minimum distance between embedded\\npoints. We are setting this to 0 as that generally results in tighter clusters.\\nWe set metric to 'cosine' as Euclidean-based methods have issues\\ndealing with high-dimensional data.\\nNote that setting a random_state in UMAP will make the results\\nreproducible across sessions but will disable parallelism and therefore slow\", 'down training.\\nCluster the Reduced Embeddings\\nThe third step is to cluster the reduced embeddings, as illustrated in\\nFigure 5-6.\\nFigure 5-6. Step 3: We cluster the documents using the embeddings with reduced dimensionality.\\nAlthough a common choice is a centroid-based algorithm like k-means,\\nwhich requires a set of clusters to be generated, we do not know the number\\nof clusters beforehand. Instead, a density-based algorithm freely calculates\\nthe number of clusters and does not force all data points to be part of a\\ncluster, as illustrated in Figure 5-7.', 'Figure 5-7. The clustering algorithm not only impacts how clusters are generated but also how they\\nare viewed.\\nA common density-based model is Hierarchical Density-Based Spatial\\nClustering of Applications with Noise (HDBSCAN).3  HDBSCAN is a\\nhierarchical variation of a clustering algorithm called DBSCAN that allows\\nfor dense (micro)-clusters to be found without having to explicitly specify\\nthe number of clusters.4  As a density-based method, HDBSCAN can also\\ndetect outliers in the data, which are data points that do not belong to any\\ncluster. These outliers will not be assigned or forced to belong to any\\ncluster. In other words, they are ignored. Since ArXiv articles might contain\\nsome niche papers, using a model that detects outliers could be helpful.\\nAs with the previous packages, using HDBSCAN is straightforward. We\\nonly need to instantiate the model and pass our reduced embeddings to it:\\nfrom hdbscan import HDBSCAN\\n# We fit the model and extract the clusters\\nhdbscan_model = HDBSCAN(\\n    min_cluster_size=50, metric=\"euclidean\", \\ncluster_selection_method=\"eom\"\\n).fit(reduced_embeddings)', 'clusters = hdbscan_model.labels_\\n# How many clusters did we generate?\\nlen(set(clusters))\\n156\\nWith HDBSCAN, we generated 156 clusters in our dataset. To create more\\nclusters, we will need to reduce the value of min_cluster_size as it\\nrepresents the minimum size that a cluster can take.\\nInspecting the Clusters\\nNow that we have generated our clusters, we can inspect each cluster\\nmanually and explore the assigned documents to get an understanding of its\\ncontent. For example, let us take a few random documents from cluster 0:\\nimport numpy as np\\n# Print first three documents in cluster 0\\ncluster = 0\\nfor index in np.where(clusters==cluster)[0][:3]:\\n    print(abstracts[index][:300] + \"... \\\\n\")\\nThis works aims to design a statistical machine translation \\nfrom English text\\nto American Sign Language (ASL). The system is based on Moses \\ntool with some\\nmodifications and the results are synthesized through a 3D \\navatar for\\ninterpretation. First, we translate the input text to gloss, a \\nwritten fo...\\nResearches on signed languages still strongly dissociate lin- \\nguistic issues\\nrelated on phonological and phonetic aspects, and gesture \\nstudies for\\nrecognition and synthesis purposes. This paper focuses on the \\nimbrication of\\nmotion and meaning for the analysis, synthesis and evaluation \\nof sign lang...', 'Modern computational linguistic software cannot produce \\nimportant aspects of\\nsign language translation. Using some researches we deduce that \\nthe majority of\\nautomatic sign language translation systems ignore many aspects \\nwhen they\\ngenerate animation; therefore the interpretation lost the truth \\ninf...\\nFrom these documents, it seems that this cluster contains documents mostly\\nabout translation from and to sign language, interesting!\\nWe can take this one step further and attempt to visualize our results instead\\nof going through all documents manually. To do so, we will need to reduce\\nour document embeddings to two dimensions, as that allows us to plot the\\ndocuments on an x/y plane:\\nimport pandas as pd\\n# Reduce 384-dimensional embeddings to two dimensions for easier \\nvisualization\\nreduced_embeddings = UMAP(\\n    n_components=2, min_dist=0.0, metric=\"cosine\", \\nrandom_state=42\\n).fit_transform(embeddings)\\n# Create dataframe\\ndf = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\\ndf[\"title\"] = titles\\ndf[\"cluster\"] = [str(c) for c in clusters]\\n# Select outliers and non-outliers (clusters)\\nto_plot = df.loc[df.cluster != \"-1\", :]\\noutliers = df.loc[df.cluster == \"-1\", :]\\nWe also created a dataframe for our clusters (clusters_df) and for the\\noutliers (outliers_df) separately since we generally want to focus on\\nthe clusters and highlight those.', 'NOTE\\nUsing any dimensionality reduction technique for visualization purposes creates\\ninformation loss. It is merely an approximation of what our original embeddings look\\nlike. Although it is informative, it might push clusters together and drive them further\\napart than they actually are. Human evaluation, inspecting the clusters ourselves, is\\ntherefore a key component of cluster analysis!\\nTo generate a static plot, we will use the well-known plotting library,\\nmatplotlib:\\nimport matplotlib.pyplot as plt\\n# Plot outliers and non-outliers separately\\nplt.scatter(outliers_df.x, outliers_df.y, alpha=0.05, s=2, \\nc=\"grey\")\\nplt.scatter(\\n    clusters_df.x, clusters_df.y, \\nc=clusters_df.cluster.astype(int),\\n    alpha=0.6, s=2, cmap=\"tab20b\"\\n)\\nplt.axis(\"off\")\\nAs we can see in Figure 5-8, it tends to capture major clusters quite well.\\nNote how clusters of points are colored in the same color, indicating that\\nHDBSCAN put them in a group together. Since we have a large number of\\nclusters, the plotting library cycles the colors between clusters, so don’t\\nthink that all green points are one cluster, for example.', 'Figure 5-8. The generated clusters (colored) and outliers (gray) are represented as a 2D\\nvisualization.\\nThis is visually appealing but does not yet allow us to see what is\\nhappening inside the clusters. Instead, we can extend this visualization by\\ngoing from text clustering to topic modeling.\\nFrom Text Clustering to Topic Modeling\\nText clustering is a powerful tool for finding structure among large\\ncollections of documents. In our previous example, we could manually\\ninspect each cluster and identify them based on their collection of\\ndocuments. For instance, we explored a cluster that contained documents\\nabout sign language. We could say that the topic of that cluster is “sign\\nlanguage.”\\nThis idea of finding themes or latent topics in a collection of textual data is\\noften referred to as topic modeling. Traditionally, it involves finding a set of', 'keywords or phrases that best represent and capture the meaning of the\\ntopic, as we illustrate in Figure 5-9.\\nFigure 5-9. Traditionally, topics are represented by a number of keywords but can take other forms.\\nInstead of labeling a topic as “sign language,” these techniques use\\nkeywords such as “sign,” “language,” and “translation” to describe the\\ntopic. As such, this does not give a single label to a topic and instead\\nrequires the user to understand the meaning of the topic through those\\nkeywords.\\nClassic approaches, like latent Dirichlet allocation, assume that each topic\\nis characterized by a probability distribution of words in a corpus’s\\nvocabulary.5  Figure 5-10 demonstrates how each word in a vocabulary is\\nscored against its relevance to each topic.', 'Figure 5-10. Keywords are extracted based on their distribution over a single topic.\\nThese approaches generally use a bag-of-words technique for the main\\nfeatures of the textual data, which does not take the context nor the meaning\\nof words and phrases into account. In contrast, our text clustering example\\ndoes take both into account as it relies on Transformer-based embeddings\\nthat are optimized for semantic similarity and contextual meaning through\\nattention.\\nIn this section, we will extend text clustering into the realm of topic\\nmodeling through a highly modular text clustering and topic modeling\\nframework, namely BERTopic.\\nBERTopic: A Modular Topic Modeling Framework\\nBERTopic is a topic modeling technique that leverages clusters of\\nsemantically similar texts to extract various types of topic representations.6 \\nThe underlying algorithm can be thought of in two steps.\\nFirst, as illustrated in Figure 5-11, we follow the same procedure as we did\\nbefore in our text clustering example. We embed documents, reduce their\\ndimensionality, and finally cluster the reduced embedding to create groups\\nof semantically similar documents.', 'Figure 5-11. The first part of BERTopic’s pipeline is to create clusters of semantically similar\\ndocuments.\\nSecond, it models a distribution over words in the corpus’s vocabulary by\\nleveraging a classic method, namely bag-of-words. The bag-of-words, as\\nwe discussed briefly in Chapter 1 and illustrate in Figure 5-12, does exactly\\nwhat its name implies, counting the number of times each word appears in a\\ndocument. The resulting representation could be used to extract the most\\nfrequent words inside a document.\\nFigure 5-12. A bag-of-words counts the number of times each word appears inside a document.', 'There are two caveats, however. First, this is a representation on a\\ndocument level and we are interested in a cluster-level perspective. To\\naddress this, the frequency of words is calculated within the entire cluster\\ninstead of only the document, as illustrated in Figure 5-13.\\nFigure 5-13. Generating c-TF by counting the frequency of words per cluster instead of per\\ndocument.\\nSecond, stop words like “the” and “I” tend to appear often in documents\\nand provide little meaning to the actual documents. BERTopic uses a class-\\nbased variant of term frequency–inverse document frequency (c-TF-IDF) to\\nput more weight on words that are more meaningful to a cluster and put less\\nweight on words that are used across all clusters.\\nEach word in the bag-of-words, the c-TF in c-TF-IDF, is multiplied by the\\nIDF value of each word. As shown in Figure 5-14, the IDF value is\\ncalculated by taking the logarithm of the average frequency of all words\\nacross all clusters divided by the total frequency of each word.', 'Figure 5-14. Creating a weighting scheme.\\nThe result is a weight (“IDF”) for each word that we can multiply with their\\nfrequency (“c-TF”) to get the weighted values (“c-TF-IDF”).\\nThis second part of the procedure, as shown in Figure 5-15, allows for\\ngenerating a distribution over words as we have seen before. We can use\\nscikit-learn’s CountVectorizer to generate the bag-of-words (or term\\nfrequency) representation. Here, each cluster is considered a topic that has a\\nspecific ranking of the corpus’s vocabulary.', 'Figure 5-15. The second part of BERTopic’s pipeline is representing the topics: the calculation of the\\nweight of term *x* in a class *c*.\\nPutting the two steps together, clustering and representing topics, results in\\nthe full pipeline of BERTopic, as illustrated in Figure 5-16. With this\\npipeline, we can cluster semantically similar documents and from the\\nclusters generate topics represented by several keywords. The higher a\\nword’s weight in a topic, the more representative it is of that topic.\\nFigure 5-16. The full pipeline of BERTopic, roughly, consists of two steps, clustering and topic\\nrepresentation.\\nA major advantage of this pipeline is that the two steps, clustering and topic\\nrepresentation, are largely independent of one another. For instance, with c-\\nTF-IDF, we are not dependent on the models used in clustering the\\ndocuments. This allows for significant modularity throughout every\\ncomponent of the pipeline. And as we will explore later in this chapter, it is\\na great starting point to fine-tune the topic representations.', 'As illustrated in Figure 5-17, although sentence-transformers is\\nused as the default embedding model, we can swap it with any other\\nembedding technique. The same applies to all other steps. If you do not\\nwant outliers generated with HDBSCAN, you can use k-means instead.\\nFigure 5-17. The modularity of BERTopic is a key component and allows you to build your own topic\\nmodel however you want.\\nYou can think of this modularity as building with Lego blocks; each part of\\nthe pipeline is completely replaceable with another, similar algorithm.\\nThrough this modularity, newly released models can be integrated within its\\narchitecture. As the field of Language AI grows, so does BERTopic!', 'THE MODULARITY OF BERTOPIC\\nThe modularity of BERTopic has another advantage: it allows it to be\\nused and adapted to different use cases using the same base model. For\\ninstance, BERTopic supports a wide variety of algorithmic variants:\\nGuided topic modeling\\n(Semi-)supervised topic modeling\\nHierarchical topic modeling\\nDynamic topic modeling\\nMultimodal topic modeling\\nMulti-aspect topic modeling\\nOnline and incremental topic modeling\\nZero-shot topic modeling\\nEtc.\\nThe modularity and algorithmic flexibility are the foundation of the\\nauthor’s aim to make BERTopic the one-stop-shop for topic modeling.\\nYou can find a full overview of its capabilities in the documentation or\\nthe repository.\\nTo run BERTopic with our ArXiv dataset, we can use our previously\\ndefined models and embeddings (although it is not mandatory):\\nfrom bertopic import BERTopic\\n# Train our model with our previously defined models\\ntopic_model = BERTopic(\\n    embedding_model=embedding_model,\\n    umap_model=umap_model,\\n    hdbscan_model=hdbscan_model,', '    verbose=True\\n).fit(abstracts, embeddings)\\nLet us start by exploring the topics that were created. The\\nget_topic_info() method is useful to get a quick description of the\\ntopics that we found:', '', 'topic_model.get_topic_info()\\nTopic Count Name\\n-1 14520 -1_the_of_and_to\\n0 2290 0_speech_asr_recognition_end\\n1 1403 1_medical_clinical_biomedical_pat\\n2 1156 2_sentiment_aspect_analysis_revie\\n3 986 3_translation_nmt_machine_neural\\n... ... ...\\n150 54 150_coherence_discourse_paragrap\\n151 54 151_prompt_prompts_optimization', 'Topic Count Name\\n152 53 152_sentence_sts_embeddings_sim\\n153 53 153_counseling_mental_health_the\\n154 50 154_backdoor_attacks_attack_trigg\\nEach of these topics is represented by several keywords, which are\\nconcatenated with a “_” in the Name column. This Name column allows us\\nto quickly get a feeling of what the topic is about as it shows the four\\nkeywords that best represent it.\\nNOTE\\nYou might also have noticed that the very first topic is labeled -1. That topic contains all\\ndocuments that could not be fitted within a topic and are considered outliers. This is a\\nresult of the clustering algorithm, HDBSCAN, which does not force all points to be\\nclustered. To remove outliers, we could either use a non-outlier algorithm like k-means\\nor use BERTopic’s reduce_outliers() function to reassign the outliers to topics.\\nWe can inspect individual topics and explore which keywords best represent\\nthem with the get_topic function. For example, topic 0 contains the', 'following keywords:\\ntopic_model.get_topic(0)\\n[(\\'speech\\', 0.028177697715245358),\\n (\\'asr\\', 0.018971184497453525),\\n (\\'recognition\\', 0.013457745472471012),\\n (\\'end\\', 0.00980445092749381),\\n (\\'acoustic\\', 0.009452082794507863),\\n (\\'speaker\\', 0.0068822647060204885),\\n (\\'audio\\', 0.006807649923681604),\\n (\\'the\\', 0.0063343444687017645),\\n (\\'error\\', 0.006320144717019838),\\n (\\'automatic\\', 0.006290216996043161)]\\nFor example, topic 0 contains the keywords “speech,” “asr,” and\\n“recognition.” Based on these keywords, it seems that the topic is about\\nautomatic speech recognition (ASR).\\nWe can use the find_topics() function to search for specific topics\\nbased on a search term. Let’s search for a topic about topic modeling:\\ntopic_model.find_topics(\"topic modeling\")\\n([22, -1, 1, 47, 32],\\n [0.95456535, 0.91173744, 0.9074769, 0.9067007, 0.90510106])\\nThis returns that topic 22 has a relatively high similarity (0.95) with our\\nsearch term. If we then inspect the topic, we can see that it is indeed a topic\\nabout topic modeling:\\ntopic_model.get_topic(22)\\n[(\\'topic\\', 0.06634619076655907),\\n (\\'topics\\', 0.035308535091932707),\\n (\\'lda\\', 0.016386314730705634),\\n (\\'latent\\', 0.013372311924864435),\\n (\\'document\\', 0.012973600191120576),', ' (\\'documents\\', 0.012383715497143821),\\n (\\'modeling\\', 0.011978375291037142),\\n (\\'dirichlet\\', 0.010078277589545706),\\n (\\'word\\', 0.008505619415413312),\\n (\\'allocation\\', 0.007930890698168108)]\\nAlthough we know that this topic is about topic modeling, let’s see if the\\nBERTopic abstract is also assigned to this topic:\\ntopic_model.topics_[titles.index(\"BERTopic: Neural topic modeling \\nwith a class-based TF-IDF procedure\")]\\n22\\nIt is! These functionalities allow us to quickly find the topics that we are\\ninterested in.\\nTIP\\nThe modularity of BERTopic gives you a lot of choices, which can be overwhelming.\\nFor that purpose, the author created a best practices guide that goes through common\\npractices to speed up training, improve representations, and more.\\nTo make exploration of the topics a bit easier, we can look back at our text\\nclustering example. There, we created a static visualization to see the\\ngeneral structure of the created topic. With BERTopic, we can create an\\ninteractive variant that allows us to quickly explore which topics exist and\\nwhich documents they contain.\\nDoing so requires us to use the two-dimensional embeddings,\\nreduced_embeddings, that we created with UMAP. Moreover, when\\nwe hover over documents, we will show the title instead of the abstract to\\nquickly get an understanding of the documents in a topic:\\n# Visualize topics and documents', 'fig = topic_model.visualize_documents(\\n    titles, \\n    reduced_embeddings=reduced_embeddings, \\n    width=1200, \\n    hide_annotations=True\\n)\\n# Update fonts of legend for easier visualization\\nfig.update_layout(font=dict(size=16))\\nAs we can see in Figure 5-18, this interactive plot quickly gives us a sense\\nof the created topics. You can zoom in to view individual documents or\\ndouble-click a topic on the righthand side to only view it.\\nFigure 5-18. The output when we visualize documents and topics.\\nThere is a wide range of visualization options in BERTopic. There are three\\nthat are worthwhile to explore to get an idea of the relationships between\\ntopics:\\n# Visualize barchart with ranked keywords', 'topic_model.visualize_barchart()\\n# Visualize relationships between topics\\ntopic_model.visualize_heatmap(n_clusters=30)\\n# Visualize the potential hierarchical structure of topics\\ntopic_model.visualize_hierarchy()\\nAdding a Special Lego Block\\nThe pipeline in BERTopic that we have explored thus far, albeit fast and\\nmodular, has a disadvantage: it still represents a topic through a bag-of-\\nwords without taking into account semantic structures.\\nThe solution is to leverage the strength of the bag-of-words representation,\\nwhich is its speed to generate a meaningful representation. We can use this\\nfirst meaningful representation and tweak it using more powerful but slower\\ntechniques, like embedding models. As shown in Figure 5-19, we can\\nrerank the initial distribution of words to improve the resulting\\nrepresentation. Note that this idea of reranking an initial set of results is a\\nmain staple in neural search, a subject that we cover in Chapter 8.\\nFigure 5-19. Fine-tune the topic representations by reranking the original c-TF-IDF distributions.\\nAs a result, we can design a new Lego block, as shown in Figure 5-20, that\\ntakes in this first topic representation and spits out an improved\\nrepresentation.', 'In BERTopic, such reranker models are referred to as representation\\nmodels. A major benefit of this approach is that the optimization of topic\\nrepresentations only needs to be done as many times as we have topics. For\\ninstance, if we have millions of documents and a hundred topics, the\\nrepresentation block only needs to be applied once for every topic instead\\nof for every document.\\nAs shown in Figure 5-21, a wide variety of representation blocks have been\\ndesigned for BERTopic that allows you to fine-tune the representations. The\\nrepresentation block can even be stacked multiple times to fine-tune\\nrepresentations using different methodologies.\\nFigure 5-20. The reranker (representation) block sits on top of the c-TF-IDF representation.', 'Figure 5-21. After applying the c-TF-IDF weighting, topics can be fine-tuned with a wide variety of\\nrepresentation models, many of which are large language models.\\nBefore we explore how we can use these representation blocks, we first\\nneed to do two things. First, we are going to save our original topic\\nrepresentations so that it will be much easier to compare with and without\\nrepresentation models:\\n# Save original representations\\nfrom copy import deepcopy\\noriginal_topics = deepcopy(topic_model.topic_representations_)\\nSecond, let’s create a short wrapper that we can use to quickly visualize the\\ndifferences in topic words to compare with and without representation\\nmodels:', 'def topic_differences(model, original_topics, nr_topics=5):\\n    \"\"\"Show the differences in topic representations between two \\nmodels \"\"\"\\n    df = pd.DataFrame(columns=[\"Topic\", \"Original\", \"Updated\"])\\n    for topic in range(nr_topics):\\n        # Extract top 5 words per topic per model\\n        og_words = \" | \".join(list(zip(*original_topics[topic]))\\n[0][:5])\\n        new_words = \" | \".join(list(zip(*model.get_topic(topic)))\\n[0][:5])\\n        df.loc[len(df)] = [topic, og_words, new_words]\\n    \\n    return df\\nKeyBERTInspired\\nThe first representation block that we are going to explore is\\nKeyBERTInspired. KeyBERTInspired is, as you might have guessed, a\\nmethod inspired by the keyword extraction package, KeyBERT.7  KeyBERT\\nextracts keywords from texts by comparing word and document\\nembeddings through cosine similarity.\\nBERTopic uses a similar approach. KeyBERTInspired uses c-TF-IDF to\\nextract the most representative documents per topic by calculating the\\nsimilarity between a document’s c-TF-IDF values and those of the topic\\nthey correspond to. As shown in Figure 5-22, the average document\\nembedding per topic is calculated and compared to the embeddings of\\ncandidate keywords to rerank the keywords.', 'Figure 5-22. KeyBERTInspired representation model procedure.\\nDue to the modular nature of BERTopic, we can update our initial topic\\nrepresentations with KeyBERTInspired without needing to perform the\\ndimensionality reduction and clustering steps:\\nfrom bertopic.representation import KeyBERTInspired\\n# Update our topic representations using KeyBERTInspired\\nrepresentation_model = KeyBERTInspired()\\ntopic_model.update_topics(abstracts, \\nrepresentation_model=representation_model)', '# Show topic differences\\ntopic_differences(topic_model, original_topics)\\nTopic Original Updated\\n0 speech | asr |\\nrecognition | end |\\nacoustic\\nspeech | encoder |\\nphonetic | language |\\ntrans...\\n1 medical | clinical |\\nbiomedical | patient |\\nhe...\\nnlp | ehr | clinical |\\nbiomedical | language\\n2 sentiment | aspect |\\nanalysis | reviews |\\nopinion\\naspect | sentiment |\\naspects | sentiments |\\ncl...\\n3 translation | nmt |\\nmachine | neural | bleu\\ntranslation | translating |\\ntranslate | transl...\\n4 summarization |\\nsummaries | summary |\\nabstract...\\nsummarization |\\nsummarizers |\\nsummaries | summ...\\nThe updated model shows that the topics are easier to read compared to the\\noriginal model. It also demonstrates the downside of using embedding-\\nbased techniques. Words in the original model, like nmt (topic 3), which\\nstands for neural machine translation, are removed as the model could not\\nproperly represent the entity. For domain experts, these abbreviations are\\nhighly informative.', 'Maximal marginal relevance\\nWith c-TF-IDF and the previously shown KeyBERTInspired techniques, we\\nstill have significant redundancy in the resulting topic representations. For\\ninstance, having both the words “summaries” and “summary” in a topic\\nrepresentation introduces redundancy as they are quite similar.\\nWe can use maximal marginal relevance (MMR) to diversify our topic\\nrepresentations. The algorithm attempts to find a set of keywords that are\\ndiverse from one another but still relate to the documents they are compared\\nto. It does so by embedding a set of candidate keywords and iteratively\\ncalculating the next best keyword to add. Doing so requires setting a\\ndiversity parameter, which indicates how diverse keywords need to be.\\nIn BERTopic, we use MMR to go from a set of initial keywords, let’s say\\n30, to a smaller but more diverse set of keywords, let’s say 10. It filters out\\nredundant words and only keeps words that contribute something new to the\\ntopic representation.\\nDoing so is rather straightforward:\\nfrom bertopic.representation import MaximalMarginalRelevance\\n# Update our topic representations to MaximalMarginalRelevance\\nrepresentation_model = MaximalMarginalRelevance(diversity=0.2)\\ntopic_model.update_topics(abstracts, \\nrepresentation_model=representation_model)', '# Show topic differences\\ntopic_differences(topic_model, original_topics)\\nTopic Original Updated\\n0 speech | asr | recognition\\n| end | acoustic\\nspeech | asr | error |\\nmodel | training\\n1 medical | clinical |\\nbiomedical | patient |\\nhe...\\nclinical | biomedical |\\npatient | healthcare |...\\n2 sentiment | aspect |\\nanalysis | reviews |\\nopinion\\nsentiment | analysis |\\nreviews | absa | polarity\\n3 translation | nmt |\\nmachine | neural | bleu\\ntranslation | nmt | bleu |\\nparallel | multilin...\\n4 summarization |\\nsummaries | summary |\\nabstract...\\nsummarization |\\ndocument | extractive |\\nrouge ...\\nThe resulting topics demonstrate more diversity in their representations. For\\ninstance, topic 4 only shows one “summary”-like word and instead adds\\nother words that might contribute more to the overall representation.\\nTIP\\nBoth KeyBERTInspired and MMR are amazing techniques for improving the first set of\\ntopic representations. KeyBERTInspired especially tends to remove nearly all stop\\nwords since it focuses on the semantic relationships between words and documents.', 'The Text Generation Lego Block\\nThe representation block in BERTopic has been acting as a reranking block\\nin our previous examples. However, as we already explored in the previous\\nchapter, generative models have great potential for a wide variety of tasks.\\nWe can use generative models in BERTopic quite efficiently by following a\\npart of the reranking procedure. Instead of using a generative model to\\nidentify the topic of all documents, of which there can potentially be\\nmillions, we will use the model to generate a label for our topic. As\\nillustrated in Figure 5-23, instead of generating or reranking keywords, we\\nask the model to generate a short label based on keywords that were\\npreviously generated and a small set of representative documents.\\nFigure 5-23. Use text generative LLMs and prompt engineering to create labels for topics from\\nkeywords and documents related to each topic.\\nThere are two components to the illustrated prompt. First, the documents\\nthat are inserted using the [DOCUMENTS] tag are a small subset of\\ndocuments, typically four, that best represent the topic. The documents with\\nthe highest cosine similarity of their c-TF-IDF values with those of the\\ntopic are selected. Second, the keywords that make up a topic are also\\npassed to the prompt and referenced using the [KEYWORDS] tag. The', 'keywords could be generated by c-TF-IDF or any of the other\\nrepresentations we discussed thus far.\\nAs a result, we only need to use the generative model once for every topic,\\nof which there could be potentially hundreds, instead of once for each\\ndocument, of which there could potentially be millions. There are many\\ngenerative models that we can choose from, both open source and\\nproprietary. Let’s start with a model that we have explored in the previous\\nchapter, the Flan-T5 model.\\nWe create a prompt that works well with the model and use it in BERTopic\\nthrough the representation_model parameter:\\nfrom transformers import pipeline\\nfrom bertopic.representation import TextGeneration\\nprompt = \"\"\"I have a topic that contains the following documents: \\n[DOCUMENTS]\\nThe topic is described by the following keywords: \\'[KEYWORDS]\\'.\\nBased on the documents and keywords, what is this topic about?\"\"\"\\n# Update our topic representations using Flan-T5\\ngenerator = pipeline(\"text2text-generation\", model=\"google/flan-\\nt5-small\")\\nrepresentation_model = TextGeneration(\\n    generator, prompt=prompt, doc_length=50, \\ntokenizer=\"whitespace\"\\n)\\ntopic_model.update_topics(abstracts, \\nrepresentation_model=representation_model)', '# Show topic differences\\ntopic_differences(topic_model, original_topics)\\nTopic Original Updated\\n0 speech | asr | recognition\\n| end | acoustic\\nSpeech-to-description\\n1 medical | clinical |\\nbiomedical | patient |\\nhe...\\nScience/Tech\\n2 sentiment | aspect |\\nanalysis | reviews |\\nopinion\\nReview\\n3 translation | nmt |\\nmachine | neural | bleu\\nAttention-based neural\\nmachine translation\\n4 summarization |\\nsummaries | summary |\\nabstract...\\nSummarization\\nSome of these labels, like “Summarization” seem to be logical when\\ncomparing them to the original representations. Others, however, like\\n“Science/Tech,” seem quite broad and do not do the original topic justice.\\nLet’s explore instead how OpenAI’s GPT-3.5 would perform considering\\nthe model is not only larger but expected to have more linguistic\\ncapabilities:\\nimport openai\\nfrom bertopic.representation import OpenAI', 'prompt = \"\"\"\\nI have a topic that contains the following documents:\\n[DOCUMENTS]\\nThe topic is described by the following keywords: [KEYWORDS]\\nBased on the information above, extract a short topic label in \\nthe following format:\\ntopic: <short topic label>\\n\"\"\"\\n# Update our topic representations using GPT-3.5\\nclient = openai.OpenAI(api_key=\"YOUR_KEY_HERE\")\\nrepresentation_model = OpenAI(\\n    client, model=\"gpt-3.5-turbo\", exponential_backoff=True, \\nchat=True, prompt=prompt\\n)\\ntopic_model.update_topics(abstracts, \\nrepresentation_model=representation_model)\\n# Show topic differences\\ntopic_differences(topic_model, original_topics)\\nTopic Original Updated\\n0 speech | asr |\\nrecognition | end |\\nacoustic\\nLeveraging External\\nData for Improving\\nLow-Res...\\n1 medical | clinical |\\nbiomedical | patient |\\nhe...\\nImproved\\nRepresentation Learning\\nfor Biomedica...\\n2 sentiment | aspect |\\nanalysis | reviews |\\nopinion\\nAdvancements in\\nAspect-Based Sentiment\\nAnalys...\\n3 translation | nmt |\\nmachine | neural | bleu\\nNeural Machine\\nTranslation', 'Topic Original Updated\\nEnhancements\\n4 summarization |\\nsummaries | summary |\\nabstract...\\nDocument\\nSummarization\\nTechniques\\nThe resulting labels are quite impressive! We are not even using GPT-4 and\\nthe resulting labels seem to be more informative than our previous example.\\nNote that BERTopic is not confined to only using OpenAI’s offering but has\\nlocal backends as well.\\nTIP\\nAlthough it seems like we do not need the keywords anymore, they are still\\nrepresentative of the input documents. No model is perfect and it is generally advised to\\ngenerate multiple topic representations. BERTopic allows for all topics to be represented\\nby different representations. You could, for example, use KeyBERTInspired, MMR, and\\nGPT-3.5 side by side to get different perspectives on the same topic.\\nWith these GPT-3.5 generated labels, we can create beautiful illustrations\\nusing the datamapplot package (Figure 5-24):\\n# Visualize topics and documents\\nfig = topic_model.visualize_document_datamap(\\n    titles,\\n    topics=list(range(20)),\\n    reduced_embeddings=reduced_embeddings,\\n    width=1200,\\n    label_font_size=11,\\n    label_wrap_width=20,\\n    use_medoids=True,\\n)', 'Figure 5-24. The top 20 topics visualized.\\nSummary\\nIn this chapter, we explored how LLMs, both generative and representative,\\ncan be used in the domain of unsupervised learning. Despite supervised\\nmethods like classification being prevalent in recent years, unsupervised\\napproaches such as text clustering hold immense potential due to their\\nability to group texts based on semantic content without prior labeling.', 'We covered a common pipeline for clustering textual documents that starts\\nwith converting input text into numerical representations, which we call\\nembeddings. Then, dimensionality reduction is applied to these embeddings\\nto simplify high-dimensional data for better clustering outcomes. Finally, a\\nclustering algorithm on the dimensionality-reduced embeddings is applied\\nto cluster the input text. Manually inspecting the clusters helped us\\nunderstand which documents they contained and how to interpret these\\nclusters.\\nTo transition away from this manual inspection, we explored how\\nBERTopic extends this text clustering pipeline with a method for\\nautomatically representing the clusters. This methodology is often referred\\nto as topic modeling, which attempts to uncover themes within large\\namounts of documents. BERTopic generates these topic representations\\nthrough a bag-of-words approach enhanced with c-TF-IDF, which weighs\\nwords based on their cluster relevance and frequency across all clusters.\\nA major benefit of BERTopic is its modular nature. In BERTopic, you can\\nchoose any model in the pipeline, which allows for additional\\nrepresentations of topics that create multiple perspectives of the same topic.\\nWe explored maximal marginal relevance and KeyBERTInspired as\\nmethodologies to fine-tune the topic representations generated with c-TF-\\nIDF. Additionally, we used the same generative LLMs as in the previous\\nchapter (Flan-T5 and GPT-3.5) to further improve the interpretability of\\ntopics by generating highly interpretable labels.\\nIn the next chapter, we shift focus and explore a common method for\\nimproving the output of generative models, namely prompt engineering.\\n1  Harold Hotelling. “Analysis of a complex of statistical variables into principal components.”\\nJournal of Educational Psychology 24.6 (1933): 417.\\n2  Leland McInnes, John Healy, and James Melville. “UMAP: Uniform Manifold\\nApproximation and Projection for dimension reduction.” arXiv preprint arXiv:1802.03426\\n(2018).', '3  Leland McInnes, John Healy, and Steve Astels. “hdbscan: Hierarchical density based\\nclustering.” J. Open Source Softw. 2.11 (2017): 205.\\n4  Martin Ester et al. “A density-based algorithm for discovering clusters in large spatial\\ndatabases with noise.” KDD’96, Aug. 1996: 226–231.\\n5  David M. Blei, Andrew Y. Ng, and Michael I. Jordan. “Latent Dirichlet allocation.” Journal of\\nMachine Learning Research 3. Jan (2003): 993–1022.\\n6  Maarten Grootendorst. “BERTopic: Neural topic modeling with a class-based TF-IDF\\nprocedure.” arXiv preprint arXiv:2203.05794 (2022).\\n7  Maarten Grootendorst. “KeyBERT: Minimal keyword extraction with BERT.” (2020).\\nOceanofPDF.com', 'Chapter 6. Prompt Engineering\\nIn the first chapters of this book, we took our first steps into the world of\\nlarge language models (LLMs). We delved into various applications, such\\nas supervised and unsupervised classification, employing models that focus\\non representing text, like BERT and its derivatives.\\nAs we progressed, we used models trained primarily for text generation,\\nmodels that are often referred to as generative pre-trained transformers\\n(GPT). These models have the remarkable ability to generate text in\\nresponse to prompts from the user. Through prompt engineering, we can\\ndesign these prompts in a way that enhances the quality of the generated\\ntext.\\nIn this chapter, we will explore these generative models in more detail and\\ndive into the realm of prompt engineering, reasoning with generative\\nmodels, verification, and even evaluating their output.\\nUsing Text Generation Models\\nBefore we start with the fundamentals of prompt engineering, it is essential\\nto explore the basics of utilizing a text generation model. How do we select\\nthe model to use? Do we use a proprietary or open source model? How can\\nwe control the generated output? These questions will serve as our stepping\\nstones into using text generation models.\\nChoosing a Text Generation Model\\nChoosing a text generation model starts with choosing between proprietary\\nmodels or open source models. Although proprietary models are generally\\nmore performant, we focus in this book more on open source models as\\nthey offer more flexibility and are free to use.', 'Figure 6-1 shows a small selection of impactful foundation models, LLMs\\nthat have been pretrained on vast amounts of text data and are often fine-\\ntuned for specific applications.\\nFigure 6-1. Foundation models are often released in several different sizes.\\nFrom those foundation models, hundreds if not thousands of models have\\nbeen fine-tuned, one more suitable for certain tasks than another. Choosing\\nthe model to use can be a daunting task!\\nWe advise starting with a small foundation model. So let’s continue using\\nPhi-3-mini, which has 3.8 billion parameters. This makes it suitable for\\nrunning with devices up to 8 GB of VRAM. Overall, scaling up to larger\\nmodels tends to be a nicer experience than scaling down. Smaller models\\nprovide a great introduction and lay a solid foundation for progressing to\\nlarger models.\\nLoading a Text Generation Model\\nThe most straightforward method of loading a model, as we have done in\\nprevious chapters, is by leveraging the transformers library:\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, \\npipeline\\n# Load model and tokenizer\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"microsoft/Phi-3-mini-4k-instruct\",', '    device_map=\"cuda\",\\n    torch_dtype=\"auto\",\\n    trust_remote_code=True,\\n)\\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-\\n4k-instruct\")\\n# Create a pipeline\\npipe = pipeline(\\n    \"text-generation\",\\n    model=model,\\n    tokenizer=tokenizer,\\n    return_full_text=False,\\n    max_new_tokens=500,\\n    do_sample=False,\\n)\\nCompared to previous chapters, we will take a closer look at developing\\nand using the prompt template.\\nTo illustrate, let’s revisit the example from Chapter 1 where we asked the\\nLLM to make a joke about chickens:\\n# Prompt\\nmessages = [\\n    {\"role\": \"user\", \"content\": \"Create a funny joke about \\nchickens.\"}\\n]\\n# Generate the output\\noutput = pipe(messages)\\nprint(output[0][\"generated_text\"])\\nWhy don\\'t chickens like to go to the gym? Because they can\\'t \\ncrack the egg-sistence of it!\\nUnder the hood, transformers.pipeline first converts our messages\\ninto a specific prompt template. We can explore this process by accessing\\nthe underlying tokenizer:', '# Apply prompt template\\nprompt = pipe.tokenizer.apply_chat_template(messages, \\ntokenize=False)\\nprint(prompt)\\n<s><|user|>\\nCreate a funny joke about chickens.<|end|>\\n<|assistant|>\\nYou may recognize the special tokens <|user|> and <|assistant|>\\nfrom Chapter 2. This prompt template, further illustrated in Figure 6-2, was\\nused during the training of the model. Not only does it provide information\\nabout who said what, but it is also used to indicate when the model should\\nstop generating text (see the <|end|> token). This prompt is passed\\ndirectly to the LLM and processed all at once.\\nIn the next chapter, we will customize parts of this template ourselves.\\nThroughout this chapter, we can use transformers.pipeline to\\nhandle chat template processing for us. Next, let us explore how we can\\ncontrol the output of the model.\\nFigure 6-2. The template Phi-3 expects when interacting with the model.', 'Controlling Model Output\\nOther than prompt engineering, we can control the kind of output we want\\nby adjusting the model parameters. In our previous example, you might\\nhave noticed that we used several parameters in the pipe function,\\nincluding temperature and top_p.\\nThese parameters control the randomness of the output. A part of what\\nmakes LLMs exciting technology is that it can generate different responses\\nfor the exact same prompt. Each time an LLM needs to generate a token, it\\nassigns a likelihood number to each possible token.\\nAs illustrated in Figure 6-3, in the sentence “I am driving a…” the\\nlikelihood of that sentence being followed by tokens like “car” or “truck” is\\ngenerally higher than a token like “elephant.” However, there is still a\\npossibility of “elephant” being generated but it is much lower.\\nFigure 6-3. The model chooses the next token to generate based on their likelihood scores.\\nWhen we loaded our model, we purposefully set do_sample=False to\\nmake sure the output is somewhat consistent. This means that no sampling\\nwill be done and only the most probable next token is selected. However, to\\nuse the temperature and top_p parameters, we will set\\ndo_sample=True in order to make use of them.\\nTemperature\\nThe temperature controls the randomness or creativity of the text\\ngenerated. It defines how likely it is to choose tokens that are less probable.\\nThe underlying idea is that a temperature of 0 generates the same', 'response every time because it always chooses the most likely word. As\\nillustrated in Figure 6-4, a higher value allows less probable words to be\\ngenerated.\\nFigure 6-4. A higher temperature increases the likelihood that less probable tokens are\\ngenerated and vice versa.\\nAs a result, a higher temperature (e.g., 0.8) generally results in a more\\ndiverse output while a lower temperature (e.g., 0.2) creates a more\\ndeterministic output.\\nYou can use temperature in your pipeline as follows:\\n# Using a high temperature\\noutput = pipe(messages, do_sample=True, temperature=1)\\nprint(output[0][\"generated_text\"])\\nWhy don\\'t chickens like to go on a rollercoaster? Because \\nthey\\'re afraid they might suddenly become chicken-soup!\\nNote that every time you rerun this piece of code, the output will change!\\ntemperature introduces stochastic behavior since the model now\\nrandomly selects tokens.\\ntop_p\\ntop_p, also known as nucleus sampling, is a sampling technique that\\ncontrols which subset of tokens (the nucleus) the LLM can consider. It will\\nconsider tokens until it reaches their cumulative probability. If we set', 'top_p to 0.1, it will consider tokens until it reaches that value. If we set\\ntop_p to 1, it will consider all tokens.\\nAs shown in Figure 6-5, by lowering the value, it will consider fewer tokens\\nand generally give less “creative” output, while increasing the value allows\\nthe LLM to choose from more tokens.\\nFigure 6-5. A higher top_p increases the number of tokens that can be selected to generate and vice\\nversa.\\nSimilarly, the top_k parameter controls exactly how many tokens the\\nLLM can consider. If you change its value to 100, the LLM will only\\nconsider the top 100 most probable tokens.\\nYou can use top_p in your pipeline as follows:\\n# Using a high top_p\\noutput = pipe(messages, do_sample=True, top_p=1)\\nprint(output[0][\"generated_text\"])\\nWhy don\\'t chickens make good comedians? Because their \\'jokes\\' \\nalways \\'feather\\' the truth!\\nAs shown in Table 6-1, these parameters allow the user to have a sliding\\nscale between being creative (high temperature and top_p) and being\\npredictable (lower temperature and top_p).', 'Table 6-1. Use case examples when selecting values for temperature and to\\np_p.\\nExample use\\ncase Temperature top_p Description\\nBrainstorming\\nsession\\nHigh High High randomness\\nwith large pool of\\npotential tokens. The\\nresults will be\\nhighly diverse, often\\nleading to very\\ncreative and\\nunexpected results.\\nEmail\\ngeneration\\nLow Low Deterministic output\\nwith high probable\\npredicted tokens.\\nThis results in\\npredictable, focused,\\nand conservative\\noutputs.\\nCreative\\nwriting\\nHigh Low High randomness\\nwith a small pool of\\npotential tokens.\\nThis combination\\nproduces creative\\noutputs but still\\nremains coherent.\\nTranslation Low High Deterministic output\\nwith high probable\\npredicted tokens.\\nProduces coherent\\noutput with a wider', 'Example use\\ncase Temperature top_p Description\\nrange of vocabulary,\\nleading to outputs\\nwith linguistic\\nvariety.\\nIntro to Prompt Engineering\\nAn essential part of working with text-generative LLMs is prompt\\nengineering. By carefully designing our prompts we can guide the LLM to\\ngenerate desired responses. Whether the prompts are questions, statements,\\nor instructions, the main goal of prompt engineering is to elicit a useful\\nresponse from the model.\\nPrompt engineering is more than designing effective prompts. It can be used\\nas a tool to evaluate the output of a model as well as to design safeguards\\nand safety mitigation methods. This is an iterative process of prompt\\noptimization and requires experimentation. There is not and unlikely will\\never be a perfect prompt design.\\nIn this section, we will go through common methods for prompt\\nengineering, and small tips and tricks to understand what the effect is of\\ncertain prompts. These skills allow us to understand the capabilities of\\nLLMs and lie at the foundation of interfacing with these kinds of models.\\nWe begin by answering the question: what should be in a prompt?\\nThe Basic Ingredients of a Prompt\\nAn LLM is a prediction machine. Based on a certain input, the prompt, it\\ntries to predict the words that might follow it. At its core (illustrated in\\nFigure 6-6), the prompt does not need to be more than a few words to elicit\\na response from the LLM.', 'Figure 6-6. A basic example of a prompt. No instruction is given so the LLM will simply try to\\ncomplete the sentence.\\nHowever, although the illustration works as a basic example, it fails to\\ncomplete a specific task. Instead, we generally approach prompt\\nengineering by asking a specific question or task the LLM should complete.\\nTo elicit the desired response, we need a more structured prompt.\\nFor example, and as shown in Figure 6-7, we could ask the LLM to classify\\na sentence into either having positive or negative sentiment. This extends\\nthe most basic prompt to one consisting of two components—the\\ninstruction itself and the data that relates to the instruction.', 'Figure 6-7. Two components of a basic instruction prompt: the instruction itself and the data it refers\\nto.\\nMore complex use cases might require more components in a prompt. For\\ninstance, to make sure the model only outputs “negative” or “positive” we\\ncan introduce output indicators that help guide the model. In Figure 6-8, we\\nprefix the sentence with “Text:” and add “Sentiment:” to prevent the model\\nfrom generating a complete sentence. Instead, this structure indicates that\\nwe expect either “negative” or “positive.” Although the model might not\\nhave been trained on these components directly, it was fed enough\\ninstructions to be able to generalize to this structure.', 'Figure 6-8. Extending the prompt with an output indicator that allows for a specific output.\\nWe can continue adding or updating the elements of a prompt until we elicit\\nthe response we are looking for. We could add additional examples,\\ndescribe the use case in more detail, provide additional context, etc. These\\ncomponents are merely examples and not a limited set of possibilities. The\\ncreativity that comes with designing these components is key.\\nAlthough a prompt is a single piece of text, it is tremendously helpful to\\nthink of prompts as pieces of a larger puzzle. Have I described the context\\nof my question? Does the prompt have an example of the output?\\nInstruction-Based Prompting\\nAlthough prompting comes in many flavors, from discussing philosophy\\nwith the LLM to role-playing with your favorite superhero, prompting is\\noften used to have the LLM answer a specific question or resolve a certain\\ntask. This is referred to as instruction-based prompting.', 'Figure 6-9 illustrates a number of use cases in which instruction-based\\nprompting plays an important role. We already did one of these in the\\nprevious example, namely supervised classification.\\nFigure 6-9. Use cases for instruction-based prompting.\\nEach of these tasks requires different prompting formats and more\\nspecifically, asking different questions of the LLM. Asking the LLM to\\nsummarize a piece of text will not suddenly result in classification. To\\nillustrate, examples of prompts for some of these use cases can be found in\\nFigure 6-10.', 'Figure 6-10. Prompt examples of common use cases. Notice how within a use case, the structure and\\nlocation of the instruction can be changed.\\nAlthough these tasks require different instructions, there is actually a lot of\\noverlap in the prompting techniques used to improve the quality of the\\noutput. A non-exhaustive list of these techniques includes:\\nSpecificity\\nAccurately describe what you want to achieve. Instead of asking the\\nLLM to “Write a description for a product” ask it to “Write a', 'description for a product in less than two sentences and use a formal\\ntone.”\\nHallucination\\nLLMs may generate incorrect information confidently, which is referred\\nto as hallucination. To reduce its impact, we can ask the LLM to only\\ngenerate an answer if it knows the answer. If it does not know the\\nanswer, it can respond with “I don’t know.”\\nOrder\\nEither begin or end your prompt with the instruction. Especially with\\nlong prompts, information in the middle is often forgotten.1  LLMs tend\\nto focus on information either at the beginning of a prompt (primacy\\neffect) or the end of a prompt (recency effect).\\nHere, specificity is arguably the most important aspect. By restricting and\\nspecifying what the model should generate, there is a smaller chance of\\nhaving it generate something not related to your use case. For instance, if\\nwe were to skip the instruction “in two to three sentences” it might generate\\ncomplete paragraphs. Like human conversations, without any specific\\ninstructions or additional context, it is difficult to derive what the task at\\nhand actually is.\\nAdvanced Prompt Engineering\\nOn the surface, creating a good prompt might seem straightforward. Ask a\\nspecific question, be accurate, add some examples, and you are done!\\nHowever, prompting can grow complex quite quickly and as a result is an\\noften-underestimated component of leveraging LLMs.', 'Here, we will go through several advanced techniques for building up your\\nprompts, starting with the iterative workflow of building up complex\\nprompts all the way to using LLMs sequentially to get improved results.\\nEventually, we will even build up to advanced reasoning techniques.\\nThe Potential Complexity of a Prompt\\nAs we explored in the intro to prompt engineering, a prompt generally\\nconsists of multiple components. In our very first example, our prompt\\nconsisted of instruction, data, and output indicators. As we mentioned\\nbefore, no prompt is limited to just these three components and you can\\nbuild it up to be as complex as you want.\\nThese advanced components can quickly make a prompt quite complex.\\nSome common components are:\\nPersona\\nDescribe what role the LLM should take on. For example, use “You are\\nan expert in astrophysics” if you want to ask a question about\\nastrophysics.\\nInstruction\\nThe task itself. Make sure this is as specific as possible. We do not want\\nto leave much room for interpretation.\\nContext\\nAdditional information describing the context of the problem or task. It\\nanswers questions like “What is the reason for the instruction?”\\nFormat\\nThe format the LLM should use to output the generated text. Without it,\\nthe LLM will come up with a format itself, which is troublesome in', 'automated systems.\\nAudience\\nThe target of the generated text. This also describes the level of the\\ngenerated output. For education purposes, it is often helpful to use ELI5\\n(“Explain it like I’m 5”).\\nTone\\nThe tone of voice the LLM should use in the generated text. If you are\\nwriting a formal email to your boss, you might not want to use an\\ninformal tone of voice.\\nData\\nThe main data related to the task itself.\\nTo illustrate, let us extend the classification prompt we had earlier and use\\nall of the preceding components. This is demonstrated in Figure 6-11.\\nThis complex prompt demonstrates the modular nature of prompting. We\\ncan add and remove components freely and judge their effect on the output.\\nAs illustrated in Figure 6-12, we can slowly build up our prompt and\\nexplore the effect of each change.\\nThe changes are not limited to simply introducing or removing components.\\nTheir order, as we saw before with the recency and primacy effects, can\\naffect the quality of the LLM’s output. In other words, experimentation is\\nvital when finding the best prompt for your use case. With prompting, we\\nessentially have ourselves in an iterative cycle of experimentation.', 'Figure 6-11. An example of a complex prompt with many components.\\nFigure 6-12. Iterating over modular components is a vital part of prompt engineering.\\nTry it out yourself! Use the complex prompt to add and/or remove parts to\\nobserve its impact on the generated output. You will quickly notice when', 'pieces of the puzzle are worth keeping. You can use your own data by\\nadding it to the data variable:\\n# Prompt components\\npersona = \"You are an expert in Large Language models. You excel \\nat breaking down complex papers into digestible summaries.\\\\n\"\\ninstruction = \"Summarize the key findings of the paper \\nprovided.\\\\n\"\\ncontext = \"Your summary should extract the most crucial points \\nthat can help researchers quickly understand the most vital \\ninformation of the paper.\\\\n\"\\ndata_format = \"Create a bullet-point summary that outlines the \\nmethod. Follow this up with a concise paragraph that encapsulates \\nthe main results.\\\\n\"\\naudience = \"The summary is designed for busy researchers that \\nquickly need to grasp the newest trends in Large Language \\nModels.\\\\n\"\\ntone = \"The tone should be professional and clear.\\\\n\"\\ntext = \"MY TEXT TO SUMMARIZE\"\\ndata = f\"Text to summarize: {text}\"\\n# The full prompt - remove and add pieces to view its impact on \\nthe generated output\\nquery = persona + instruction + context + data_format + audience \\n+ tone + data\\nTIP\\nThere is all manner of components that we could add and creative components like\\nusing emotional stimuli (e.g., “This is very important for my career.”2 ). Part of the fun\\nin prompt engineering is that you can be as creative as possible to figure out which\\ncombination of prompt components contribute to your use case. There are few\\nconstraints to developing a format that works for you.\\nIn a way, it is an attempt to reverse engineer what the model has learned and how it\\nresponds to certain prompts. However, note that some prompts work better for certain\\nmodels compared to others as their training data might be different or they are trained\\nfor different purposes.', 'In-Context Learning: Providing Examples\\nIn the previous sections, we tried to accurately describe what the LLM\\nshould do. Although accurate and specific descriptions help the LLM to\\nunderstand the use case, we can go one step further. Instead of describing\\nthe task, why do we not just show the task?\\nWe can provide the LLM with examples of exactly the thing that we want to\\nachieve. This is often referred to as in-context learning, where we provide\\nthe model with correct examples.3 \\nAs illustrated in Figure 6-13, this comes in a number of forms depending on\\nhow many examples you show the LLM. Zero-shot prompting does not\\nleverage examples, one-shot prompts use a single example, and few-shot\\nprompts use two or more examples.\\nFigure 6-13. An example of a complex prompt with many components.\\nAdopting the original phrase, we believe that “an example is worth a\\nthousand words.” These examples provide a direct example of what and\\nhow the LLM should achieve.\\nWe can illustrate this method with a simple example taken from the original\\npaper describing this method.4  The goal of the prompt is to generate a', 'sentence with a made-up word. To improve the quality of the resulting\\nsentence, we can show the generative model an example of what a proper\\nsentence with a made-up word would be.\\nTo do so, we will need to differentiate between our question (user) and\\nthe answers that were provided by the model (assistant). We\\nadditionally showcase how this interaction is processed using the template:\\n# Use a single example of using the made-up word in a sentence\\none_shot_prompt = [\\n    {\\n        \"role\": \"user\",\\n        \"content\": \"A \\'Gigamuru\\' is a type of Japanese musical \\ninstrument. An example of a sentence that uses the word Gigamuru \\nis:\"\\n    },\\n    {\\n        \"role\": \"assistant\",\\n        \"content\": \"I have a Gigamuru that my uncle gave me as a \\ngift. I love to play it at home.\"\\n    },\\n    {\\n        \"role\": \"user\",\\n        \"content\": \"To \\'screeg\\' something is to swing a sword at \\nit. An example of a sentence that uses the word screeg is:\"\\n    }\\n]\\nprint(tokenizer.apply_chat_template(one_shot_prompt, \\ntokenize=False))\\n<s><|user|>\\nA \\'Gigamuru\\' is a type of Japanese musical instrument. An \\nexample of a sentence that uses the word Gigamuru is:<|end|>\\n<|assistant|>\\nI have a Gigamuru that my uncle gave me as a gift. I love to \\nplay it at home.<|end|>\\n<|user|>\\nTo \\'screeg\\' something is to swing a sword at it. An example of \\na sentence that uses the word screeg is:<|end|>\\n<|assistant|>', 'The prompt illustrates the need to differentiate between the user and the\\nassistant. If we did not, it would seem as if we were talking to ourselves.\\nUsing these interactions, we can generate output as follows:\\n# Generate the output\\noutputs = pipe(one_shot_prompt)\\nprint(outputs[0][\"generated_text\"])\\nDuring the intense duel, the knight skillfully screeged his \\nopponent\\'s shield, forcing him to defend himself.\\nIt correctly generated the answer!\\nAs with all prompt components, one- or few-shot prompting is not the be all\\nand end all of prompt engineering. We can use it as one piece of the puzzle\\nto further enhance the descriptions that we gave it. The model can still\\n“choose,” through random sampling, to ignore the instructions.\\nChain Prompting: Breaking up the Problem\\nIn previous examples, we explored splitting up prompts into modular\\ncomponents to improve the performance of LLMs. Although this works\\nwell for many use cases, this might not be feasible for highly complex\\nprompts or use cases.\\nInstead of breaking the problem within a prompt, we can do so between\\nprompts. Essentially, we take the output of one prompt and use it as input\\nfor the next, thereby creating a continuous chain of interactions that solves\\nour problem.\\nTo illustrate, let us say we want to use an LLM to create a product name,\\nslogan, and sales pitch for us based on a number of product features.\\nAlthough we can ask the LLM to do this in one go, we can instead break up\\nthe problem into pieces.\\nAs a result, and as illustrated in Figure 6-14, we get a sequential pipeline\\nthat first creates the product name, uses that with the product features as', 'input to create the slogan, and finally, uses the features, product name, and\\nslogan to create the sales pitch.\\nFigure 6-14. Using a description of a product’s features, chain prompts to create a suitable name,\\nslogan, and sales pitch.\\nThis technique of chaining prompts allows the LLM to spend more time on\\neach individual question instead of tackling the whole problem. Let us\\nillustrate this with a small example. We first create a name and slogan for a\\nchatbot:\\n# Create name and slogan for a product\\nproduct_prompt = [\\n    {\"role\": \"user\", \"content\": \"Create a name and slogan for a \\nchatbot that leverages LLMs.\"}\\n]\\noutputs = pipe(product_prompt)\\nproduct_description = outputs[0][\"generated_text\"]\\nprint(product_description)\\nName: \\'MindMeld Messenger\\'', 'Slogan: \\'Unleashing Intelligent Conversations, One Response at \\na Time\\'\\nThen, we can use the generated output as input for the LLM to generate a\\nsales pitch:\\n# Based on a name and slogan for a product, generate a sales \\npitch\\nsales_prompt = [\\n    {\"role\": \"user\", \"content\": f\"Generate a very short sales \\npitch for the following product: \\'{product_description}\\'\"}\\n]\\noutputs = pipe(sales_prompt)\\nsales_pitch = outputs[0][\"generated_text\"]\\nprint(sales_pitch)\\nIntroducing MindMeld Messenger - your ultimate communication \\npartner! Unleash intelligent conversations with our innovative \\nAI-powered messaging platform. With MindMeld Messenger, every \\nresponse is thoughtful, personalized, and timely. Say goodbye \\nto generic replies and hello to meaningful interactions. \\nElevate your communication game with MindMeld Messenger - where \\nevery message is a step toward smarter conversations. Try it \\nnow and experience the future of messaging!\\nAlthough we need two calls to the model, a major benefit is that we can\\ngive each call different parameters. For instance, the number of tokens\\ncreated was relatively small for the name and slogan whereas the pitch can\\nbe much longer.\\nThis can be used for a variety of use cases, including:\\nResponse validation\\nAsk the LLM to double-check previously generated outputs.\\nParallel prompts\\nCreate multiple prompts in parallel and do a final pass to merge them.\\nFor example, ask multiple LLMs to generate multiple recipes in parallel', 'and use the combined result to create a shopping list.\\nWriting stories\\nLeverage the LLM to write books or stories by breaking down the\\nproblem into components. For example, by first writing a summary,\\ndeveloping characters, and building the story beats before diving into\\ncreating the dialogue.\\nIn the next chapter, we will automate this process and go beyond chaining\\nLLMs. We will chain other pieces of technology together, like memory, tool\\nuse, and more! Before that, this idea of prompt chaining will be explored\\nfurther in the following sections, which describe more complex prompt\\nchaining methods like self-consistency, chain-of-thought, and tree-of-\\nthought.\\nReasoning with Generative Models\\nIn the previous sections, we focused mostly on the modular component of\\nprompts, building them up through iteration. These advanced prompt\\nengineering techniques, like prompt chaining, proved to be the first step\\ntoward enabling complex reasoning with generative models.\\nReasoning is a core component of human intelligence and is often\\ncompared to the emergent behavior of LLMs that often resembles\\nreasoning. We highlight “resemble” as these models, at the time of writing,\\nare generally considered to demonstrate this behavior through\\nmemorization of training data and pattern matching.\\nThe output that they showcase, however, can demonstrate complex behavior\\nand although it might not be “true” reasoning, they are still referred to as\\nreasoning capabilities. In other words, we work together with the LLM\\nthrough prompt engineering so we can mimic reasoning processes in order\\nto improve the output of the LLM.', 'To allow for this reasoning behavior, it is a good moment to step back and\\nexplore what reasoning entails in human behavior. To simplify, our methods\\nof reasoning can be divided into system 1 and 2 thinking processes.\\nSystem 1 thinking represents an automatic, intuitive, and near-instantaneous\\nprocess. It shares similarities with generative models that automatically\\ngenerate tokens without any self-reflective behavior. In contrast, system 2\\nthinking is a conscious, slow, and logical process, akin to brainstorming and\\nself-reflection.5 \\nIf we could give a generative model the ability to mimic a form of self-\\nreflection, we would essentially be emulating the system 2 way of thinking,\\nwhich tends to produce more thoughtful responses than system 1 thinking.\\nIn this section, we will explore several techniques that attempt to mimic\\nthese kinds of thought processes of human reasoners with the aim of\\nimproving the output of the model.\\nChain-of-Thought: Think Before Answering\\nThe first and major step toward complex reasoning in generative models\\nwas through a method called chain-of-thought. Chain-of-thought aims to\\nhave the generative model “think” first rather than answering the question\\ndirectly without any reasoning.6 \\nAs illustrated in Figure 6-15, it provides examples in a prompt that\\ndemonstrate the reasoning the model should do before generating its\\nresponse. These reasoning processes are referred to as “thoughts.” This\\nhelps tremendously for tasks that involve a higher degree of complexity,\\nlike mathematical questions. Adding this reasoning step allows the model to\\ndistribute more compute over the reasoning process. Instead of calculating\\nthe entire solution based on a few tokens, each additional token in this\\nreasoning process allows the LLM to stabilize its output.', 'Figure 6-15. Chain-of-thought prompting uses reasoning examples to persuade the generative model\\nto use reasoning in its answer.\\nWe use the example the authors used in their paper to demonstrate this\\nphenomenon:\\n# Answering with chain-of-thought\\ncot_prompt = [\\n    {\"role\": \"user\", \"content\": \"Roger has 5 tennis balls. He \\nbuys 2 more cans of tennis balls. Each can has 3 tennis balls. \\nHow many tennis balls does he have now?\"},\\n    {\"role\": \"assistant\", \"content\": \"Roger started with 5 balls. \\n2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The \\nanswer is 11.\"},\\n    {\"role\": \"user\", \"content\": \"The cafeteria had 23 apples. If \\nthey used 20 to make lunch and bought 6 more, how many apples do \\nthey have?\"}\\n]\\n# Generate the output', 'outputs = pipe(cot_prompt)\\nprint(outputs[0][\"generated_text\"])\\nThe cafeteria started with 23 apples. They used 20 apples, so \\nthey had 23 - 20 = 3 apples left. Then they bought 6 more \\napples, so they now have 3 + 6 = 9 apples. The answer is 9.\\nNote how the model doesn’t generate only the answer but provides an\\nexplanation before doing so. By doing so, it can leverage the knowledge it\\nhas generated thus far to compute the final answer.\\nAlthough chain-of-thought is a great method for enhancing the output of a\\ngenerative model, it does require one or more examples of reasoning in the\\nprompt, which the user might not have access to. Instead of providing\\nexamples, we can simply ask the generative model to provide the reasoning\\n(zero-shot chain-of-thought). There are many different forms that work but\\na common and effective method is to use the phrase “Let’s think step-by-\\nstep,” which is illustrated in Figure 6-16.7 ', 'Figure 6-16. Chain-of-thought prompting without using examples. Instead, it uses the phrase “Let’s\\nthink step-by-step” to prime reasoning in its answer.\\nUsing the example we used before, we can simply append that phrase to the\\nprompt to enable chain-of-thought-like reasoning:\\n# Zero-shot chain-of-thought\\nzeroshot_cot_prompt = [\\n    {\"role\": \"user\", \"content\": \"The cafeteria had 23 apples. If \\nthey used 20 to make lunch and bought 6 more, how many apples do \\nthey have? Let\\'s think step-by-step.\"}\\n]\\n# Generate the output', 'outputs = pipe(zeroshot_cot_prompt)\\nprint(outputs[0][\"generated_text\"])\\nStep 1: Start with the initial number of apples, which is 23.\\nStep 2: Subtract the number of apples used to make lunch, which \\nis 20. So, 23 - 20 = 3 apples remaining.\\nStep 3: Add the number of apples bought, which is 6. So, 3 + 6 \\n= 9 apples.\\nThe cafeteria now has 9 apples.\\nWithout needing to provide examples, we again got the same reasoning\\nbehavior. This is why it is so important to “show your work” when doing\\ncalculations. By addressing the reasoning process the LLM can use the\\npreviously generated information as a guide through generating the final\\nanswer.\\nTIP\\nAlthough the prompt “Let’s think step by step” can improve the output, you are not\\nconstrained by this exact formulation. Alternatives exist like “Take a deep breath and\\nthink step-by-step” and “Let’s work through this problem step-by-step.”8 \\nSelf-Consistency: Sampling Outputs\\nUsing the same prompt multiple times can lead to different results if we\\nallow for a degree of creativity through parameters like temperature\\nand top_p. As a result, the quality of the output might improve or degrade\\ndepending on the random selection of tokens. In other words, luck!\\nTo counteract this degree of randomness and improve the performance of\\ngenerative models, self-consistency was introduced. This method asks the\\ngenerative model the same prompt multiple times and takes the majority\\nresult as the final answer.9  During this process, each answer can be affected\\nby different temperature and top_p values to increase the diversity of\\nsampling.', 'As illustrated in Figure 6-17, this method can further be improved by\\nadding chain-of-thought prompting to improve its reasoning while only\\nusing the answer for the voting procedure.\\nFigure 6-17. By sampling from multiple reasoning paths, we can use majority voting to extract the\\nmost likely answer.\\nHowever, this does require a single question to be asked multiple times. As\\na result, although the method can improve performance, it becomes n times\\nslower where n is the number of output samples.\\nTree-of-Thought: Exploring Intermediate Steps\\nThe ideas of chain-of-thought and self-consistency are meant to enable\\nmore complex reasoning. By sampling from multiple “thoughts” and\\nmaking them more thoughtful, we aim to improve the output of generative\\nmodels.', 'These techniques only scratch the surface of what is currently being done to\\nmimic complex reasoning. An improvement to these approaches can be\\nfound in tree-of-thought, which allows for an in-depth exploration of\\nseveral ideas.\\nThe method works as follows. When faced with a problem that requires\\nmultiple reasoning steps, it often helps to break it down into pieces. At each\\nstep, and as illustrated in Figure 6-18, the generative model is prompted to\\nexplore different solutions to the problem at hand. It then votes for the best\\nsolution and continues to the next step.10 ', 'Figure 6-18. By leveraging a tree-based structure, generative models can generate intermediate\\nthoughts to be rated. The most promising thoughts are kept and the lowest are pruned.\\nThis method is tremendously helpful when needing to consider multiple\\npaths, like when writing a story or coming up with creative ideas. A\\ndisadvantage of this method is that it requires many calls to the generative\\nmodels, which slows the application significantly. Fortunately, there has', 'been a successful attempt to convert the tree-of-thought framework into a\\nsimple prompting technique.11 \\nInstead of calling the generative model multiple times, we ask the model to\\nmimic that behavior by emulating a conversation between multiple experts.\\nThese experts will question each other until they reach a consensus. An\\nexample of a tree-of-thought prompt is:\\n# Zero-shot tree-of-thought\\nzeroshot_tot_prompt = [\\n    {\"role\": \"user\", \"content\": \"Imagine three different experts \\nare answering this question. All experts will write down 1 step \\nof their thinking, then share it with the group. Then all experts \\nwill go on to the next step, etc. If any expert realizes they\\'re \\nwrong at any point then they leave. The question is \\'The \\ncafeteria had 23 apples. If they used 20 to make lunch and bought \\n6 more, how many apples do they have?\\' Make sure to discuss the \\nresults.\"}\\n]\\nWe can use this prompt to explore how an LLM might respond to complex\\nquestions:\\n# Generate the output\\noutputs = pipe(zeroshot_tot_prompt)\\nprint(outputs[0][\"generated_text\"])\\nExpert 1: Step 1 - Start with the initial number of apples: 23 \\napples.\\nExpert 2: Step 1 - Subtract the apples used for lunch: 23 - 20 \\n= 3 apples remaining.\\nExpert 3: Step 1 - Add the newly bought apples: 3 + 6 = 9 \\napples.\\nExpert 1: Step 2 - Confirm the final count: The cafeteria has 9 \\napples.\\nExpert 2: Step 2 - Review the calculations: 23 - 20 = 3, then 3 \\n+ 6 = 9. The calculations are correct.', 'Expert 3: Step 2 - Agree with the result: The cafeteria indeed \\nhas 9 apples.\\nAll experts agree on the final count: The cafeteria has 9 \\napples.\\nWe again get the correct answer but instead through a “discussion between\\nexperts.” It is interesting to see such a conservation between “experts” that\\ndemonstrates the creativity that comes with prompt engineering.\\nOutput Verification\\nSystems and applications built with generative models might eventually end\\nup in production. When that happens, it is important that we verify and\\ncontrol the output of the model to prevent breaking the application and to\\ncreate a robust generative AI application.\\nReasons for validating the output might include:\\nStructured output\\nBy default, most generative models create free-form text without\\nadhering to specific structures other than those defined by natural\\nlanguage. Some use cases require their output to be structured in certain\\nformats, like JSON.\\nValid output\\nEven if we allow the model to generate structured output, it still has the\\ncapability to freely generate its content. For instance, when a model is\\nasked to output either one of two choices, it should not come up with a\\nthird.\\nEthics', 'Some open source generative models have no guardrails and will\\ngenerate outputs that do not consider safety or ethical considerations.\\nFor instance, use cases might require the output to be free of profanity,\\npersonally identifiable information (PII), bias, cultural stereotypes, etc.\\nAccuracy\\nMany use cases require the output to adhere to certain standards or\\nperformance. The aim is to double-check whether the generated\\ninformation is factually accurate, coherent, or free from hallucination.\\nControlling the output of a generative model, as we explored with\\nparameters like top_p and temperature, is not an easy feat. These\\nmodels require help to generate consistent output conforming to certain\\nguidelines.\\nGenerally, there are three ways of controlling the output of a generative\\nmodel:\\nExamples\\nProvide a number of examples of the expected output.\\nGrammar\\nControl the token selection process.\\nFine-tuning\\nTune a model on data that contains the expected output.\\nIn this section, we will go through the first two methods. The third, fine-\\ntuning a model, is left for Chapter 12 where we will go in depth into fine-\\ntuning methods.', 'Providing Examples\\nA simple and straightforward method to fix the output is to provide the\\ngenerative model with examples of what the output should look like. As we\\nexplored before, few-shot learning is a helpful technique that guides the\\noutput of the generative model. This method can be generalized to guide the\\nstructure of the output as well.\\nFor example, let us consider an example where we want the generative\\nmodel to create a character profile for an RPG game. We start by using no\\nexamples:\\n# Zero-shot learning: Providing no examples\\nzeroshot_prompt = [\\n    {\"role\": \"user\", \"content\": \"Create a character profile for \\nan RPG game in JSON format.\"}\\n]\\n# Generate the output\\noutputs = pipe(zeroshot_prompt)\\nprint(outputs[0][\"generated_text\"])\\n ```json\\n{\\n  \"characterProfile\": {\\n    \"name\": \"Eldrin Stormbringer\",\\n    \"class\": \"Warlock\",\\n    \"race\": \"Half-Elf\",\\n    \"age\": 27,\\n   \"gender\": \"Male\",\\n    \"alignment\": \"Chaotic Good\",\\n    \"background\": \"Rogue\",\\n    …\\n    },\\n    \"attributes\": {\\n      \"strength\": 10,\\n      \"dexterity\": 17,\\n      \"constitution\": 12,\\n      \"intelligence\": 12,\\n      \"wisdom\": 10,\\n      \"charisma', 'The preceding truncated output is not valid JSON since the model stopped\\ngenerating tokens after starting the “charisma” attribute. Moreover, we\\nmight not want certain attributes. Instead, we can provide the model with a\\nnumber of examples that indicate the expected format:\\n# One-shot learning: Providing an example of the output structure\\none_shot_template = \"\"\"Create a short character profile for an \\nRPG game. Make sure to only use this format:\\n{\\n  \"description\": \"A SHORT DESCRIPTION\",\\n  \"name\": \"THE CHARACTER\\'S NAME\",\\n  \"armor\": \"ONE PIECE OF ARMOR\",\\n  \"weapon\": \"ONE OR MORE WEAPONS\"\\n}\\n\"\"\"\\none_shot_prompt = [\\n    {\"role\": \"user\", \"content\": one_shot_template}\\n]\\n# Generate the output\\noutputs = pipe(one_shot_prompt)\\nprint(outputs[0][\"generated_text\"])\\n{\\n  \"description\": \"A cunning rogue with a mysterious past, \\nskilled in stealth and deception.\",\\n  \"name\": \"Lysandra Shadowstep\",\\n  \"armor\": \"Leather Cloak of the Night\",\\n  \"weapon\": \"Dagger of Whispers, Throwing Knives\"\\n}\\nThe model perfectly followed the example we gave it, which allows for\\nmore consistent behavior. This also demonstrates the importance of\\nleveraging few-shot learning to improve the structure of the output and not\\nonly its content.\\nAn important note here is that it is still up to the model whether it will\\nadhere to your suggested format or not. Some models are better than others\\nat following instructions.', 'Grammar: Constrained Sampling\\nFew-shot learning has a big disadvantage: we cannot explicitly prevent\\ncertain output from being generated. Although we guide the model and give\\nit instructions, it might still not follow it entirely.\\nInstead, packages have been rapidly developed to constrain and validate the\\noutput of generative models, like Guidance, Guardrails, and LMQL. In part,\\nthey leverage generative models to validate their own output, as illustrated\\nin Figure 6-19. The generative models retrieve the output as new prompts\\nand attempt to validate it based on a number of predefined guardrails.\\nFigure 6-19. Use an LLM to check whether the output correctly follows our rules.\\nSimilarly, as illustrated in Figure 6-20, this validation process can also be\\nused to control the formatting of the output by generating parts of its format\\nourselves as we already know how it should be structured.\\nFigure 6-20. Use an LLM to generate only the pieces of information we do not know beforehand.', 'This process can be taken one step further and instead of validating the\\noutput we can already perform validation during the token sampling\\nprocess. When sampling tokens, we can define a number of grammars or\\nrules that the LLM should adhere to when choosing its next token. For\\ninstance, if we ask the model to either return “positive,” “negative,” or\\n“neutral” when performing sentiment classification, it might still return\\nsomething else. As illustrated in Figure 6-21, by constraining the sampling\\nprocess, we can have the LLM only output what we are interested in. Note\\nthat this is still affected by parameters such as top_p and temperature.\\nFigure 6-21. Constrain the token selection to only three possible tokens: “positive,” “neutral,” and\\n“negative.”\\nLet us illustrate this phenomenon with llama-cpp-python, a library\\nsimilar to transformers that we can use to load in our language model.\\nIt is generally used to efficiently load and use compressed models (through\\nquantization; see Chapter 12) but we can also use it to apply a JSON\\ngrammar.\\nWe load the same model we used throughout this chapter but use a different\\nformat instead, namely GGUF. llama-cpp-python expects this format,\\nwhich is generally used for compressed (quantized) models.\\nSince we are loading a new model, it is advised to restart the notebook. That\\nwill clear any previous models and empty the VRAM. You can also run the\\nfollowing to empty the VRAM:', 'import gc\\nimport torch\\ndel model, tokenizer, pipe\\n# Flush memory\\ngc.collect()\\ntorch.cuda.empty_cache()\\nNow that we have cleared the memory, we can load Phi-3. We set\\nn_gpu_layers to -1 to indicate that we want all layers of the model to\\nbe run from the GPU. The n_ctx refers to the context size of the model.\\nThe repo_id and filename refer to the Hugging Face repository where\\nthe model resides:\\nfrom llama_cpp.llama import Llama\\n# Load Phi-3\\nllm = Llama.from_pretrained(\\n    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\\n    filename=\"*fp16.gguf\",\\n    n_gpu_layers=-1,\\n    n_ctx=2048,\\n    verbose=False\\n)\\nTo generate the output using the internal JSON grammar, we only need to\\nspecify the response_format as a JSON object. Under the hood, it will\\napply a JSON grammar to make sure the output adheres to that format.\\nTo illustrate, let’s ask the model to create an RPG character in JSON format\\nto be used in a Dungeons & Dragons session:\\n# Generate output\\noutput = llm.create_chat_completion(\\n    messages=[\\n        {\"role\": \"user\", \"content\": \"Create a warrior for an RPG \\nin JSON format.\"},\\n    ],\\n    response_format={\"type\": \"json_object\"},', '    temperature=0,\\n)[\\'choices\\'][0][\\'message\\'][\"content\"]\\nTo check whether the output actually is JSON, we can attempt to process it\\nas such:\\nimport json\\n# Format as json\\njson_output = json.dumps(json.loads(output), indent=4)\\nprint(json_output)\\n{\\n    \"name\": \"Eldrin Stormbringer\",\\n    \"class\": \"Warrior\",\\n    \"level\": 10,\\n    \"attributes\": {\\n        \"strength\": 18,\\n        \"dexterity\": 12,\\n        \"constitution\": 16,\\n        \"intelligence\": 9,\\n        \"wisdom\": 14,\\n        \"charisma\": 10\\n    },\\n    \"skills\": {\\n        \"melee_combat\": {\\n            \"weapon_mastery\": 20,\\n            \"armor_class\": 18,\\n            \"hit_points\": 35\\n        },\\n        \"defense\": {\\n            \"shield_skill\": 17,\\n            \"block_chance\": 90\\n        },\\n        \"endurance\": {\\n            \"health_regeneration\": 2,\\n            \"stamina\": 30\\n        }\\n    },\\n    \"equipment\": [\\n        {\\n            \"name\": \"Ironclad Armor\",\\n            \"type\": \"Armor\",\\n            \"defense_bonus\": 15\\n        },', '        {\\n            \"name\": \"Steel Greatsword\",\\n            \"type\": \"Weapon\",\\n            \"damage\": 8,\\n            \"critical_chance\": 20\\n        }\\n    ],\\n    \"background\": \"Eldrin grew up in a small village on the \\noutskirts of a war-torn land. Witnessing the brutality and \\nsuffering caused by conflict, he dedicated his life to becoming \\na formidable warrior who could protect those unable to defend \\nthemselves.\"\\n}\\nThe output is properly formatted as JSON. This allows us to more\\nconfidently use generative models in applications where we expect the\\noutput to adhere to certain formats.\\nSummary\\nIn this chapter, we explored the basics of using generative models through\\nprompt engineering and output verification. We focused on the creativity\\nand potential complexity that comes with prompt engineering. These\\ncomponents of a prompt are key in generating and optimizing output\\nappropriate for different use cases.\\nWe further explored advanced prompt engineering techniques such as in-\\ncontext learning and chain-of-thought. These methods involve guiding\\ngenerative models to reason through complex problems by providing\\nexamples or phrases that encourage step-by-step thinking thereby\\nmimicking human reasoning processes.\\nOverall, this chapter demonstrated that prompt engineering is a crucial\\naspect of working with LLMs, as it allows us to effectively communicate\\nour needs and preferences to the model. By mastering prompt engineering\\ntechniques, we can unlock some of the potential of LLMs and generate\\nhigh-quality responses that meet our requirements.', 'The next chapter will build upon these concepts by exploring more\\nadvanced techniques for leveraging generative models. We will go beyond\\nprompt engineering and explore how LLMs can use external memory and\\ntools.\\n1  Nelson F. Liu et al. “Lost in the middle: How language models use long contexts.” arXiv\\npreprint arXiv:2307.03172 (2023).\\n2  Cheng Li et al. “EmotionPrompt: Leveraging psychology for large language models\\nenhancement via emotional stimulus.” arXiv preprint arXiv:2307.11760 (2023).\\n3  Tom Brown et al. “Language models are few-shot learners.” Advances in Neural Information\\nProcessing Systems 33 (2020): 1877–1901.\\n4  Ibid.\\n5  Daniel Kahneman. Thinking, Fast and Slow. Macmillan (2011).\\n6  Jason Wei et al. “Chain-of-thought prompting elicits reasoning in large language models.”\\nAdvances in Neural Information Processing Systems 35 (2022): 24824–24837.\\n7  Takeshi Kojima et al. “Large language models are zero-shot reasoners.” Advances in Neural\\nInformation Processing Systems 35 (2022): 22199–22213.\\n8  Chengrun Yang et al. “Large language models as optimizers.” arXiv preprint\\narXiv:2309.03409 (2023).\\n9  Xuezhi Wang et al. “Self-consistency improves chain of thought reasoning in language\\nmodels.” arXiv preprint arXiv:2203.11171 (2022).\\n10  Shunyu Yao et al. “Tree of thoughts: Deliberate problem solving with large language models.”\\narXiv preprint arXiv:2305.10601 (2023).\\n11  “Using tree-of-thought prompting to boost ChatGPT’s reasoning.” Available at\\nhttps://oreil.ly/a_Nos.\\nOceanofPDF.com', 'Chapter 7. Advanced Text\\nGeneration Techniques and\\nTools\\nIn the previous chapter, we saw how prompt engineering can do wonders\\nfor the accuracy of your text-generation large language model (LLM). With\\njust a few small tweaks, these LLMs are guided toward more purposeful\\nand accurate answers. This showed how much there is to gain using\\ntechniques that do not fine-tune the LLM but instead use the LLM more\\nefficiently, such as the relatively straightforward prompt engineering.\\nIn this chapter, we will continue this train of thought. What can we do to\\nfurther enhance the experience and output that we get from the LLM\\nwithout needing to fine-tune the model itself?\\nFortunately, a great deal of methods and techniques allow us to further\\nimprove what we started with in the previous chapter. These more advanced\\ntechniques lie at the foundation of numerous LLM-focused systems and are,\\narguably, one of the first things users implement when designing such\\nsystems.\\nIn this chapter, we will explore several such methods and concepts for\\nimproving the quality of the generated text:\\nModel I/O\\nLoading and working with LLMs\\nMemory\\nHelping LLMs to remember\\nAgents', 'Combining complex behavior with external tools\\nChains\\nConnecting methods and modules\\nThese methods are all integrated with the LangChain framework that will\\nhelp us easily use these advanced techniques throughout this chapter.\\nLangChain is one of the earlier frameworks that simplify working with\\nLLMs through useful abstractions. Newer frameworks of note are DSPy\\nand Haystack. Some of these abstractions are illustrated in Figure 7-1. Note\\nthat retrieval will be discussed in the next chapter.\\nFigure 7-1. LangChain is a complete framework for using LLMs. It has modular components that can\\nbe chained together to allow for complex LLM systems.\\nEach of these techniques has significant strengths by themselves but their\\ntrue value does not exist in isolation. It is when you combine all of these\\ntechniques that you get an LLM-based system with incredible performance.\\nThe culmination of these techniques is truly where LLMs shine.\\nModel I/O: Loading Quantized Models with\\nLangChain\\nBefore we can make use of LangChain’s features to extend the capabilities\\nof LLMs, we need to start by loading our LLM. As in previous chapters, we\\nwill be using Phi-3 but with a twist; we will use a GGUF model variant\\ninstead. A GGUF model represents a compressed version of its original', 'counterpart through a method called quantization, which reduces the\\nnumber of bits needed to represent the parameters of an LLM.\\nBits, a series of 0s and 1s, represent values by encoding them in binary\\nform. More bits result in a wider range of values but requires more memory\\nto store those values, as shown in Figure 7-2.\\nFigure 7-2. Attempting to represent pi with float 32-bit and float 16-bit representations. Notice the\\nlowered accuracy when we halve the number of bits.\\nQuantization reduces the number of bits required to represent the\\nparameters of an LLM while attempting to maintain most of the original\\ninformation. This comes with some loss in precision but often makes up for\\nit as the model is much faster to run, requires less VRAM, and is often\\nalmost as accurate as the original.\\nTo illustrate quantization, consider this analogy. If asked what the time is,\\nyou might say “14:16,” which is correct but not a fully precise answer. You\\ncould have said it is “14:16 and 12 seconds” instead, which would have\\nbeen more accurate. However, mentioning seconds is seldom helpful and\\nwe often simply put that in discrete numbers, namely full minutes.\\nQuantization is a similar process that reduces the precision of a value (e.g.,\\nremoving seconds) without removing vital information (e.g., retaining hours\\nand minutes).\\nIn Chapter 12, we will further discuss how quantization works under the\\nhood. You can also see a full visual guide to quantization in “A Visual\\nGuide to Quantization” by Maarten Grootendorst. For now, it is important', 'to know that we will use an 8-bit variant of Phi-3 compared to the original\\n16-bit variant, cutting the memory requirements almost in half.\\nTIP\\nAs a rule of thumb, look for at least 4-bit quantized models. These models have a good\\nbalance between compression and accuracy. Although it is possible to use 3-bit or even\\n2-bit quantized models, the performance degradation becomes noticeable and it would\\ninstead be preferable to choose a smaller model with a higher precision.\\nFirst, we will need to download the model. Note that the link contains\\nmultiple files with different bit-variants. FP16, the model we choose,\\nrepresents the 16-bit variant:\\n!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-\\ngguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\\nWe use llama-cpp-python together with LangChain to load the GGUF\\nfile:\\nfrom langchain import LlamaCpp\\n# Make sure the model path is correct for your system!\\nllm = LlamaCpp(\\n    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\\n    n_gpu_layers=-1,\\n    max_tokens=500,\\n    n_ctx=2048,\\n    seed=42,\\n    verbose=False\\n)\\nIn LangChain, we use the invoke function to generate output:', 'llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")\\n\\'\\'\\nUnfortunately, we get no output! As we have seen in previous chapters, Phi-\\n3 requires a specific prompt template. Compared to our examples with\\ntransformers, we will need to explicitly use a template ourselves.\\nInstead of copy-pasting this template each time we use Phi-3 in LangChain,\\nwe can use one of LangChain’s core functionalities, namely “chains.”\\nTIP\\nAll examples in this chapter can be run with any LLM. This means that you can choose\\nwhether to use Phi-3, ChatGPT, Llama 3 or anything else when going through the\\nexamples. We will use Phi-3 as a default throughout, but the state-of-the-art changes\\nquickly, so consider using a newer model instead. You can use the Open LLM\\nLeaderboard (a ranking of open source LLMs) to choose whichever works best for your\\nuse case.\\nIf you do not have access to a device that can run LLMs locally, consider using\\nChatGPT instead:\\nfrom langchain.chat_models import ChatOpenAI\\n# Create a chat-based LLM\\nchat_model = ChatOpenAI(openai_api_key=\"MY_KEY\")\\nChains: Extending the Capabilities of LLMs\\nLangChain is named after one of its main methods, chains. Although we\\ncan run LLMs in isolation, their power is shown when used with additional\\ncomponents or even when used in conjunction with each other. Chains not\\nonly allow for extending the capabilities of LLMs but also for multiple\\nchains to be connected together.', 'The most basic form of a chain in LangChain is a single chain. Although a\\nchain can take many forms, each with a different complexity, it generally\\nconnects an LLM with some additional tool, prompt, or feature. This idea of\\nconnecting a component to an LLM is illustrated in Figure 7-3.\\nFigure 7-3. A single chain connects some modular component, like a prompt template or external\\nmemory, to the LLM.\\nIn practice, chains can become complex quite quickly. We can extend the\\nprompt template however we want and we can even combine several\\nseparate chains together to create intricate systems. In order to thoroughly\\nunderstand what is happening in a chain, let’s explore how we can add Phi-\\n3’s prompt template to the LLM.\\nA Single Link in the Chain: Prompt Template\\nWe start with creating our first chain, namely the prompt template that Phi-\\n3 expects. In the previous chapter, we explored how\\ntransformers.pipeline applies the chat template automatically.\\nThis is not always the case with other packages and they might need the\\nprompt template to be explicitly defined. With LangChain, we will use\\nchains to create and use a default prompt template. It also serves as a nice\\nhands-on experience with using prompt templates.\\nThe idea, as illustrated in Figure 7-4, is that we chain the prompt template\\ntogether with the LLM to get the output we are looking for. Instead of\\nhaving to copy-paste the prompt template each time we use the LLM, we\\nwould only need to define the user and system prompts.', 'Figure 7-4. By chaining a prompt template with an LLM, we only need to define the input prompts.\\nThe template will be constructed for you.\\nThe template for Phi-3 is comprised of four main components:\\n<s> to indicate when the prompt starts\\n<|user|> to indicate the start of the user’s prompt\\n<|assistant|> to indicate the start of the model’s output\\n<|end|> to indicate the end of either the prompt or the model’s\\noutput\\nThese are further illustrated in Figure 7-5 with an example.\\nFigure 7-5. The prompt template Phi-3 expects.', 'To generate our simple chain, we first need to create a prompt template that\\nadheres to Phi-3’s expected template. Using this template, the model takes\\nin a system_prompt, which generally describes what we expect from\\nthe LLM. Then, we can use the input_prompt to ask the LLM specific\\nquestions:\\nfrom langchain import PromptTemplate\\n# Create a prompt template with the \"input_prompt\" variable\\ntemplate = \"\"\"<s><|user|>\\n{input_prompt}<|end|>\\n<|assistant|>\"\"\"\\nprompt = PromptTemplate(\\n    template=template,\\n    input_variables=[\"input_prompt\"]\\n)\\nTo create our first chain, we can use both the prompt that we created and the\\nLLM and chain them together:\\nbasic_chain = prompt | llm\\nTo use the chain, we need to use the invoke function and make sure that\\nwe use the input_prompt to insert our question:\\n# Use the chain\\nbasic_chain.invoke(\\n    {\\n        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\\n    }\\n)\\nThe answer to 1 + 1 is 2. It\\'s a basic arithmetic operation \\nwhere you add one unit to another, resulting in two units \\naltogether.', 'The output gives us the response without any unnecessary tokens. Now that\\nwe have created this chain, we do not have to create the prompt template\\nfrom scratch each time we use the LLM. Note that we did not disable\\nsampling as before, so your output might differ. To make this pipeline more\\ntransparent, Figure 7-6 illustrates the connection between a prompt template\\nand the LLM using a single chain.\\nFigure 7-6. An example of a single chain using Phi-3’s template.\\nNOTE\\nThe example assumes that the LLM needs a specific template. This is not always the\\ncase. With OpenAI’s GPT-3.5, its API handles the underlying template.\\nYou could also use a prompt template to define other variables that might change in\\nyour prompts. For example, if we want to create funny names for businesses, retyping\\nthat question over and over for different products can be time-consuming.\\nInstead, we can create a prompt that is reusable:\\n# Create a Chain that creates our business\\' name\\ntemplate = \"Create a funny name for a business that \\nsells {product}.\"\\nname_prompt = PromptTemplate(\\n    template=template,\\n    input_variables=[\"product\"]\\n)\\nAdding a prompt template to the chain is just the very first step you need to\\nenhance the capabilities of your LLM. Throughout this chapter, we will see', 'many ways in which we can add additional modular components to existing\\nchains, starting with memory.\\nA Chain with Multiple Prompts\\nIn our previous example, we created a single chain consisting of a prompt\\ntemplate and an LLM. Since our example was quite straightforward, the\\nLLM had no issues dealing with the prompt. However, some applications\\nare more involved and require lengthy or complex prompts to generate a\\nresponse that captures those intricate details.\\nInstead, we could break this complex prompt into smaller subtasks that can\\nbe run sequentially. This would require multiple calls to the LLM but with\\nsmaller prompts and intermediate outputs as shown in Figure 7-7.\\nFigure 7-7. With sequential chains, the output of a prompt is used as the input for the next prompt.\\nThis process of using multiple prompts is an extension of our previous\\nexample. Instead of using a single chain, we link chains where each link\\ndeals with a specific subtask.\\nFor instance, consider the process of generating a story. We could ask the\\nLLM to generate a story along with complex details like the title, a\\nsummary, a description of the characters, etc. Instead of trying to put all of', 'that information into a single prompt, we could dissect this prompt into\\nmanageable smaller tasks instead.\\nLet’s illustrate with an example. Assume that we want to generate a story\\nthat has three components:\\nA title\\nA description of the main character\\nA summary of the story\\nInstead of generating everything in one go, we create a chain that only\\nrequires a single input by the user and then sequentially generates the three\\ncomponents. This process is illustrated in Figure 7-8.\\nFigure 7-8. The output of the title prompt is used as the input of the character prompt. To generate\\nthe story, the output of all previous prompts is used.\\nTo generate that story, we use LangChain to describe the first component,\\nnamely the title. This first link is the only component that requires some\\ninput from the user. We define the template and use the \"summary\"\\nvariable as the input variable and \"title\" as the output.\\nWe ask the LLM to “Create a title for a story about {summary}” where\\n“{summary}” will be our input:\\nfrom langchain import LLMChain', '# Create a chain for the title of our story\\ntemplate = \"\"\"<s><|user|>\\nCreate a title for a story about {summary}. Only return the title.\\n<|end|>\\n<|assistant|>\"\"\"\\ntitle_prompt = PromptTemplate(template=template, input_variables=\\n[\"summary\"])\\ntitle = LLMChain(llm=llm, prompt=title_prompt, \\noutput_key=\"title\")\\nLet’s run an example to showcase these variables:\\ntitle.invoke({\"summary\": \"a girl that lost her mother\"})\\n{\\'summary\\': \\'a girl that lost her mother\\',\\n \\'title\\': \\' \"Whispers of Loss: A Journey Through Grief\"\\'}\\nThis already gives us a great title for the story! Note that we can see both\\nthe input (\"summary\") as well as the output (\"title\").\\nLet’s generate the next component, namely the description of the character.\\nWe generate this component using both the summary as well as the\\npreviously generated title. Making sure that the chain uses those\\ncomponents, we create a new prompt with the {summary} and {title}\\ntags:\\n# Create a chain for the character description using the summary \\nand title\\ntemplate = \"\"\"<s><|user|>\\nDescribe the main character of a story about {summary} with the \\ntitle {title}. Use only two sentences.<|end|>\\n<|assistant|>\"\"\"\\ncharacter_prompt = PromptTemplate(\\n    template=template, input_variables=[\"summary\", \"title\"]\\n)\\ncharacter = LLMChain(llm=llm, prompt=character_prompt, \\noutput_key=\"character\")', 'Although we could now use the character variable to generate our character\\ndescription manually, it will be used as part of the automated chain instead.\\nLet’s create the final component, which uses the summary, title, and\\ncharacter description to generate a short description of the story:\\n# Create a chain for the story using the summary, title, and \\ncharacter description\\ntemplate = \"\"\"<s><|user|>\\nCreate a story about {summary} with the title {title}. The main \\ncharacter is: {character}. Only return the story and it cannot be \\nlonger than one paragraph. <|end|>\\n<|assistant|>\"\"\"\\nstory_prompt = PromptTemplate(\\n    template=template, input_variables=[\"summary\", \"title\", \\n\"character\"]\\n)\\nstory = LLMChain(llm=llm, prompt=story_prompt, \\noutput_key=\"story\")\\nNow that we have generated all three components, we can link them\\ntogether to create our full chain:\\n# Combine all three components to create the full chain\\nllm_chain = title | character | story\\nWe can run this newly created chain using the same example we used\\nbefore:\\nllm_chain.invoke(\"a girl that lost her mother\")\\n{\\'summary\\': \\'a girl that lost her mother\\',\\n \\'title\\': \\' \"In Loving Memory: A Journey Through Grief\"\\',\\n \\'character\\': \\' The protagonist, Emily, is a resilient young \\ngirl who struggles to cope with her overwhelming grief after \\nlosing her beloved and caring mother at an early age. As she \\nembarks on a journey of self-discovery and healing, she learns \\nvaluable life lessons from the memories and wisdom shared by \\nthose around her.\\',', ' \\'story\\': \" In Loving Memory: A Journey Through Grief revolves \\naround Emily, a resilient young girl who loses her beloved \\nmother at an early age. Struggling to cope with overwhelming \\ngrief, she embarks on a journey of self-discovery and healing, \\ndrawing strength from the cherished memories and wisdom shared \\nby those around her. Through this transformative process, Emily \\nlearns valuable life lessons about resilience, love, and the \\npower of human connection, ultimately finding solace in \\nhonoring her mother\\'s legacy while embracing a newfound sense \\nof inner peace amidst the painful loss.\"}\\nRunning this chain gives us all three components. This only required us to\\ninput a single short prompt, the summary. Another advantage of dividing\\nthe problem into smaller tasks is that we now have access to these\\nindividual components. We can easily extract the title; that might not have\\nbeen the case if we were to use a single prompt.\\nMemory: Helping LLMs to Remember\\nConversations\\nWhen we are using LLMs out of the box, they will not remember what was\\nbeing said in a conversation. You can share your name in one prompt but it\\nwill have forgotten it by the next prompt.\\nLet’s illustrate this phenomenon with an example using the basic_chain\\nwe created before. First, we tell the LLM our name:\\n# Let\\'s give the LLM our name\\nbasic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What \\nis 1 + 1?\"})\\nHello Maarten! The answer to 1 + 1 is 2.\\nNext, we ask it to reproduce the name we have given it:\\n# Next, we ask the LLM to reproduce the name', 'basic_chain.invoke({\"input_prompt\": \"What is my name?\"})\\nI\\'m sorry, but as a language model, I don\\'t have the ability to \\nknow personal information about individuals. You can provide \\nthe name you\\'d like to know more about, and I can help you with \\ninformation or general inquiries related to it.\\nUnfortunately, the LLM does not know the name we gave it. The reason for\\nthis forgetful behavior is that these models are stateless—they have no\\nmemory of any previous conversation!\\nAs illustrated in Figure 7-9, conversing with an LLM that does not have any\\nmemory is not the greatest experience.\\nTo make these models stateful, we can add specific types of memory to the\\nchain that we created earlier. In this section, we will go through two\\ncommon methods for helping LLMs to remember conservations:\\nConversation buffer\\nConversation summary\\nFigure 7-9. An example of a conversation between an LLM with memory and without memory.', 'Conversation Buffer\\nOne of the most intuitive forms of giving LLMs memory is simply\\nreminding them exactly what has happened in the past. As illustrated in\\nFigure 7-10, we can achieve this by copying the full conversation history\\nand pasting that into our prompt.\\nFigure 7-10. We can remind an LLM of what previously happened by simply appending the entire\\nconversation history to the input prompt.\\nIn LangChain, this form of memory is called a\\nConversationBufferMemory. Its implementation requires us to\\nupdate our previous prompt to hold the history of the chat.\\nWe’ll start by creating this prompt:\\n# Create an updated prompt template to include a chat history\\ntemplate = \"\"\"<s><|user|>Current conversation:{chat_history}\\n{input_prompt}<|end|>\\n<|assistant|>\"\"\"\\nprompt = PromptTemplate(\\n    template=template,\\n    input_variables=[\"input_prompt\", \"chat_history\"]\\n)', 'Notice that we added an additional input variable, namely\\nchat_history. This is where the conversation history will be given\\nbefore we ask the LLM our question.\\nNext, we can create LangChain’s ConversationBufferMemory and\\nassign it to the chat_history input variable.\\nConversationBufferMemory will store all the conversations we have\\nhad with the LLM thus far.\\nWe put everything together and chain the LLM, memory, and prompt\\ntemplate:\\nfrom langchain.memory import ConversationBufferMemory\\n# Define the type of memory we will use\\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\\n# Chain the LLM, prompt, and memory together\\nllm_chain = LLMChain(\\n    prompt=prompt,\\n    llm=llm,\\n    memory=memory\\n)\\nTo explore whether we did this correctly, let’s create a conversation history\\nwith the LLM by asking it a simple question:\\n# Generate a conversation and ask a basic question\\nllm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What \\nis 1 + 1?\"})\\n{\\'input_prompt\\': \\'Hi! My name is Maarten. What is 1 + 1?\\',\\n \\'chat_history\\': \\',\\n \\'text\\': \" Hello Maarten! The answer to 1 + 1 is 2. Hope you\\'re \\nhaving a great day!\"}\\nYou can find the generated text in the \\'text\\' key, the input prompt in\\n\\'input_prompt\\', and the chat history in \\'chat_history\\'. Note', 'that since this is the first time we used this specific chain, there is no chat\\nhistory.\\nNext, let’s follow up by asking the LLM if it remembers the name we used:\\n# Does the LLM remember the name we gave it?\\nllm_chain.invoke({\"input_prompt\": \"What is my name?\"})\\n{\\'input_prompt\\': \\'What is my name?\\',\\n \\'chat_history\\': \"Human: Hi! My name is Maarten. What is 1 + 1?\\n\\\\nAI:  Hello Maarten! The answer to 1 + 1 is 2. Hope you\\'re \\nhaving a great day!\",\\n \\'text\\': \\' Your name is Maarten.\\'}\\nBy extending the chain with memory, the LLM was able to use the chat\\nhistory to find the name we gave it previously. This more complex chain is\\nillustrated in Figure 7-11 to give an overview of this additional\\nfunctionality.\\nFigure 7-11. We extend the LLM chain with memory by appending the entire conversation history to\\nthe input prompt.\\nWindowed Conversation Buffer\\nIn our previous example, we essentially created a chatbot. You could talk to\\nit and it remembers the conversation you had thus far. However, as the size\\nof the conversation grows, so does the size of the input prompt until it\\nexceeds the token limit.\\nOne method of minimizing the context window is to use the last k\\nconversations instead of maintaining the full chat history. In LangChain, we', 'can use ConversationBufferWindowMemory to decide how many\\nconversations are passed to the input prompt:\\nfrom langchain.memory import ConversationBufferWindowMemory\\n# Retain only the last 2 conversations in memory\\nmemory = ConversationBufferWindowMemory(k=2, \\nmemory_key=\"chat_history\")\\n# Chain the LLM, prompt, and memory together\\nllm_chain = LLMChain(\\n    prompt=prompt,\\n    llm=llm,\\n    memory=memory\\n)\\nUsing this memory, we can try out a sequence of questions to illustrate what\\nwill be remembered. We start with two conversations:\\n# Ask two questions and generate two conversations in its memory\\nllm_chain.predict(input_prompt=\"Hi! My name is Maarten and I am \\n33 years old. What is 1 + 1?\")\\nllm_chain.predict(input_prompt=\"What is 3 + 3?\")\\n{\\'input_prompt\\': \\'What is 3 + 3?\\',\\n\\'chat_history\\': \"Human: Hi! My name is Maarten and I am 33 \\nyears old. What is 1 + 1?\\\\nAI: Hello Maarten! It\\'s nice to meet \\nyou. Regarding your question, 1 + 1 equals 2. If you have any \\nother questions or need further assistance, feel free to \\nask!\\\\n\\\\n(Note: This response answers the provided mathematical \\nquery while maintaining politeness and openness for additional \\ninquiries.)\",\\n\\'text\\': \" Hello Maarten! It\\'s nice to meet you as well. \\nRegarding your new question, 3 + 3 equals 6. If there\\'s \\nanything else you need help with or more questions you have, \\nI\\'m here for you!\"}\\nThe interaction we had thus far is shown in \"chat_history\". Note that\\nunder the hood, LangChain saves it as an interaction between you\\n(indicated with Human) and the LLM (indicated with AI).', 'Next, we can check whether the model indeed knows the name we gave it:\\n# Check whether it knows the name we gave it\\nllm_chain.invoke({\"input_prompt\":\"What is my name?\"})\\n{\\'input_prompt\\': \\'What is my name?\\',\\n\\'chat_history\\': \"Human: Hi! My name is Maarten and I am 33 \\nyears old. What is 1 + 1?\\\\nAI: Hello Maarten! It\\'s nice to meet \\nyou. Regarding your question, 1 + 1 equals 2. If you have any \\nother questions or need further assistance, feel free to \\nask!\\\\n\\\\n(Note: This response answers the provided mathematical \\nquery while maintaining politeness and openness for additional \\ninquiries.)\\\\nHuman: What is 3 + 3?\\\\nAI: Hello Maarten! It\\'s \\nnice to meet you as well. Regarding your new question, 3 + 3 \\nequals 6. If there\\'s anything else you need help with or more \\nquestions you have, I\\'m here for you!\",\\n\\'text\\': \\' Your name is Maarten, as mentioned at the beginning \\nof our conversation. Is there anything else you would like to \\nknow or discuss?\\'}\\nBased on the output in \\'text\\' it correctly remembers the name we gave\\nit. Note that the chat history is updated with the previous question.\\nNow that we have added another conversation we are up to three\\nconversations. Considering the memory only retains the last two\\nconversations, our very first question is not remembered.\\nSince we provided an age in our first interaction, we check whether the\\nLLM indeed does not know the age anymore:\\n# Check whether it knows the age we gave it\\nllm_chain.invoke({\"input_prompt\":\"What is my age?\"})\\n{\\'input_prompt\\': \\'What is my age?\\',\\n\\'chat_history\\': \"Human: What is 3 + 3?\\\\nAI: Hello again! 3 + 3 \\nequals 6. If there\\'s anything else I can help you with, just \\nlet me know!\\\\nHuman: What is my name?\\\\nAI: Your name is \\nMaarten.\",\\n\\'text\\': \" I\\'m unable to determine your age as I don\\'t have \\naccess to personal information. Age isn\\'t something that can be ', 'inferred from our current conversation unless you choose to \\nshare it with me. How else may I assist you today?\"}\\nThe LLM indeed has no access to our age since that was not retained in the\\nchat history.\\nAlthough this method reduces the size of the chat history, it can only retain\\nthe last few conversations, which is not ideal for lengthy conversations.\\nLet’s explore how we can summarize the chat history instead.\\nConversation Summary\\nAs we have discussed previously, giving your LLM the ability to remember\\nconversations is vital for a good interactive experience. However, when\\nusing ConversationBufferMemory, the conversation starts to\\nincrease in size and will slowly approach your token limit. Although\\nConversationBufferWindowMemory resolves the issue of token\\nlimits to an extent, only the last k conversations are retained.\\nAlthough a solution would be to use an LLM with a larger context window,\\nthese tokens still need to be processed before generation tokens, which can\\nincrease compute time. Instead, let’s look toward a more sophisticated\\ntechnique, ConversationSummaryMemory. As the name implies, this\\ntechnique summarizes an entire conversation history to distill it into the\\nmain points.\\nThis summarization process is enabled by another LLM that is given the\\nconversation history as input and asked to create a concise summary. A nice\\nadvantage of using an external LLM is that we are not confined to using the\\nsame LLM during conversation. The summarization process is illustrated in\\nFigure 7-12.', 'Figure 7-12. Instead of passing the conversation history directly to the prompt, we use another LLM\\nto summarize it first.\\nThis means that whenever we ask the LLM a question, there are two calls:\\nThe user prompt\\nThe summarization prompt\\nTo use this in LangChain, we first need to prepare a summarization\\ntemplate that we will use as the summarization prompt:\\n# Create a summary prompt template\\nsummary_prompt_template = \"\"\"<s><|user|>Summarize the \\nconversations and update with the new lines.\\nCurrent summary:\\n{summary}\\nnew lines of conversation:\\n{new_lines}\\nNew summary:<|end|>\\n<|assistant|>\"\"\"\\nsummary_prompt = PromptTemplate(\\n    input_variables=[\"new_lines\", \"summary\"],\\n    template=summary_prompt_template\\n)', 'Using ConversationSummaryMemory in LangChain is similar to what\\nwe did with the previous examples. The main difference is that we\\nadditionally need to supply it with an LLM that performs the summarization\\ntask. Although we use the same LLM for both summarizing and user\\nprompting, you could use a smaller LLM for the summarization task to\\nspeed up computation:\\nfrom langchain.memory import ConversationSummaryMemory\\n# Define the type of memory we will use\\nmemory = ConversationSummaryMemory(\\n    llm=llm, \\n    memory_key=\"chat_history\", \\n    prompt=summary_prompt\\n)\\n# Chain the LLM, prompt, and memory together\\nllm_chain = LLMChain(\\n    prompt=prompt,\\n    llm=llm,\\n    memory=memory\\n)\\nHaving created our chain, we can test out its summarization capabilities by\\ncreating a short conversation:\\n# Generate a conversation and ask for the name\\nllm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What \\nis 1 + 1?\"})\\nllm_chain.invoke({\"input_prompt\": \"What is my name?\"})\\n{\\'input_prompt\\': \\'What is my name?\\',\\n\\'chat_history\\': \\' Summary: Human, identified as Maarten, asked \\nthe AI about the sum of 1 + 1, which was correctly answered by \\nthe AI as 2 and offered additional assistance if needed.\\',\\n\\'text\\': \\' Your name in this context was referred to as \\n\"Maarten\". However, since our interaction doesn\\\\\\'t retain \\npersonal data beyond a single session for privacy reasons, I \\ndon\\\\\\'t have access to that information. How can I assist you \\nfurther today?\\'}', 'After each step, the chain will summarize the conversation up until that\\npoint. Note how the first conversation was summarized in\\n\\'chat_history\\' by creating a description of the conversation.\\nWe can continue the conversation and at each step, the conversation will be\\nsummarized and new information will be added as necessary:\\n# Check whether it has summarized everything thus far\\nllm_chain.invoke({\"input_prompt\": \"What was the first question I \\nasked?\"})\\n{\\'input_prompt\\': \\'What was the first question I asked?\\',\\n\\'chat_history\\': \\' Summary: Human, identified as Maarten in the \\ncontext of this conversation, first asked about the sum of 1 + \\n1 and received an answer of 2 from the AI. Later, Maarten \\ninquired about their name but the AI clarified that personal \\ndata is not retained beyond a single session for privacy \\nreasons. The AI offered further assistance if needed.\\',\\n\\'text\\': \\' The first question you asked was \"what\\\\\\'s 1 + 1?\"\\'}\\nAfter asking another question, the LLM updated the summary to include the\\nprevious conversation and correctly inferred the original question.\\nTo get the most recent summary, we can access the memory variable we\\ncreated previously:\\n# Check what the summary is thus far\\nmemory.load_memory_variables({})\\n{\\'chat_history\\': \\' Maarten, identified in this conversation, \\ninitially asked about the sum of 1+1 which resulted in an \\nanswer from the AI being 2. Subsequently, he sought \\nclarification on his name but the AI informed him that no \\npersonal data is retained beyond a single session due to \\nprivacy reasons. The AI then offered further assistance if \\nrequired. Later, Maarten recalled and asked about the first \\nquestion he inquired which was \"what\\\\\\'s 1+1?\"\\'}', 'This more complex chain is illustrated in Figure 7-13 to give an overview\\nof this additional functionality.\\nFigure 7-13. We extend the LLM chain with memory by summarizing the entire conversation history\\nbefore giving it to the input prompt.\\nThis summarization helps keep the chat history relatively small without\\nusing too many tokens during inference. However, since the original\\nquestion was not explicitly saved in the chat history, the model needed to\\ninfer it based on the context. This is a disadvantage if specific information\\nneeds to be stored in the chat history. Moreover, multiple calls to the same\\nLLM are needed, one for the prompt and one for the summarization. This\\ncan slow down computing time.\\nOften, it is a trade-off between speed, memory, and accuracy. Where\\nConversationBufferMemory is instant but hogs tokens,\\nConversationSummaryMemory is slow but frees up tokens to use.\\nAdditional pros and cons of the memory types we have explored thus far\\nare described in Table 7-1.', 'Table 7-1. The pros and cons of different memory types.\\nMemory typePros Cons\\nConversation\\nBuffer Easiest\\nimplementation\\nEnsures no\\ninformation loss\\nwithin context\\nwindow\\nSlower\\ngeneration\\nspeed as more\\ntokens are\\nneeded\\nOnly suitable\\nfor large-context\\nLLMs\\nLarger chat\\nhistories make\\ninformation\\nretrieval\\ndifficult\\nWindowed\\nConversation\\nBuffer\\nLarge-context\\nLLMs are not\\nneeded unless\\nchat history is\\nlarge\\nNo information\\nloss over the last\\nk interactions\\nOnly captures\\nthe last k\\ninteractions\\nNo compression\\nof the last k\\ninteractions\\nConversation\\nSummary Captures the full\\nhistory\\nEnables long\\nconversations\\nAn additional\\ncall is necessary\\nfor each\\ninteraction', 'Memory typePros Cons\\nReduces tokens\\nneeded to capture\\nfull history\\nQuality is\\nreliant on the\\nLLM’s\\nsummarization\\ncapabilities\\nAgents: Creating a System of LLMs\\nThus far, we have created systems that follow a user-defined set of steps to\\ntake. One of the most promising concepts in LLMs is their ability to\\ndetermine the actions they can take. This idea is often called agents,\\nsystems that leverage a language model to determine which actions they\\nshould take and in what order.\\nAgents can make use of everything we have seen thus far, such as model\\nI/O, chains, and memory, and extend it further with two vital components:\\nTools that the agent can use to do things it could not do itself\\nThe agent type, which plans the actions to take or tools to use\\nUnlike the chains we have seen thus far, agents are able to show more\\nadvanced behavior like creating and self-correcting a roadmap to achieve a\\ngoal. They can interact with the real world through the use of tools. As a\\nresult, these agents can perform a variety of tasks that go beyond what an\\nLLM is capable of in isolation.\\nFor example, LLMs are notoriously bad at mathematical problems and\\noften fail at solving simple math-based tasks but they could do much more\\nif we provide access to a calculator. As illustrated in Figure 7-14, the\\nunderlying idea of agents is that they utilize LLMs not only to understand\\nour query but also to decide which tool to use and when.', 'Figure 7-14. Giving LLMs the ability to choose which tools they use for a particular problem results\\nin more complex and accurate behavior.\\nIn this example, we would expect the LLM to use the calculator when it\\nfaces a mathematical task. Now imagine we extend this with dozens of\\nother tools, like a search engine or a weather API. Suddenly, the capabilities\\nof LLMs increase significantly.\\nIn other words, agents that make use of LLMs can be powerful general\\nproblem solvers. Although the tools they use are important, the driving\\nforce of many agent-based systems is the use of a framework called\\nReasoning and Acting (ReAct1 ).\\nThe Driving Power Behind Agents: Step-by-step\\nReasoning\\nReAct is a powerful framework that combines two important concepts in\\nbehavior: reasoning and acting. LLMs are exceptionally powerful when it\\ncomes to reasoning as we explored in detail in Chapter 5.\\nActing is a bit of a different story. LLMs are not able to act like you and I\\ndo. To give them the ability to act, we could tell an LLM that it can use\\ncertain tools, like a weather forecasting API. However, since LLMs can\\nonly generate text, they would need to be instructed to use specific queries\\nto trigger the forecasting API.', 'ReAct merges these two concepts and allows reasoning to affect acting and\\nactions to affect reasoning. In practice, the framework consists of iteratively\\nfollowing these three steps:\\nThought\\nAction\\nObservation\\nIllustrated in Figure 7-15, the LLM is asked to create a “thought” about the\\ninput prompt. This is similar to asking the LLM what it thinks it should do\\nnext and why. Then, based on the thought, an “action” is triggered. The\\naction is generally an external tool, like a calculator or a search engine.\\nFinally, after the results of the “action” are returned to the LLM it\\n“observes” the output, which is often a summary of whatever result it\\nretrieved.\\nTo illustrate with an example, imagine you are on holiday in the United\\nStates and interested in buying a MacBook Pro. Not only do you want to\\nknow the price but you need it converted to EUR as you live in Europe and\\nare more comfortable with those prices.\\nAs illustrated in Figure 7-16, the agent will first search the web for current\\nprices. It might find one or more prices depending on the search engine.\\nAfter retrieving the price, it will use a calculator to convert USD to EUR\\nassuming we know the exchange rate.', 'Figure 7-15. An example of a ReAct prompt template.', 'Figure 7-16. An example of two cycles in a ReAct pipeline.\\nDuring this process, the agent describes its thoughts (what it should do), its\\nactions (what it will do), and its observations (the results of the action). It is\\na cycle of thoughts, actions, and observations that results in the agent’s\\noutput.\\nReAct in LangChain\\nTo illustrate how agents work in LangChain, we are going to build a\\npipeline that can search the web for answers and perform calculations with\\na calculator. These autonomous processes generally require an LLM that is\\npowerful enough to properly follow complex instructions.\\nThe LLM that we used thus far is relatively small and not sufficient to run\\nthese examples. Instead, we will be using OpenAI’s GPT-3.5 model as it\\nfollows these complex instructions more closely:', 'import os\\nfrom langchain_openai import ChatOpenAI\\n# Load OpenAI\\'s LLMs with LangChain\\nos.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\"\\nopenai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", \\ntemperature=0)\\nNOTE\\nAlthough the LLM we used throughout the chapter is insufficient for this example, it\\ndoes not mean that only OpenAI’s LLMs are. Larger useful LLMs exist but they require\\nsignificantly more compute and VRAM. For instance, local LLMs often come in\\ndifferent sizes and within a family of models, increasing a model’s size leads to better\\nperformance. To keep the necessary compute at a minimum, we choose a smaller LLM\\nthroughout the examples in this chapter.\\nHowever, as the field of generative models evolves, so do these smaller LLMs. We\\nwould be anything but surprised if eventually smaller LLMs, like the one used in this\\nchapter, would be capable enough to run this example.\\nAfter doing so, we will define the template for our agent. As we have\\nshown before, it describes the ReAct steps it needs to follow:\\n# Create the ReAct template\\nreact_template = \"\"\"Answer the following questions as best you \\ncan. You have access to the following tools:\\n{tools}\\nUse the following format:\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N \\ntimes)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question', 'Begin!\\nQuestion: {input}\\nThought:{agent_scratchpad}\"\"\"\\nprompt = PromptTemplate(\\n    template=react_template,\\n    input_variables=[\"tools\", \"tool_names\", \"input\", \\n\"agent_scratchpad\"]\\n)\\nThis template illustrates the process of starting with a question and\\ngenerating intermediate thoughts, actions, and observations.\\nTo have the LLM interact with the outside world, we will describe the tools\\nit can use:\\nfrom langchain.agents import load_tools, Tool\\nfrom langchain.tools import DuckDuckGoSearchResults\\n# You can create the tool to pass to an agent\\nsearch = DuckDuckGoSearchResults()\\nsearch_tool = Tool(\\n    name=\"duckduck\",\\n    description=\"A web search engine. Use this to as a search \\nengine for general queries.\",\\n    func=search.run,\\n)\\n# Prepare tools\\ntools = load_tools([\"llm-math\"], llm=openai_llm)\\ntools.append(search_tool)\\nThe tools include the DuckDuckGo search engine and a math tool that\\nallows it to access a basic calculator.\\nFinally, we create the ReAct agent and pass it to the AgentExecutor,\\nwhich handles executing the steps:\\nfrom langchain.agents import AgentExecutor, create_react_agent', '# Construct the ReAct agent\\nagent = create_react_agent(openai_llm, tools, prompt)\\nagent_executor = AgentExecutor(\\n    agent=agent, tools=tools, verbose=True, \\nhandle_parsing_errors=True\\n)\\nTo test whether the agent works, we use the previous example, namely\\nfinding the price of a MacBook Pro:\\n# What is the price of a MacBook Pro?\\nagent_executor.invoke(\\n    {\\n        \"input\": \"What is the current price of a MacBook Pro in \\nUSD? How much would it cost in EUR if the exchange rate is 0.85 \\nEUR for 1 USD.\"\\n    }\\n)\\nWhile executing, the model generates multiple intermediate steps similar to\\nthe steps illustrated in Figure 7-17.\\nFigure 7-17. An example of the ReAct process in LangChain.\\nThese intermediate steps illustrate how the model processes the ReAct\\ntemplate and what tools it accesses. This allows us to debug issues and\\nexplore whether the agent uses the tools correctly.\\nWhen finished, the model gives us an output like this:\\n{\\'input\\': \\'What is the current price of a MacBook Pro in USD? \\nHow much would it cost in EUR if the exchange rate is 0.85 EUR \\nfor 1 USD?\\',\\n \\'output\\': \\'The current price of a MacBook Pro in USD is ', \"$2,249.00. It would cost approximately 1911.65 EUR with an \\nexchange rate of 0.85 EUR for 1 USD.'}\\nConsidering the limited tools the agent has, this is quite impressive! Using\\njust a search engine and a calculator the agent could give us an answer.\\nWhether that answer is actually correct should be taken into account. By\\ncreating this relatively autonomous behavior, we are not involved in the\\nintermediate steps. As such, there is no human in the loop to judge the\\nquality of the output or reasoning process.\\nThis double-edged sword requires a careful system design to improve its\\nreliability. For instance, we could have the agent return the website’s URL\\nwhere it found the MacBook Pro’s price or ask whether the output is correct\\nat each step.\\nSummary\\nIn this chapter, we explored several ways to extend the capabilities of LLMs\\nby adding modular components. We began by creating a simple but reusable\\nchain that connected the LLM with a prompt template. We then expanded\\non this concept by adding memory to the chain, which allowed the LLM to\\nremember conversations. We explored three different methods to add\\nmemory and discussed their strengths and weaknesses.\\nWe then delved into the world of agents that leverage LLMs to determine\\ntheir actions and make decisions. We explored the ReAct framework, which\\nuses an intuitive prompting framework that allows agents to reason about\\ntheir thoughts, take actions, and observe the results. This led us to build an\\nagent that is able to freely use the tools at its disposal, such as searching the\\nweb and using a calculator, demonstrating the potential power of agents.\\nWith this foundation in place, we are now poised to explore ways in which\\nLLMs can be used to improve existing search systems and even become the\\ncore of new, more powerful search systems, as discussed in the next\\nchapter.\", '1  Shunyu Yao et al. “ReAct: Synergizing reasoning and acting in language models.” arXiv\\npreprint arXiv:2210.03629 (2022).\\nOceanofPDF.com', 'Chapter 8. Semantic Search\\nand Retrieval-Augmented\\nGeneration\\nSearch was one of the first language model applications to see broad\\nindustry adoption. Months after the release of the seminal “BERT: Pre-\\ntraining of deep bidirectional transformers for language understanding”\\n(2018) paper, Google announced it was using it to power Google Search\\nand that it represented “one of the biggest leaps forward in the history of\\nSearch.” Not to be outdone, Microsoft Bing also stated that “Starting from\\nApril of this year, we used large transformer models to deliver the largest\\nquality improvements to our Bing customers in the past year.”\\nThis is a clear testament to the power and usefulness of these models. Their\\naddition instantly and dramatically improves some of the most mature,\\nwell-maintained systems that billions of people around the planet rely on.\\nThe ability they add is called semantic search, which enables searching by\\nmeaning, and not simply keyword matching.\\nOn a separate track, the fast adoption of text generation models led many\\nusers to ask the models questions and expect factual answers. And while the\\nmodels were able to answer fluently and confidently, their answers were not\\nalways correct or up-to-date. This problem grew to be known as model\\n“hallucinations,” and one of the leading ways to reduce it is to build\\nsystems that can retrieve relevant information and provide it to the LLM to\\naid it in generating more factual answers. This method, called RAG, is one\\nof the most popular applications of LLMs.', 'Overview of Semantic Search and RAG\\nThere’s a lot of research on how to best use language models for search.\\nThree broad categories of these models are dense retrieval, reranking, and\\nRAG. Here is an overview of these three categories that the rest of the\\nchapter will then explain in more detail:\\nDense retrieval\\nDense retrieval systems rely on the concept of embeddings, the same\\nconcept we’ve encountered in the previous chapters, and turn the search\\nproblem into retrieving the nearest neighbors of the search query (after\\nboth the query and the documents are converted into embeddings).\\nFigure 8-1 shows how dense retrieval takes a search query, consults its\\narchive of texts, and outputs a set of relevant results.\\nFigure 8-1. Dense retrieval is one of the key types of semantic search, relying on the similarity\\nof text embeddings to retrieve relevant results.\\nReranking\\nSearch systems are often pipelines of multiple steps. A reranking\\nlanguage model is one of these steps and is tasked with scoring the', 'relevance of a subset of results against the query; the order of results is\\nthen changed based on these scores. Figure 8-2 shows how rerankers are\\ndifferent from dense retrieval in that they take an additional input: a set\\nof search results from a previous step in the search pipeline.\\nFigure 8-2. Rerankers, the second key type of semantic search, take a search query and a\\ncollection of results, and reorder them by relevance, often resulting in vastly improved results.\\nRAG\\nThe growing LLM capability of text generation led to a new type of\\nsearch systems that include a model that generates an answer in\\nresponse to a query. Figure 8-3 shows an example of such a generative\\nsearch system.\\nGenerative search is a subset of a broader type of category of systems\\nbetter called RAG systems. These are text generation systems that\\nincorporate search capabilities to reduce hallucinations, increase\\nfactuality, and/or ground the generation model on a specific dataset.', 'Figure 8-3. A RAG system formulates an answer to a question and (preferably) cites its information\\nsources.\\nThe rest of the chapter covers these three types of systems in more detail.\\nWhile these are the major categories, they are not the only LLM\\napplications in the domain of search.\\nSemantic Search with Language Models\\nLet’s now dive into more detail on the major categories of systems that can\\nupgrade the search capabilities of our language models. We’ll start with\\ndense retrieval and then move on through reranking and RAG.\\nDense Retrieval\\nRecall that embeddings turn text into numeric representations. Those can be\\nthought of as points in space, as we can see in Figure 8-4. Points that are\\nclose together mean that the text they represent is similar. So in this\\nexample, text 1 and text 2 are more similar to each other (because they are\\nnear each other) than text 3 (because it’s farther away).', 'Figure 8-4. The intuition of embeddings: each text is a point and texts with similar meaning are close\\nto each other.\\nThis is the property that is used to build search systems. In this scenario,\\nwhen a user enters a search query, we embed the query, thus projecting it\\ninto the same space as our text archive. Then we simply find the nearest\\ndocuments to the query in that space, and those would be the search results\\n(Figure 8-5).', 'Figure 8-5. Dense retrieval relies on the property that search queries will be close to their relevant\\nresults.\\nJudging by the distances in Figure 8-5, “text 2” is the best result for this\\nquery, followed by “text 1.” Two questions could arise here, however:\\nShould text 3 even be returned as a result? That’s a decision for\\nyou, the system designer. It’s sometimes desirable to have a max\\nthreshold of similarity score to filter out irrelevant results (in case\\nthe corpus has no relevant results for the query).\\nAre a query and its best result semantically similar? Not always.\\nThis is why language models need to be trained on question-\\nanswer pairs to become better at retrieval. This process is\\nexplained in more detail in Chapter 10.\\nFigure 8-6 shows how we chunk a document before proceeding to embed\\neach chunk. Those embedding vectors are then stored in the vector database\\nand are ready for retrieval.', 'Figure 8-6. Convert some external knowledge base to a vector database. We can then query this\\nvector database for information about the knowledge base.\\nDense retrieval example\\nLet’s take a look at a dense retrieval example by using Cohere to search the\\nWikipedia page for the film Interstellar. In this example, we will do the\\nfollowing:\\n1. Get the text we want to make searchable and apply some light\\nprocessing to chunk it into sentences.\\n2. Embed the sentences.\\n3. Build the search index.\\n4. Search and see the results.\\nGet your Cohere API key by signing up at https://oreil.ly/GxrQ1. Paste it in\\nthe following code. You will not have to pay anything to run through this\\nexample.\\nLet’s import the libraries we’ll need:\\nimport cohere\\nimport numpy as np\\nimport pandas as pd\\nfrom tqdm import tqdm\\n# Paste your API key here. Remember to not share publicly', 'api_key = \\'\\'\\n# Create and retrieve a Cohere API key from os.cohere.ai\\nco = cohere.Client(api_key)\\nGetting the text archive and chunking it\\nLet’s use the first section of the Wikipedia article on the film Interstellar.\\nWe’ll get the text, then break it into sentences:\\ntext = \"\"\"\\nInterstellar is a 2014 epic science fiction film co-written, \\ndirected, and produced by Christopher Nolan. \\nIt stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, \\nBill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine. \\nSet in a dystopian future where humanity is struggling to \\nsurvive, the film follows a group of astronauts who travel \\nthrough a wormhole near Saturn in search of a new home for \\nmankind.\\nBrothers Christopher and Jonathan Nolan wrote the screenplay, \\nwhich had its origins in a script Jonathan developed in 2007. \\nCaltech theoretical physicist and 2017 Nobel laureate in \\nPhysics[4] Kip Thorne was an executive producer, acted as a \\nscientific consultant, and wrote a tie-in book, The Science of \\nInterstellar. \\nCinematographer Hoyte van Hoytema shot it on 35 mm movie film in \\nthe Panavision anamorphic format and IMAX 70 mm. \\nPrincipal photography began in late 2013 and took place in \\nAlberta, Iceland, and Los Angeles. \\nInterstellar uses extensive practical and miniature effects and \\nthe company Double Negative created additional digital effects.\\nInterstellar premiered on October 26, 2014, in Los Angeles. \\nIn the United States, it was first released on film stock, \\nexpanding to venues using digital projectors. \\nThe film had a worldwide gross over $677 million (and $773 \\nmillion with subsequent re-releases), making it the tenth-highest \\ngrossing film of 2014. \\nIt received acclaim for its performances, direction, screenplay, \\nmusical score, visual effects, ambition, themes, and emotional \\nweight. \\nIt has also received praise from many astronomers for its \\nscientific accuracy and portrayal of theoretical astrophysics. \\nSince its premiere, Interstellar gained a cult following,[5] and ', 'now is regarded by many sci-fi experts as one of the best \\nscience-fiction films of all time.\\nInterstellar was nominated for five awards at the 87th Academy \\nAwards, winning Best Visual Effects, and received numerous other \\naccolades\"\"\"\\n# Split into a list of sentences\\ntexts = text.split(\\'.\\')\\n# Clean up to remove empty spaces and new lines\\ntexts = [t.strip(\\' \\\\n\\') for t in texts]\\nEmbedding the text chunks\\nLet’s now embed the texts. We’ll send them to the Cohere API, and get back\\na vector for each text:\\n# Get the embeddings\\nresponse = co.embed(\\n  texts=texts,\\n  input_type=\"search_document\",\\n).embeddings\\nembeds = np.array(response)\\nprint(embeds.shape)\\nThis outputs (15, 4096), which indicates that we have 15 vectors, each\\none of size 4,096.\\nBuilding the search index\\nBefore we can search, we need to build a search index. An index stores the\\nembeddings and is optimized to quickly retrieve the nearest neighbors even\\nif we have a very large number of points:\\nimport faiss\\ndim = embeds.shape[1]\\nindex = faiss.IndexFlatL2(dim)\\nprint(index.is_trained)\\nindex.add(np.float32(embeds))', 'Search the index\\nWe can now search the dataset using any query we want. We simply embed\\nthe query and present its embedding to the index, which will retrieve the\\nmost similar sentence from the Wikipedia article.\\nLet’s define our search function:\\ndef search(query, number_of_results=3):\\n  \\n  # 1. Get the query\\'s embedding\\n  query_embed = co.embed(texts=[query], \\n                input_type=\"search_query\",).embeddings[0]\\n  # 2. Retrieve the nearest neighbors\\n  distances , similar_item_ids = \\nindex.search(np.float32([query_embed]), number_of_results) \\n  # 3. Format the results\\n  texts_np = np.array(texts) # Convert texts list to numpy for \\neasier indexing\\n  results = pd.DataFrame(data={\\'texts\\': \\ntexts_np[similar_item_ids[0]], \\n                              \\'distance\\': distances[0]})\\n  \\n  # 4. Print and return the results\\n  print(f\"Query:\\'{query}\\'\\\\nNearest neighbors:\")\\n  return results\\nWe are now ready to write a query and search the texts!\\nquery = \"how precise was the science\"\\nresults = search(query)\\nresults\\nThis produces the following output:\\nQuery: \\'how precise was the science\\'\\nNearest neighbors:', ' texts distance\\n0 It has also received praise from\\nmany astronomers for its\\nscientific accuracy and portrayal\\nof theoretical astrophysics\\n10757.379883\\n1 Caltech theoretical physicist and\\n2017 Nobel laureate in\\nPhysics[4] Kip Thorne was an\\nexecutive producer, acted as a\\nscientific consultant, and wrote a\\ntie-in book, The Science of\\nInterstellar\\n11566.131836\\n2 Interstellar uses extensive\\npractical and miniature effects\\nand the company Double\\nNegative created additional\\ndigital effects\\n11922.833008\\nThe first result has the least distance, and so is the most similar to the query.\\nLooking at it, it answers the question perfectly. Notice that this wouldn’t\\nhave been possible if we were only doing keyword search because the top\\nresult did not include the same keywords in the query.\\nWe can actually verify that by defining a keyword search function to\\ncompare the two. We’ll use the BM25 algorithm, which is one of the\\nleading lexical search methods. See this notebook for the source of these\\ncode snippets:\\nfrom rank_bm25 import BM25Okapi\\nfrom sklearn.feature_extraction import _stop_words\\nimport string', 'def bm25_tokenizer(text):\\n    tokenized_doc = []\\n    for token in text.lower().split():\\n        token = token.strip(string.punctuation)\\n        if len(token) > 0 and token not in \\n_stop_words.ENGLISH_STOP_WORDS:\\n            tokenized_doc.append(token)\\n    return tokenized_doc\\ntokenized_corpus = []\\nfor passage in tqdm(texts):\\n    tokenized_corpus.append(bm25_tokenizer(passage))\\nbm25 = BM25Okapi(tokenized_corpus)\\ndef keyword_search(query, top_k=3, num_candidates=15):\\n    print(\"Input question:\", query)\\n    ##### BM25 search (lexical search) #####\\n    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\\n    top_n = np.argpartition(bm25_scores, -num_candidates)[-\\nnum_candidates:]\\n    bm25_hits = [{\\'corpus_id\\': idx, \\'score\\': bm25_scores[idx]} for \\nidx in top_n]\\n    bm25_hits = sorted(bm25_hits, key=lambda x: x[\\'score\\'], \\nreverse=True)\\n    \\n    print(f\"Top-3 lexical search (BM25) hits\")\\n    for hit in bm25_hits[0:top_k]:\\n        print(\"\\\\t{:.3f}\\\\t{}\".format(hit[\\'score\\'], \\ntexts[hit[\\'corpus_id\\']].replace(\"\\\\n\", \" \")))\\nNow when we search for the same query, we get a different set of results\\nfrom the dense retrieval search:\\nkeyword_search(query = \"how precise was the science\")', \"Results:\\nInput question: how precise was the science\\nTop-3 lexical search (BM25) hits\\n      1.789 Interstellar is a 2014 epic science fiction film \\nco-written, directed, and produced by Christopher Nolan\\n      1.373 Caltech theoretical physicist and 2017 Nobel \\nlaureate in Physics[4] Kip Thorne was an executive producer, \\nacted as a scientific consultant, and wrote a tie-in book, The \\nScience of Interstellar\\n      0.000 It stars Matthew McConaughey, Anne Hathaway, \\nJessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and \\nMichael Caine\\nNote that the first result does not really answer the question despite it\\nsharing the word “science” with the query. In the next section, we’ll see\\nhow adding a reranker can improve this search system. But before that, let’s\\ncomplete our overview of dense retrieval by looking at its caveats and go\\nover some methods of breaking down texts into chunks.\\nCaveats of dense retrieval\\nIt’s useful to be aware of some of the drawbacks of dense retrieval and how\\nto address them. What happens, for example, if the texts don’t contain the\\nanswer? We still get results and their distances. For example:\\nQuery:'What is the mass of the moon?'\\nNearest neighbors:\", ' texts distance\\n0 The film had a worldwide gross\\nover $677 million (and $773\\nmillion with subsequent re-\\nreleases), making it the tenth-\\nhighest grossing film of 2014\\n1.298275\\n1 It has also received praise from\\nmany astronomers for its\\nscientific accuracy and portrayal\\nof theoretical astrophysics\\n1.324389\\n2 Cinematographer Hoyte van\\nHoytema shot it on 35 mm movie\\nfilm in the Panavision\\nanamorphic format and IMAX 70\\nmm\\n1.328375\\nIn cases like this, one possible heuristic is to set a threshold level—a\\nmaximum distance for relevance, for example. A lot of search systems\\npresent the user with the best info they can get and leave it up to the user to\\ndecide if it’s relevant or not. Tracking the information of whether the user\\nclicked on a result (and were satisfied by it) can improve future versions of\\nthe search system.\\nAnother caveat of dense retrieval is when a user wants to find an exact\\nmatch for a specific phrase. That’s a case that’s perfect for keyword\\nmatching. That’s one reason why hybrid search, which includes both\\nsemantic search and keyword search, is advised instead of relying solely on\\ndense retrieval.\\nDense retrieval systems also find it challenging to work properly in\\ndomains other than the ones that they were trained on. So, for example, if', 'you train a retrieval model on internet and Wikipedia data, and then deploy\\nit on legal texts (without having enough legal data as part of the training\\nset), the model will not work as well in that legal domain.\\nThe final thing we’d like to point out is that this is a case where each\\nsentence contained a piece of information, and we showed queries that\\nspecifically ask for that information. What about questions whose answers\\nspan multiple sentences? This highlights one of the important design\\nparameters of dense retrieval systems: what is the best way to chunk long\\ntexts? And why do we need to chunk them in the first place?\\nChunking long texts\\nOne limitation of Transformer language models is that they are limited in\\ncontext sizes, meaning we cannot feed them very long texts that go above\\nthe number of words or tokens that the model supports. So how do we\\nembed long texts?\\nThere are several possible ways, and two possible approaches shown in\\nFigure 8-7 include indexing one vector per document and indexing multiple\\nvectors per document.\\nFigure 8-7. It’s possible to create one vector representing an entire document, but it’s better for\\nlonger documents to be split into smaller chunks that get their own embeddings.', 'One vector per document\\nIn this approach, we use a single vector to represent the whole document.\\nThe possibilities here include:\\nEmbedding only a representative part of the document and\\nignoring the rest of the text. This may mean embedding only the\\ntitle, or only the beginning of the document. This is useful to get\\nquickly started with building a demo but it leaves a lot of\\ninformation unindexed and therefore unsearchable. As an\\napproach, it may work better for documents where the beginning\\ncaptures the main points of a document (think: Wikipedia article).\\nBut it’s really not the best approach for a real system because a lot\\nof information would be left out of the index and would be\\nunsearchable.\\nEmbedding the document in chunks, embedding those chunks, and\\nthen aggregating those chunks into a single vector. The usual\\nmethod of aggregation here is to average those vectors. A downside\\nof this approach is that it results in a highly compressed vector that\\nloses a lot of the information in the document.\\nThis approach can satisfy some information needs, but not others. A lot of\\nthe time, a search is for a specific piece of information contained in an\\narticle, which is better captured if the concept had its own vector.\\nMultiple vectors per document\\nIn this approach, we chunk the document into smaller pieces, and embed\\nthose chunks. Our search index then becomes that of chunk embeddings,\\nnot entire document embeddings. Figure 8-8 shows a number of possible\\ntext chunking approaches.', 'Figure 8-8. Several chunking methods and their effects on the input text. Overlapping chunks can be\\nimportant to prevent the absence of context.\\nThe chunking approach is better because it has full coverage of the text and\\nbecause the vectors tend to capture individual concepts inside the text. This\\nleads to a more expressive search index. Figure 8-9 shows a number of\\npossible approaches.\\nFigure 8-9. A number of possible options for chunking a document for embedding.\\nThe best way of chunking a long text will depend on the types of texts and\\nqueries your system anticipates. Approaches include:', 'Each sentence is a chunk. The issue here is this could be too\\ngranular and the vectors don’t capture enough of the context.\\nEach paragraph is a chunk. This is great if the text is made up of\\nshort paragraphs. Otherwise, it may be that every 3–8 sentences is\\na chunk.\\nSome chunks derive a lot of their meaning from the text around\\nthem. So we can incorporate some context via:\\nAdding the title of the document to the chunk.\\nAdding some of the text before and after them to the\\nchunk. This way, the chunks can overlap so they include\\nsome surrounding text that also appears in adjacent\\nchunks. This is what we can see in Figure 8-10.\\nExpect more chunking strategies to arise as the field develops—some of\\nwhich may even use LLMs to dynamically split a text into meaningful\\nchunks.\\nFigure 8-10. Chunking the text into overlapping segments is one strategy to retain more of the\\ncontext around different segments.\\nNearest neighbor search versus vector databases\\nOnce the query is embedded, we need to find the nearest vectors to it from\\nour text archive as we can see in Figure 8-11. The most straightforward way\\nto find the nearest neighbors is to calculate the distances between the query\\nand the archive. That can easily be done with NumPy and is a reasonable\\napproach if you have thousands or tens of thousands of vectors in your\\narchive.', 'Figure 8-11. As we saw in Chapter 3, we can compare embeddings to quickly find the most similar\\ndocuments to a query.\\nAs you scale beyond to the millions of vectors, an optimized approach for\\nretrieval is to rely on approximate nearest neighbor search libraries like\\nAnnoy or FAISS. These allow you to retrieve results from massive indexes\\nin milliseconds and some of them can improve their performance by\\nutilizing GPUs and scaling to clusters of machines to serve very large\\nindices.\\nAnother class of vector retrieval systems are vector databases like Weaviate\\nor Pinecone. A vector database allows you to add or delete vectors without\\nhaving to rebuild the index. They also provide ways to filter your search or\\ncustomize it in ways beyond merely vector distances.\\nFine-tuning embedding models for dense retrieval\\nJust as we discussed in Chapter 4 on text classification, we can improve the\\nperformance of an LLM on a task using fine-tuning. As in that case,\\nretrieval needs to optimize text embeddings and not simply token\\nembeddings. The process for this fine-tuning is to get training data\\ncomposed of queries and relevant results.', 'Let’s look at one example from our dataset, the sentence “Interstellar\\npremiered on October 26, 2014, in Los Angeles.” Two possible queries\\nwhere this is a relevant result are:\\nRelevant query 1: “Interstellar release date”\\nRelevant query 2: “When did Interstellar premier”\\nThe fine-tuning process aims to make the embeddings of these queries close\\nto the embedding of the resulting sentence. It also needs to see negative\\nexamples of queries that are not relevant to the sentence, for example:\\nIrrelevant query: “Interstellar cast”\\nWith these examples, we now have three pairs—two positive pairs and one\\nnegative pair. Let’s assume, as we can see in Figure 8-12, that before fine-\\ntuning, all three queries have the same distance from the result document.\\nThat’s not far-fetched because they all talk about Interstellar.\\nFigure 8-12. Before fine-tuning, the embeddings of both relevant and irrelevant queries may be close\\nto a particular document.', 'The fine-tuning step works to make the relevant queries closer to the\\ndocument and at the same time make irrelevant queries farther from the\\ndocument. We can see this effect in Figure 8-13.\\nFigure 8-13. After the fine-tuning process, the text embedding model becomes better at this search\\ntask by incorporating how we define relevance on our dataset using the examples we provided of\\nrelevant and irrelevant documents.\\nReranking\\nA lot of organizations have already built search systems. For those\\norganizations, an easier way to incorporate language models is as a final\\nstep inside their search pipeline. This step is tasked with changing the order\\nof the search results based on relevance to the search query. This one step\\ncan vastly improve search results and it’s in fact what Microsoft Bing added\\nto achieve the improvements to search results using BERT-like models.\\nFigure 8-14 shows the structure of a rerank search system serving as the\\nsecond stage in a two-stage search system.', 'Figure 8-14. LLM rerankers operate as part of a search pipeline with the goal of reordering a\\nnumber of shortlisted search results by relevance.\\nReranking example\\nA reranker takes in the search query and a number of search results, and\\nreturns the optimal ordering of these documents so the most relevant ones\\nto the query are higher in ranking. Cohere’s Rerank endpoint is a simple\\nway to start using a first reranker. We simply pass it the query and texts and\\nget the results back. We don’t need to train or tune it:\\nquery = \"how precise was the science\"\\nresults = co.rerank(query=query, documents=texts, top_n=3, \\nreturn_documents=True)\\nresults.results\\nWe can print these results:\\nfor idx, result in enumerate(results.results):\\n  print(idx, result.relevance_score , result.document.text)\\nOutput:\\n0 0.1698185 It has also received praise from many astronomers \\nfor its scientific accuracy and portrayal of theoretical \\nastrophysics\\n1 0.07004896 The film had a worldwide gross over $677 million ', '(and $773 million with subsequent re-releases), making it the \\ntenth-highest grossing film of 2014\\n2 0.0043994132 Caltech theoretical physicist and 2017 Nobel \\nlaureate in Physics[4] Kip Thorne was an executive producer, \\nacted as a scientific consultant, and wrote a tie-in book, The \\nScience of Interstellar\\nThis shows the reranker is much more confident about the first result,\\nassigning it a relevance score of 0.16, while the other results are scored\\nmuch lower in relevance.\\nIn this basic example, we passed our reranker all 15 of our documents.\\nMore often, however, our index would have thousands or millions of\\nentries, and we need to shortlist, say one hundred or one thousand results\\nand then present those to the reranker. This shortlisting step is called the\\nfirst stage of the search pipeline.\\nThe first-stage retriever can be keyword search, dense retrieval, or better yet\\n—hybrid search that uses both of them. We can revisit our previous\\nexample to see how adding a reranker after a keyword search system\\nimproves its performance.\\nLet’s tweak our keyword search function so it retrieves a list of the top 10\\nresults using keyword search, then use rerank to choose the top 3 results\\nfrom those 10:\\ndef keyword_and_reranking_search(query, top_k=3, \\nnum_candidates=10):\\n    print(\"Input question:\", query)\\n    ##### BM25 search (lexical search) #####\\n    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\\n    top_n = np.argpartition(bm25_scores, -num_candidates)[-\\nnum_candidates:]\\n    bm25_hits = [{\\'corpus_id\\': idx, \\'score\\': bm25_scores[idx]} for \\nidx in top_n]\\n    bm25_hits = sorted(bm25_hits, key=lambda x: x[\\'score\\'], \\nreverse=True)\\n    \\n    print(f\"Top-3 lexical search (BM25) hits\")', '    for hit in bm25_hits[0:top_k]:\\n        print(\"\\\\t{:.3f}\\\\t{}\".format(hit[\\'score\\'], \\ntexts[hit[\\'corpus_id\\']].replace(\"\\\\n\", \" \")))\\n   \\n    #Add re-ranking\\n    docs = [texts[hit[\\'corpus_id\\']] for hit in bm25_hits]\\n    \\n    print(f\"\\\\nTop-3 hits by rank-API ({len(bm25_hits)} BM25 hits \\nre-ranked)\")\\n    results = co.rerank(query=query, documents=docs, top_n=top_k, \\nreturn_documents=True)\\n    # print(results.results)\\n    for hit in results.results:\\n        # print(hit)\\n        print(\"\\\\t{:.3f}\\\\t{}\".format(hit.relevance_score, \\nhit.document.text.replace(\"\\\\n\", \" \")))\\nNow we can send our query and check the results of keyword search and\\nthen the result of keyword search shortlisting its top 10 results, then pass\\nthem on to the reranker:\\nkeyword_and_reranking_search(query = \"how precise was the \\nscience\")\\nResults:\\nInput question: how precise was the science\\nTop-3 lexical search (BM25) hits\\n1.789 Interstellar is a 2014 epic science fiction film co-\\nwritten, directed, and produced by Christopher Nolan\\n1.373 Caltech theoretical physicist and 2017 Nobel laureate in \\nPhysics[4] Kip Thorne was an executive producer, acted as a \\nscientific consultant, and wrote a tie-in book, The Science of \\nInterstellar\\n0.000 Interstellar uses extensive practical and miniature \\neffects and the company Double Negative created additional \\ndigital effects\\nTop-3 hits by rank-API (10 BM25 hits re-ranked)', '0.004 Caltech theoretical physicist and 2017 Nobel laureate in \\nPhysics[4] Kip Thorne was an executive producer, acted as a \\nscientific consultant, and wrote a tie-in book, The Science of \\nInterstellar\\n0.004 Set in a dystopian future where humanity is struggling to \\nsurvive, the film follows a group of astronauts who travel \\nthrough a wormhole near Saturn in search of a new home for \\nmankind\\n0.003 Brothers Christopher and Jonathan Nolan wrote the \\nscreenplay, which had its origins in a script Jonathan \\ndeveloped in 2007\\nWe see that keyword search assigns scores to only two results that share\\nsome of the keywords. In the second set of results, the reranker elevates the\\nsecond result appropriately as the most relevant result for the query. This is\\na toy example that gives us a glimpse of the effect, but in practice, such a\\npipeline significantly improves search quality. On a multilingual benchmark\\nlike MIRACL, a reranker can boost performance from 36.5 to 62.8,\\nmeasured as nDCG@10 (more on evaluation later in this chapter).\\nOpen source retrieval and reranking with sentence transformers\\nIf you want to locally set up retrieval and reranking on your own machine,\\nthen you can use the Sentence Transformers library. Refer to the\\ndocumentation at https://oreil.ly/jJOhV for setup. Check the “Retrieve &\\nRe-Rank” section for instructions and code examples for how to conduct\\nthese steps in the library.\\nHow reranking models work\\nOne popular way of building LLM search rerankers is to present the query\\nand each result to an LLM working as a cross-encoder. This means that a\\nquery and possible result are presented to the model at the same time\\nallowing the model to view both these texts before it assigns a relevance\\nscore, as we can see in Figure 8-15. All of the documents are processed\\nsimultaneously as a batch yet each document is evaluated against the query\\nindependently. The scores then determine the new order of the results. This', 'method is described in more detail in a paper titled “Multi-stage document\\nranking with BERT” and is sometimes referred to as monoBERT.\\nFigure 8-15. A reranker assigns a relevance score to each document by looking at the document and\\nthe query at the same time.\\nThis formulation of search as relevance scoring basically boils down to\\nbeing a classification problem. Given those inputs, the model outputs a\\nscore from 0–1 where 0 is irrelevant and 1 is highly relevant. This should be\\nfamiliar from our classification discussions in Chapter 4.\\nTo learn more about the development of using LLMs for search, \"Pretrained\\ntransformers for text tanking: BERT and beyond\" is a highly recommended\\nlook at the developments of these models until about 2021.\\nRetrieval Evaluation Metrics\\nSemantic search is evaluated using metrics from the Information Retrieval\\n(IR) field. Let’s discuss one of these popular metrics: mean average\\nprecision (MAP).\\nEvaluating search systems needs three major components: a text archive, a\\nset of queries, and relevance judgments indicating which documents are\\nrelevant for each query. We see these components in Figure 8-16.', 'Figure 8-16. To evaluate search systems, we need a test suite including queries and relevance\\njudgments indicating which documents in our archive are relevant for each query.\\nUsing this test suite, we can proceed to explore evaluating search systems.\\nLet’s start with a simple example. Let’s assume we pass query 1 to two\\ndifferent search systems. And get two sets of results. Say we limit the\\nnumber of results to three, as we can see in Figure 8-17.', 'Figure 8-17. To compare two search systems, we pass the same query from our test suite to both\\nsystems and look at their top results.\\nTo tell which is a better system, we turn to the relevance judgments that we\\nhave for the query. Figure 8-18 shows which of the returned results are\\nrelevant.', 'Figure 8-18. Looking at the relevance judgments from our test suite, we can see that system 1 did a\\nbetter job than system 2.\\nThis shows us a clear case where system 1 is better than system 2.\\nIntuitively, we may just count how many relevant results each system\\nretrieved. System 1 got two out of three correct, and system 2 got only one\\nout of three correct. But what about a case like Figure 8-19 where both\\nsystems only get one relevant result out of three, but they’re in different\\npositions?', 'Figure 8-19. We need a scoring system that rewards system 1 for assigning a high position to a\\nrelevant result—even though both systems retrieved only one relevant result in their top three results.\\nIn this case, we can intuit that system 1 did a better job than system 2\\nbecause the result in the first position (the most important position) is\\ncorrect. But how can we assign a number or score to how much better that\\nresult is? Mean average precision is a measure that is able to quantify this\\ndistinction.\\nOne common way to assign numeric scores in this scenario is average\\nprecision, which evaluates system 1’s result for the query to be 1 and\\nsystem 2’s to be 0.3. So let’s see how average precision is calculated to\\nevaluate one set of results, and then how it’s aggregated to evaluate a\\nsystem across all the queries in the test suite.\\nScoring a single query with average precision\\nTo score a search system on this query, we can focus on scoring the relevant\\ndocuments. Let’s start by looking at a query that only has one relevant\\ndocument in the test suite.', 'The first one is easy: the search system placed the relevant result (the only\\navailable one for this query) at the top. This gets the system the perfect\\nscore of 1. Figure 8-20 shows this calculation: looking at the first position,\\nwe have a relevant result leading to a precision at position 1 of 1.0\\n(calculated as the number of relevant results at position 1, divided by the\\nposition we’re currently looking at).\\nFigure 8-20. To calculate mean average precision, we start by calculating precision at each position,\\nstarting with position 1.\\nSince we’re only scoring relevant documents we can ignore the scores of\\nnonrelevant documents and stop our calculation here. What if the system\\nactually placed the only relevant result at the third position, however? How\\nwould that affect the score? Figure 8-21 shows how that results in a penalty.', 'Figure 8-21. If the system places nonrelevant documents ahead of a relevant document, its precision\\nscore is penalized.\\nLet’s now look at a query with more than one relevant document. Figure 8-\\n22 shows that calculation and how averaging now comes into the picture.\\nFigure 8-22. Average precision of a document with multiple relevant documents considers the\\nprecision at k results of all the relevant documents.\\nScoring across multiple queries with mean average precision\\nNow that we’re familiar with precision at k and average precision, we can\\nextend this knowledge to a metric that can score a search system against all\\nthe queries in our test suite. That metric is called mean average precision.', 'Figure 8-23 shows how to calculate this metric by taking the mean of the\\naverage precisions of each query.\\nFigure 8-23. The mean average precision takes into consideration the average precision score of a\\nsystem for every query in the test suite. By averaging them, it produces a single metric that we can\\nuse to compare a search system against another.\\nYou may be wondering why the same operation is called “mean” and\\n“average.” It’s likely an aesthetic choice because MAP sounds better than\\naverage average precision.\\nNow we have a single metric that we can use to compare different systems.\\nIf you want to learn more about evaluation metrics, see the “Evaluation in\\nInformation Retrieval” chapter of Introduction to Information Retrieval\\n(Cambridge University Press) by Christopher D. Manning, Prabhakar\\nRaghavan, and Hinrich Schütze.\\nIn addition to mean average precision, another metric commonly used for\\nsearch systems is normalized discounted cumulative gain (nDCG), which is\\nmore nuanced in that the relevance of documents is not binary (relevant\\nversus not relevant) and one document can be labeled as more relevant than\\nanother in the test suite and scoring mechanism.', 'Retrieval-Augmented Generation (RAG)\\nThe mass adoption of LLMs quickly led to people asking them questions\\nand expecting factual answers. While the models can answer some\\nquestions correctly, they also confidently answer lots of questions\\nincorrectly. The leading method the industry turned to remedy this behavior\\nis RAG, described in the paper “Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks” (2020)1  and illustrated in Figure 8-24.\\nFigure 8-24. A basic RAG pipeline is made up of a search step followed by a grounded generation\\nstep where the LLM is prompted with the question and the information retrieved from the search step.\\nRAG systems incorporate search capabilities in addition to generation\\ncapabilities. They can be seen as an improvement to generation systems\\nbecause they reduce their hallucinations and improve their factuality. They\\nalso enable use cases of “chat with my data” that consumers and companies\\ncan use to ground an LLM on internal company data, or a specific data\\nsource of interest (e.g., chatting with a book).\\nThis also extends to search systems. More search engines are incorporating\\nan LLM to summarize results or answer questions submitted to the search\\nengine. Examples include Perplexity, Microsoft Bing AI, and Google\\nGemini.\\nFrom Search to RAG\\nLet’s now turn our search system into a RAG system. We do that by adding\\nan LLM to the end of the search pipeline. We present the question and the\\ntop retrieved documents to the LLM, and ask it to answer the question', 'given the context provided by the search results. We can see an example in\\nFigure 8-25.\\nThis generation step is called grounded generation because the retrieved\\nrelevant information we provide the LLM establishes a certain context that\\ngrounds the LLM in the domain we’re interested in. Figure 8-26 shows how\\ngrounded generation fits after search if we continue our embeddings search\\nexample from earlier.\\nFigure 8-25. Generative search formulates answers and summaries at the end of a search pipeline\\nwhile citing its sources (returned by the previous steps in the search system).', 'Figure 8-26. Find the most relevant information to an input prompt by comparing the similarities\\nbetween embeddings. The most relevant information is added to the prompt before giving it to the\\nLLM.\\nExample: Grounded Generation with an LLM API\\nLet’s look at how to add a grounded generation step after the search results\\nto create our first RAG system. For this example, we’ll use Cohere’s\\nmanaged LLM, which builds on the search systems we’ve seen earlier in\\nthe chapter. We’ll use embedding search to retrieve the top documents, then\\nwe’ll pass those to the co.chat endpoint along with the questions to\\nprovide a grounded answer:\\nquery = \"income generated\"\\n# 1- Retrieval\\n# We\\'ll use embedding search. But ideally we\\'d do hybrid\\nresults = search(query)\\n# 2- Grounded Generation\\ndocs_dict = [{\\'text\\': text} for text in results[\\'texts\\']]\\nresponse = co.chat(\\n    message = query,\\n    documents=docs_dict\\n)\\nprint(response.text)', \"Result:\\nThe film generated a worldwide gross of over $677 million, or \\n$773 million with subsequent re-releases.\\nWe are highlighting some of the text because the model indicated the source\\nfor these spans of text to be the first document we passed in:\\ncitations=[ChatCitation(start=21, end=36, text='worldwide \\ngross', document_ids=['doc_0']), ChatCitation(start=40, end=57, \\ntext='over $677 million', document_ids=['doc_0']), \\nChatCitation(start=62, end=103, text='$773 million with \\nsubsequent re-releases.', document_ids=['doc_0'])]\\ndocuments=[{'id': 'doc_0', 'text': 'The film had a worldwide \\ngross over $677 million (and $773 million with subsequent re-\\nreleases), making it the tenth-highest grossing film of 2014'}]\\nExample: RAG with Local Models\\nLet us now replicate this basic functionality with local models. We will lose\\nthe ability to do span citations and the smaller local model isn’t going to\\nwork as well as the larger managed model, but it’s useful to demonstrate the\\nflow. We’ll start by downloading a quantized model.\\nLoading the generation model\\nWe start by downloading our model:\\n!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-\\ngguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\\nUsing llama.cpp, llama-cpp-python, and LangChain, we load the\\ntext generation model:\\nfrom langchain import LlamaCpp\", '# Make sure the model path is correct for your system!\\nllm = LlamaCpp(\\n    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\\n    n_gpu_layers=-1,\\n    max_tokens=500,\\n    n_ctx=2048,\\n    seed=42,\\n    verbose=False\\n)\\nLoading the embedding model\\nLet’s now load an embedding language model. In this example, we will\\nchoose the BAAI/bge-small-en-v1.5 model. At the time of writing, it is high\\non the MTEB leaderboard for embedding models and relatively small:\\nfrom langchain.embeddings.huggingface import \\nHuggingFaceEmbeddings\\n# Embedding model for converting text to numerical \\nrepresentations\\nembedding_model = HuggingFaceEmbeddings(\\n    model_name=\\'thenlper/gte-small\\'\\n)\\nWe can now use the embedding model to set up our vector database:\\nfrom langchain.vectorstores import FAISS\\n# Create a local vector database\\ndb = FAISS.from_texts(texts, embedding_model)\\nThe RAG prompt\\nA prompt template plays a vital part in the RAG pipeline. It is the central\\nplace where we communicate the relevant documents to the LLM. To do so,\\nwe will create an additional input variable named context that can\\nprovide the LLM with the retrieved documents:', 'from langchain import PromptTemplate\\n# Create a prompt template\\ntemplate = \"\"\"<|user|>\\nRelevant information:\\n{context}\\nProvide a concise answer the following question using the \\nrelevant information provided above:\\n{question}<|end|>\\n<|assistant|>\"\"\"\\nprompt = PromptTemplate(\\n    template=template,\\n    input_variables=[\"context\", \"question\"]\\n)\\nfrom langchain.chains import RetrievalQA\\n# RAG pipeline\\nrag = RetrievalQA.from_chain_type(\\n    llm=llm,\\n    chain_type=\\'stuff\\',\\n    retriever=db.as_retriever(),\\n    chain_type_kwargs={\\n        \"prompt\": prompt\\n    },\\n    verbose=True\\n)\\nNow we’re ready to call the model and ask it a question:\\nrag.invoke(\\'Income generated\\')\\nResult:\\nThe Income generated by the film in 2014 was over $677 million \\nworldwide. This made it the tenth-highest grossing film of that \\nyear. It should be noted, however, this figure includes both \\ninitial ticket sales as well as any subsequent re-releases. \\nWith these additional releases, total earnings surged to \\napproximately $773 million. The release format transitioned ', \"from traditional film stock projection in theaters to digital \\nprojectors once it was expanded to various venues in the United \\nStates. This shift might have contributed to wider audience \\nreach and potentially higher grossing figures over time. \\nHowever, specific data on how this affected total earnings \\nisn't provided in the information above.\\nAs always, we can adjust the prompt to control the model’s generation (e.g.,\\nanswer length and tone).\\nAdvanced RAG Techniques\\nThere are several additional techniques to improve the performance of RAG\\nsystems. Some of them are laid out here.\\nQuery rewriting\\nIf the RAG system is a chatbot, the preceding simple RAG implementation\\nwould likely struggle with the search step if a question is too verbose, or to\\nrefer to context in previous messages in the conversation. This is why it’s a\\ngood idea to use an LLM to rewrite the query into one that aids the retrieval\\nstep in getting the right information. An example of this is a message such\\nas:\\nUser Question: “We have an essay due tomorrow. We have to write about\\nsome animal. I love penguins. I could write about them. But I could also\\nwrite about dolphins. Are they animals? Maybe. Let’s do dolphins. Where\\ndo they live for example?”\\nThis should actually be rewritten into a query like:\\nQuery: “Where do dolphins live”\\nThis rewriting behavior can be done through a prompt (or through an API\\ncall). Cohere’s API, for example, has a dedicated query-rewriting mode for\\nco.chat.\", 'Multi-query RAG\\nThe next improvement we can introduce is to extend the query rewriting to\\nbe able to search multiple queries if more than one is needed to answer a\\nspecific question. Take for example:\\nUser Question: “Compare the financial results of Nvidia in 2020 vs.\\n2023”\\nWe may find one document that contains the results for both years, but\\nmore likely, we’re better off making two search queries:\\nQuery 1: “Nvidia 2020 financial results”\\nQuery 2: “Nvidia 2023 financial results”\\nWe then present the top results of both queries to the model for grounded\\ngeneration. An additional small improvement here is to also give the query\\nrewriter the option to determine if no search is required and if it can directly\\ngenerate a confident answer without searching.\\nMulti-hop RAG\\nA more advanced question may require a series of sequential queries. Take\\nfor example a question like:\\nUser Question: “Who are the largest car manufacturers in 2023? Do\\nthey each make EVs or not?”\\nTo answer this, the system must first search for:\\nStep 1, Query 1: “largest car manufacturers 2023”\\nThen after it gets this information (the result being Toyota, Volkswagen, and\\nHyundai), it should ask follow-up questions:\\nStep 2, Query 1: “Toyota Motor Corporation electric vehicles”\\nStep 2, Query 2: “Volkswagen AG electric vehicles”\\nStep 2, Query 3: “Hyundai Motor Company electric vehicles”', 'Query routing\\nAn additional enhancement is to give the model the ability to search\\nmultiple data sources. We can, for example, specify for the model that if it\\ngets a question about HR, it should search the company’s HR information\\nsystem (e.g., Notion) but if the question is about customer data, that it\\nshould search the customer relationship management (CRM) (e.g.,\\nSalesforce).\\nAgentic RAG\\nYou may be able to now see that the list of previous enhancements slowly\\ndelegates more and more responsibility to the LLM to solve more and more\\ncomplex problems. This relies on the LLM’s capability to gauge the\\nrequired information needs as well as its ability to utilize multiple data\\nsources. This new nature of the LLM starts to become closer and closer to\\nan agent that acts on the world. The data sources can also now be abstracted\\ninto tools. We saw, for example, that we can search Notion, but by the same\\ntoken, we should be able to post to Notion as well.\\nNot all LLMs will have the RAG capabilities mentioned here. At the time\\nof writing, likely only the largest managed models may be able to attempt\\nthis behavior. Thankfully, Cohere’s Command R+ excels at these tasks and\\nis available as an open-weights model as well.\\nRAG Evaluation\\nThere are still ongoing developments in how to evaluate RAG models. A\\ngood paper to read on this topic is “Evaluating verifiability in generative\\nsearch engines” (2023), which runs human evaluations on different\\ngenerative search systems.2 \\nIt evaluates results along four axes:\\nFluency\\nWhether the generated text is fluent and cohesive.', 'Perceived utility\\nWhether the generated answer is helpful and informative.\\nCitation recall\\nThe proportion of generated statements about the external world that are\\nfully supported by their citations.\\nCitation precision\\nThe proportion of generated citations that support their associated\\nstatements.\\nWhile human evaluation is always preferred, there are approaches that\\nattempt to automate these evaluations by having a capable LLM act as a\\njudge (called LLM-as-a-judge) and score the different generations along the\\ndifferent axes. Ragas is a software library that does exactly this. It also\\nscores some additional useful metrics like:\\nFaithfulness\\nWhether the answer is consistent with the provided context\\nAnswer relevance\\nHow relevant the answer is to the question\\nThe Ragas documentation site provides more details about the formulas to\\nactually calculate these metrics.\\nSummary\\nIn this chapter, we looked at different ways of using language models to\\nimprove existing search systems and even be the core of new, more', 'powerful search systems. These include:\\nDense retrieval, which relies on the similarity of text embeddings.\\nThese are systems that embed a search query and retrieve the\\ndocuments with the nearest embeddings to the query’s embedding.\\nRerankers, systems (like monoBERT) that look at a query and\\ncandidate results and score the relevance of each document to that\\nquery. These relevance scores are then used to order the shortlisted\\nresults according to their relevance to the query, often producing an\\nimproved results ranking.\\nRAG, where search systems have a generative LLM at the end of\\nthe pipeline to formulate an answer based on retrieved documents\\nwhile citing sources.\\nWe also looked at one of the possible methods of evaluating search systems.\\nMean average precision allows us to score search systems to be able to\\ncompare across a test suite of queries and their known relevance to the test\\nqueries. Evaluating RAG systems requires multiple axes, however, like\\nfaithfulness, fluency, and others that can be evaluated by humans or by\\nLLM-as-a-judge.\\nIn the next chapter, we will explore how language models can be made\\nmultimodal and reason not just about text but images as well.\\n1  Patrick Lewis et al. “Retrieval-augmented generation for knowledge-intensive NLP tasks.”\\nAdvances in Neural Information Processing Systems 33 (2020): 9459–9474.\\n2  Nelson F. Liu, Tianyi Zhang, and Percy Liang. “Evaluating verifiability in generative search\\nengines.” arXiv preprint arXiv:2304.09848 (2023).\\nOceanofPDF.com', 'Chapter 9. Multimodal Large\\nLanguage Models\\nWhen you think about large language models (LLMs), multimodality might\\nnot be the first thing that comes to mind. After all, they are language\\nmodels! But we can quickly see that models can be much more useful if\\nthey’re able to handle types of data other than text. It’s very useful, for\\nexample, if a language model is able to glance at a picture and answer\\nquestions about it. A model that is able to handle text and images (each of\\nwhich is called a modality) is said to be multimodal, as we can see in\\nFigure 9-1.\\nFigure 9-1. Models that are able to deal with different types (or modalities) of data, such as images,\\naudio, video, or sensors, are said to be multimodal. It’s possible for a model to accept a modality as\\ninput yet not be able to generate in that modality.\\nWe have seen all manner of emerging behaviors rising from LLMs, from\\ngeneralization capabilities and reasoning to arithmetic and linguistics. As\\nmodels grow larger and smarter, so do their skill sets.1 \\nThe ability to receive and reason with multimodal input might further\\nincrease and help emerge capabilities that were previously locked. In', 'practice, language does not solely live in a vacuum. As an example, your\\nbody language, facial expressions, intonation, etc. are all methods of\\ncommunication that enhance the spoken word.\\nThe same thing applies to LLMs; if we can enable them to reason about\\nmultimodal information, their capabilities might increase and we become\\nable to deploy them to solve new kinds of problems.\\nIn this chapter, we will explore a number of different LLMs that have\\nmultimodal capabilities and what that means for practical use cases. We will\\nstart by exploring how images are converted to numerical representations\\nusing an adaptation of the original Transformer technique. Then, we will\\nshow how LLMs can be extended to include vision tasks using this\\nTransformer.\\nTransformers for Vision\\nThroughout the chapters of this book, we have seen the success of using\\nTransformer-based models for a variety of language modeling tasks, from\\nclassification and clustering to search and generative modeling. So it might\\nnot be surprising that researchers have been looking at a way to generalize\\nsome of the Transformer’s success to the field of computer vision.\\nThe method they came up with is called the Vision Transformer (ViT),\\nwhich has been shown to do tremendously well on image recognition tasks\\ncompared to the previously default convolutional neural networks (CNNs).2 \\nLike the original Transformer, ViT is used to transform unstructured data,\\nan image, into representations that can be used for a variety of tasks, like\\nclassification, as illustrated in Figure 9-2.\\nViT relies on an important component of the Transformer architecture,\\nnamely the encoder. As we saw in Chapter 1, the encoder is responsible for\\nconverting textual input into numerical representations before being passed\\nto the decoder. However, before the encoder can perform its duties, the\\ntextual input needs to be tokenized first, as is illustrated in Figure 9-3.', 'Figure 9-2. Both the original Transformer as well as the Vision Transformer take unstructured data,\\nconvert it to numerical representations, and finally use that for tasks like classification.\\nFigure 9-3. Text is passed to one or multiple encoders by first tokenizing it using a tokenizer.\\nSince an image does not consist of words this tokenization process cannot\\nbe used for visual data. Instead, the authors of ViT came up with a method\\nfor tokenizing images into “words,” which allowed them to use the original\\nencoder structure.\\nImagine that you have an image of a cat. This image is represented by a\\nnumber of pixels, let’s say 512 × 512 pixels. Each individual pixel does not', 'convey much information but when you combine patches of pixels, you\\nslowly start to see more information.\\nViT uses a principle much like that. Instead of splitting up text into tokens,\\nit converts the original image into patches of images. In other words, it cuts\\nthe image into a number of pieces horizontally and vertically as illustrated\\nin Figure 9-4.\\nFigure 9-4. The “tokenization” process for image input. It converts an image into patches of\\nsubimages.\\nJust like we are converting text into tokens of text, we are converting an\\nimage into patches of images. The flattened input of image patches can be\\nthought of as the tokens in a piece of text. However, unlike tokens, we\\ncannot just assign each patch with an ID since these patches will rarely be\\nfound in other images, unlike the vocabulary of a text.\\nInstead, the patches are linearly embedded to create numerical\\nrepresentations, namely embeddings. These can then be used as the input of\\na Transformer model. That way, the patches of images are treated the same\\nway as tokens. The full process is illustrated in Figure 9-5.\\nFor illustrative purposes, the images in the examples were patched into 3 ×\\n3 patches but the original implementation used 16 × 16 patches. After all,\\nthe paper is called “An Image is Worth 16x16 Words.”\\nWhat is so interesting about this approach is that the moment the\\nembeddings are passed to the encoder, they are treated as if they were\\ntextual tokens. From that point forward, there is no difference in how a text\\nor image trains.\\nDue to these similarities, the ViT is often used to make all kinds of\\nlanguage models multimodal. One of the most straightforward ways to use', 'it is during the training of embedding models.\\nFigure 9-5. The main algorithm behind ViT. After patching the images and linearly projecting them,\\nthe patch embeddings are passed to the encoder and treated as if they were textual tokens.\\nMultimodal Embedding Models\\nIn previous chapters, we used embedding models to capture the semantic\\ncontent of textual representations, such as papers and documents. We saw\\nthat we could use these embeddings or numerical representations to find\\nsimilar documents, apply classification tasks, and even perform topic\\nmodeling.\\nAs we have seen many times before, embeddings often are an important\\ndriver behind LLM applications. They are an efficient method for capturing', 'large-scale information and searching for the needle in the haystack of\\ninformation.\\nThat said, we have looked at text-only embedding models thus far, which\\nfocus on generating embeddings for textual representations. Although\\nembedding models exist for solely embedding imagery, we will look at\\nembedding models that can capture both textual as well as visual\\nrepresentations. We illustrate this in Figure 9-6.\\nFigure 9-6. Multimodal embedding models can create embeddings for multiple modalities in the\\nsame vector space.\\nAn advantage is that this allows for comparing multimodal representations\\nsince the resulting embeddings lie in the same vector space (Figure 9-7).\\nFor instance, using such a multimodal embedding model, we can find\\nimages based on input text. What images would we find if we search for\\nimages similar to “pictures of a puppy”? Vice versa would also be possible.\\nWhich documents are best related to this question?', 'Figure 9-7. Despite having coming from different modalities, embeddings with similar meaning will\\nbe close to each other in vector space.\\nThere are a number of multimodal embedding models, but the most well-\\nknown and currently most-used model is Contrastive Language-Image Pre-\\ntraining (CLIP).\\nCLIP: Connecting Text and Images\\nCLIP is an embedding model that can compute embeddings of both images\\nand texts. The resulting embeddings lie in the same vector space, which\\nmeans that the embeddings of images can be compared with the\\nembeddings of text.3  This comparison capability makes CLIP, and similar\\nmodels, usable for tasks such as:\\nZero-shot classification\\nWe can compare the embedding of an image with that of the description\\nof its possible classes to find which class is most similar.\\nClustering', 'Cluster both images and a collection of keywords to find which\\nkeywords belong to which sets of images.\\nSearch\\nAcross billions of texts or images, we can quickly find what relates to\\nan input text or image.\\nGeneration\\nUse multimodal embeddings to drive the generation of images (e.g.,\\nstable diffusion4 ).\\nHow Can CLIP Generate Multimodal Embeddings?\\nThe procedure of CLIP is actually quite straightforward. Imagine that you\\nhave a dataset with millions of images alongside captions as we illustrate in\\nFigure 9-8.\\nFigure 9-8. The type of data that is needed to train a multimodal embedding model.\\nThis dataset can be used to create two representations for each pair, the\\nimage and its caption. To do so, CLIP uses a text encoder to embed text and\\nan image encoder to embed images. As is shown in Figure 9-9, the result is\\nan embedding for both the image and its corresponding caption.', 'Figure 9-9. In the first step of training CLIP, both images and text are embedded using an image and\\ntext encoder, respectively.\\nThe pair of embeddings that are generated are compared through cosine\\nsimilarity. As we saw in Chapter 4, cosine similarity is the cosine of the\\nangle between vectors, which is calculated through the dot product of the\\nembeddings and divided by the product of their lengths.\\nWhen we start training, the similarity between the image embedding and\\ntext embedding will be low as they are not yet optimized to be within the\\nsame vector space. During training, we optimize for the similarity between\\nthe embeddings and want to maximize them for similar image/caption pairs\\nand minimize them for dissimilar image/caption pairs (Figure 9-10).\\nAfter calculating their similarity, the model is updated and the process starts\\nagain with new batches of data and updated representations (Figure 9-11).\\nThis method is called contrastive learning, and we will go in depth into its', 'inner workings in Chapter 10 where we will create our own embedding\\nmodel.\\nFigure 9-10. In the second step of training CLIP, the similarity between the sentence and image\\nembedding is calculated using cosine similarity.\\nFigure 9-11. In the third step of training CLIP, the text and image encoders are updated to match\\nwhat the intended similarity should be. This updates the embeddings such that they are closer in\\nvector space if the inputs are similar.\\nEventually, we expect the embedding of an image of a cat would be similar\\nto the embedding of the phrase “a picture of a cat.” As we will see in', 'Chapter 10, to make sure the representations are as accurate as possible,\\nnegative examples of images and captions that are not related should also\\nbe included in the training process. Modeling similarity is not only knowing\\nwhat makes things similar to one another, but also what makes them\\ndifferent and dissimilar.\\nOpenCLIP\\nFor our next example, we are going to be using models from the open\\nsource variant of CLIP, namely OpenCLIP. Using OpenCLIP, or any CLIP\\nmodel, boils down to two things: processing the textual and image inputs\\nbefore passing them to the main model.\\nBefore doing so, let’s take a look at a small example where we will be using\\none of the images we have seen before, namely, an AI-generated image\\n(through stable diffusion) of a puppy playing in the snow, as illustrated in\\nFigure 9-12:\\nfrom urllib.request import urlopen\\nfrom PIL import Image\\n# Load an AI-generated image of a puppy playing in the snow\\npuppy_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-\\nOn-Large-Language-Models/main/chapter09/images/puppy.png\"\\nimage = Image.open(urlopen(puppy_path)).convert(\"RGB\")\\ncaption = \"a puppy playing in the snow\"', 'Figure 9-12. An AI-generated image of a puppy playing in the snow.\\nSince we have a caption for this image, we can use OpenCLIP to generate\\nembeddings for both.\\nTo do so, we load in three models:\\nA tokenizer for tokenizing the textual input\\nA preprocessor to preprocess and resize the image\\nThe main model that converts the previous outputs to embeddings\\nfrom transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\\nmodel_id = \"openai/clip-vit-base-patch32\"', '# Load a tokenizer to preprocess the text\\nclip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\\n# Load a processor to preprocess the images\\nclip_processor = CLIPProcessor.from_pretrained(model_id)\\n# Main model for generating text and image embeddings\\nmodel = CLIPModel.from_pretrained(model_id)\\nAfter having loaded in the models, preprocessing our input is\\nstraightforward. Let’s start with the tokenizer and see what happens if we\\npreprocess our input:\\n# Tokenize our input\\ninputs = clip_tokenizer(caption, return_tensors=\"pt\")\\ninputs\\nThis outputs a dictionary that contains the IDs of the input:\\n{\\'input_ids\\': tensor([[49406, 320, 6829, 1629, 530, 518, 2583, \\n49407]]), \\'attention_mask\\': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\\nTo see what those IDs represent, we can convert them to tokens using the\\naptly named convert_ids_to_tokens function:\\n# Convert our input back to tokens\\nclip_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\\nThis gives us the following output:\\n[\\'<|startoftext|>\\',\\n \\'a</w>\\',\\n \\'puppy</w>\\',\\n \\'playing</w>\\',\\n \\'in</w>\\',\\n \\'the</w>\\',', ' \\'snow</w>\\',\\n \\'<|endoftext|>\\']\\nAs we often have seen before, the text is split up into tokens. Additionally,\\nwe now also see that the start and end of the text is indicated to separate it\\nfrom a potential image embedding. You might also notice that the [CLS]\\ntoken is missing. In CLIP, the [CLS] token is actually used to represent the\\nimage embedding.\\nNow that we have preprocessed our caption, we can create the embedding:\\n# Create a text embedding\\ntext_embedding = model.get_text_features(**inputs)\\ntext_embedding.shape\\nThis results in an embedding that has 512 values for this single string:\\n    torch.Size([1, 512])\\nBefore we can create our image embedding, like the text embedding, we\\nwill need to preprocess it as the model expects the input image to have\\ncertain characteristics, like its size and shape.\\nTo do so, we can use the processor that we created before:\\n# Preprocess image\\nprocessed_image = clip_processor(\\n    text=None, images=image, return_tensors=\"pt\"\\n)[\"pixel_values\"]\\nprocessed_image.shape', 'The original image was 512 × 512 pixels. Notice that the preprocessing of\\nthis image reduced its size to 224 × 224 pixels as that is its expected size:\\n    torch.Size([1, 3, 224, 224])\\nLet’s visualize the results of this preprocessing as shown in Figure 9-13:\\nimport torch\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n# Prepare image for visualization\\nimg = processed_image.squeeze(0)\\nimg = img.permute(*torch.arange(img.ndim - 1, -1, -1))\\nimg = np.einsum(\"ijk->jik\", img)\\n# Visualize preprocessed image\\nplt.imshow(img)\\nplt.axis(\"off\")\\nFigure 9-13. The preprocessed input image by CLIP.', 'To convert this preprocessed image into embeddings, we can call the model\\nas we did before and explore what shape it returns:\\n# Create the image embedding\\nimage_embedding = model.get_image_features(processed_image)\\nimage_embedding.shape\\nThis gives us the following shape:\\n    torch.Size([1, 512])\\nNotice that the shape of the resulting image embedding is the same as that\\nof the text embedding. This is important as it allows us to compare their\\nembeddings and see if they are similar.\\nWe can use these embeddings to calculate how similar they are. To do so,\\nwe normalize the embeddings first before calculating the dot product to\\ngive us a similarity score:\\n# Normalize the embeddings\\ntext_embedding /= text_embedding.norm(dim=-1, keepdim=True)\\nimage_embedding /= image_embedding.norm(dim=-1, keepdim=True)\\n# Calculate their similarity\\ntext_embedding = text_embedding.detach().cpu().numpy()\\nimage_embedding = image_embedding.detach().cpu().numpy()\\nscore = np.dot(text_embedding, image_embedding.T)\\nscore\\nThis gives us the following score:\\narray([[0.33149648]], dtype=float32)\\nWe get a similarity score of 0.33, which is difficult to interpret considering\\nwe don’t know what the model considers a low versus a high similarity', 'score. Instead, let’s extend the example with more images and captions as\\nillustrated in Figure 9-14.\\nFigure 9-14. The similarity matrix between three images and three captions.\\nIt seems that a score of 0.33 is indeed high considering the similarities with\\nother images are quite a bit lower.', 'USING SENTENCE-TRANSFORMERS TO LOAD CLIP\\nsentence-transformers implements a few CLIP-based models\\nthat make it much easier to create embeddings. It only takes a few lines\\nof code:\\nfrom sentence_transformers import SentenceTransformer, util\\n# Load SBERT-compatible CLIP model\\nmodel = SentenceTransformer(\"clip-ViT-B-32\")\\n# Encode the images\\nimage_embeddings = model.encode(images)\\n# Encode the captions\\ntext_embeddings = model.encode(captions)\\n#Compute cosine similarities\\nsim_matrix = util.cos_sim(\\n    image_embeddings, text_embeddings\\n)\\nMaking Text Generation Models Multimodal\\nTraditionally, text generation models have been, as you might expect,\\nmodels that interpret textual representations. Models like Llama 2 and\\nChatGPT excel at reasoning about textual information and responding with\\nnatural language.\\nThey are, however, limited to the modality they were trained in, namely\\ntext. As we have seen before with multimodal embedding models, the\\naddition of vision can enhance the capabilities of a model.\\nIn the case of text generation models, we would like it to reason about\\ncertain input images. For example, we could give it an image of a pizza and\\nask it what ingredients it contains. You could show it a picture of the Eiffel', 'Tower and ask when it was built or where it is located. This conversational\\nability is further illustrated in Figure 9-15.\\nFigure 9-15. An example of a multimodal text generation model (BLIP-2) that can reason about input\\nimages.\\nTo bridge the gap between these two domains, attempts have been made to\\nintroduce a form of multimodality to existing models. One such method is\\ncalled BLIP-2: Bootstrapping Language-Image Pre-training for Unified\\nVision-Language Understanding and Generation 2. BLIP-2 is an easy-to-\\nuse and modular technique that allows for introducing vision capabilities to\\nexisting language models.\\nBLIP-2: Bridging the Modality Gap\\nCreating a multimodal language model from scratch requires significant\\ncomputing power and data. We would have to use billions of images, text,\\nand image-text pairs to create such a model. As you can imagine, this is not\\neasily feasible!', 'Instead of building the architecture from scratch, BLIP-2 bridges the vision-\\nlanguage gap by building a bridge, named the Querying Transformer (Q-\\nFormer), that connects a pretrained image encoder and a pretrained LLM.5 \\nBy leveraging pretrained models, BLIP-2 only needs to train the bridge\\nwithout needing to train the image encoder and LLM from scratch. It makes\\ngreat use of the technology and models that are already out there! This\\nbridge is illustrated in Figure 9-16.\\nFigure 9-16. The Querying Transformer is the bridge between vision (ViT) and text (LLM) that is the\\nonly trainable component of the pipeline.\\nTo connect the two pretrained models, the Q-Former mimics their\\narchitectures. It has two modules that share their attention layers:\\nAn Image Transformer to interact with the frozen Vision\\nTransformer for feature extraction\\nA Text Transformer that can interact with the LLM\\nThe Q-Former is trained in two stages, one for each modality, as illustrated\\nin Figure 9-17.\\nIn step 1, image-document pairs are used to train the Q-Former to represent\\nboth images and text. These pairs are generally captions of images, as we\\nhave seen before when training CLIP.\\nThe images are fed to the frozen ViT to extract vision embeddings. These\\nembeddings are used as the input of Q-Former’s ViT. The captions are used\\nas the input of Q-Former’s Text Transformer.', 'Figure 9-17. In step 1, representation learning is applied to learn representations for vision and\\nlanguage simultaneously. In step 2, these representations are converted to soft visual prompts to feed\\nthe LLM.\\nWith these inputs, the Q-Former is then trained on three tasks:\\nImage-text contrastive learning\\nThis task attempts to align pairs of image and text embeddings such that\\nthey maximize their mutual information.\\nImage-text matching\\nA classification task to predict whether an image and text pair is\\npositive (matched) or negative (unmatched).\\nImage-grounded text generation\\nTrains the model to generate text based on information extracted from\\nthe input image.\\nThese three objectives are jointly optimized to improve the visual\\nrepresentations that are extracted from the frozen ViT. In a way, we are\\ntrying to inject textual information into the embeddings of the frozen ViT so\\nthat we can use them in the LLM. This first step of BLIP-2 is illustrated in\\nFigure 9-18.', 'Figure 9-18. In step 1, the output of the frozen ViT is used together with its caption and trained on\\nthree contrastive-like tasks to learn visual-text representations.\\nIn step 2, the learnable embeddings derived from step 1 now contain visual\\ninformation in the same dimensional space as the corresponding textual\\ninformation. The learnable embeddings are then passed to the LLM. In a\\nway, these embeddings serve as soft visual prompts that condition the LLM\\non the visual representations that were extracted by the Q-Former.\\nThere is also a fully connected linear layer in between them to make sure\\nthat the learnable embeddings have the same shape as the LLM expects.\\nThis second step of converting vision to language is represented in\\nFigure 9-19.\\nFigure 9-19. In step 2, the learned embeddings from the Q-Former are passed to the LLM through a\\nprojection layer. The projected embeddings serve as a soft visual prompt.', 'When we put these steps together, they make it possible for the Q-Former to\\nlearn visual and textual representations in the same dimensional space,\\nwhich can be used as a soft prompt to the LLM. As a result, the LLM will\\nbe given information about the image in a similar manner to the context you\\nwould provide an LLM when prompting. The full in-depth process is\\nillustrated in Figure 9-20.\\nFigure 9-20. The full BLIP-2 procedure.\\nSince BLIP-2, many other visual LLMs have been released that have\\nsimilar processes, like LLaVA, a framework for making textual LLMs\\nmultimodal6  or Idefics 2, an efficient visual LLM based on the Mistral 7B\\nLLM.7  Both visual LLMs, although having different architectures, connect\\npretrained CLIP-like visual encoders with textual LLMs. The goal of these\\narchitectures is to project visual features from the input images to language\\nembeddings such that they can be used as the input for an LLM. Similar to\\nthe Q-Former, they attempt to bridge the gap between images and text.', 'Preprocessing Multimodal Inputs\\nNow that we know how BLIP-2 is created, there are a number of interesting\\nuse cases for such a model, not limited to captioning images, answering\\nvisual questions, and even performing prompting.\\nBefore we go through some use cases, let’s first load the model and explore\\nhow you can use it:\\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\\nimport torch\\n# Load processor and main model\\nblip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-\\nopt-2.7b\")\\nmodel = Blip2ForConditionalGeneration.from_pretrained(\\n    \"Salesforce/blip2-opt-2.7b\",\\n    torch_dtype=torch.float16\\n)\\n# Send the model to GPU to speed up inference\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\nmodel.to(device)\\nTIP\\nUsing model.vision_model and model.language_model, we can see which\\nViT and generative model are used, respectively, in the BLIP-2 model we loaded.\\nWe loaded two components that make up our full pipeline: a processor and\\na model. The processor can be compared to the tokenizer of language\\nmodels. It converts unstructured input, such as images and text, to\\nrepresentations that the model generally expects.\\nPreprocessing images\\nLet’s start by exploring what the processor does to images. We start by\\nloading the picture of a very wide image for illustration purposes:', '# Load image of a supercar\\ncar_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-\\nOn-Large-Language-Models/main/chapter09/images/car.png\"\\nimage = Image.open(urlopen(car_path)).convert(\"RGB\")\\nimage\\nThe image has 520 × 492 pixels, which is generally an unusual format. So\\nlet’s see what our processor does to it:\\n# Preprocess the image\\ninputs = blip_processor(image, return_tensors=\"pt\").to(device, \\ntorch.float16)\\ninputs[\"pixel_values\"].shape', 'This gives us the following shape:\\ntorch.Size([1, 3, 224, 224])\\nThe result is a 224 × 224-sized image. Quite a bit smaller than we initially\\nhad! This also means that all the original different shapes of the image will\\nbe processed into squares. So be careful inputting very wide or tall images\\nas they might get distorted.\\nPreprocessing text\\nLet’s continue this exploration of the processor with text instead. First, we\\ncan access the tokenizer used to tokenize the input text:\\nblip_processor.tokenizer\\nThis gives us the following output:\\nGPT2TokenizerFast(name_or_path=\\'Salesforce/blip2-opt-2.7b\\', \\nvocab_size=50265, \\nmodel_max_length=1000000000000000019884624838656, is_fast=True, \\npadding_side=\\'right\\', truncation_side=\\'right\\', special_tokens=\\n{\\'bos_token\\': \\'</s>\\', \\'eos_token\\': \\'</s>\\', \\'unk_token\\': \\'</s>\\', \\n\\'pad_token\\': \\'<pad>\\'}, clean_up_tokenization_spaces=True), \\nadded_tokens_decoder={\\n1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, \\nsingle_word=False, normalized=True, special=True),\\n2: AddedToken(\"</s>\", rstrip=False, lstrip=False, \\nsingle_word=False, normalized=True, special=True),\\n}\\nThe BLIP-2 model here uses a GPT2Tokenizer. As we explored in\\nChapter 2, how tokenizers deal with input text can differ greatly.\\nTo explore how GPT2Tokenizer works, we can try it out with a small\\nsentence. We start by converting the sentence to token IDs before', 'converting them back to tokens:\\n# Preprocess the text\\ntext = \"Her vocalization was remarkably melodic\"\\ntoken_ids = blip_processor(image, text=text, return_tensors=\"pt\")\\ntoken_ids = token_ids.to(device, torch.float16)[\"input_ids\"][0]\\n# Convert input ids back to tokens\\ntokens = \\nblip_processor.tokenizer.convert_ids_to_tokens(token_ids)\\ntokens\\nThis gives us the following tokens:\\n[\\'</s>\\', \\'Her\\', \\'Ġvocal\\', \\'ization\\', \\'Ġwas\\', \\'Ġremarkably\\', \\n\\'Ġmel\\', \\'odic\\']\\nWhen we inspect the tokens, you might notice a strange symbol at the\\nbeginning of some tokens, namely, the Ġ symbol. This is actually supposed\\nto be a space. However, an internal function takes characters in certain code\\npoints and moves them up by 256 to make them printable. As a result, the\\nspace (code point 32) becomes Ġ (code point 288).\\nWe will convert them to underscores for illustrative purposes:\\n# Replace the space token with an underscore\\ntokens = [token.replace(\"Ġ\", \"_\") for token in tokens]\\ntokens\\nThis gives us a nicer output:\\n[\\'</s>\\', \\'Her\\', \\'_vocal\\', \\'ization\\', \\'_was\\', \\'_remarkably\\', \\n\\'_mel\\', \\'odic\\']', 'The output shows that the underscore indicates the beginning of a word.\\nThat way, words that are made up of multiple tokens can be recognized.\\nUse Case 1: Image Captioning\\nThe most straightforward usage of a model like BLIP-2 is to create captions\\nof images that you have in your data. You might be a store that wants to\\ncreate descriptions of its clothing or perhaps you are a photographer that\\ndoes not have the time to manually label the 1,000+ pictures of a wedding.\\nThe process of captioning an image closely follows the processing. An\\nimage is converted to pixel values that the model can read. These pixel\\nvalues are passed to BLIP-2 to be converted into soft visual prompts that\\nthe LLM can use to decide on a proper caption.\\nLet’s take the image of a supercar and use the processor to derive pixels in\\nthe expected shape:\\n# Load an AI-generated image of a supercar\\nimage = Image.open(urlopen(car_path)).convert(\"RGB\")\\n# Convert an image into inputs and preprocess it\\ninputs = blip_processor(image, return_tensors=\"pt\").to(device, \\ntorch.float16)\\nimage', 'The next step is converting the image into token IDs using the BLIP-2\\nmodel. After doing so, we can convert the IDs into text (the generated\\ncaption):\\n# Generate image ids to be passed to the decoder (LLM)\\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\\n# Generate text from the image ids\\ngenerated_text = blip_processor.batch_decode(\\n    generated_ids, skip_special_tokens=True\\n)\\ngenerated_text = generated_text[0].strip()\\ngenerated_text', 'generated_text contains the caption:\\nan orange supercar driving on the road at sunset\\nThis seems like a perfect description for this image!\\nImage captioning is a great way to get to learn this model before stepping\\ninto more complex use cases. Try it out with a few images yourself and see\\nwhere it performs well and where it performs poorly. Domain-specific\\nimages, like pictures of specific cartoon characters or imaginary creations,\\nmay fail as the model was trained on largely public data.\\nLet’s end this use case with a fun example, namely an image from the\\nRorschach test, which is illustrated in Figure 9-21. It is part of an old\\npsychological experiment that tests the individual’s perception of inkblots.8 \\nWhat someone sees in such an inkblot supposedly tells you something\\nabout a person’s personality characteristics. It is quite a subjective test but\\nthat just makes it more fun!', 'Figure 9-21. An image from the Rorschach test. What do you see in it?\\nLet’s take the image illustrated in Figure 9-21 and use that as our input:\\n# Load Rorschach image\\nurl = \\n\"https://upload.wikimedia.org/wikipedia/commons/7/70/Rorschach_bl\\not_01.jpg\"\\nimage = Image.open(urlopen(url)).convert(\"RGB\")\\n# Generate caption\\ninputs = blip_processor(image, return_tensors=\"pt\").to(device, \\ntorch.float16)\\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\\ngenerated_text = blip_processor.batch_decode(\\n    generated_ids, skip_special_tokens=True\\n)\\ngenerated_text = generated_text[0].strip()\\ngenerated_text', 'As before, when we inspect the generated_text variable, we can take\\na look at the caption:\\na black and white ink drawing of a bat\\nI can definitely see how the model would caption this image using such a\\ndescription. Since this is a Rorschach test, what do you think it says about\\nthe model?\\nUse Case 2: Multimodal Chat-Based Prompting\\nAlthough captioning is an important task, we can extend its use case even\\nfurther. In the previous example, we showed going from one modality,\\nvision (image), to another, text (caption).\\nInstead of following this linear structure, we can try to present both\\nmodalities simultaneously by performing what is called visual question\\nanswering. In this particular use case, we give the model an image along\\nwith a question about that specific image for it to answer. The model needs\\nto process both the image as well as the question at once.\\nTo demonstrate, let’s start with the picture of a car and ask BLIP-2 to\\ndescribe the image. To do so, we first need to preprocess the image as we\\ndid a few times before:\\n# Load an AI-generated image of a supercar\\nimage = Image.open(urlopen(car_path)).convert(\"RGB\")\\nTo perform our visual question answering we need to give BLIP-2 more\\nthan just the image, namely the prompt. Without it, the model would\\ngenerate a caption as it did before. We will ask the model to describe the\\nimage we just processed:\\n# Visual question answering', 'prompt = \"Question: Write down what you see in this picture. \\nAnswer:\"\\n# Process both the image and the prompt\\ninputs = blip_processor(image, text=prompt, \\nreturn_tensors=\"pt\").to(device, torch.float16)\\n# Generate text\\ngenerated_ids = model.generate(**inputs, max_new_tokens=30)\\ngenerated_text = blip_processor.batch_decode(\\n    generated_ids, skip_special_tokens=True\\n)\\ngenerated_text = generated_text[0].strip()\\ngenerated_text\\nThis gives us the following output:\\nA sports car driving on the road at sunset\\nIt correctly describes the image. However, this is a rather simple example\\nsince our question is essentially asking the model to create a caption.\\nInstead, we can ask follow-up questions in a chat-based manner.\\nTo do so, we can give the model our previous conversation, including its\\nanswer to our question. We then ask it a follow-up question:\\n# Chat-like prompting\\nprompt = \"Question: Write down what you see in this picture. \\nAnswer: A sports car driving on the road at sunset. Question: \\nWhat would it cost me to drive that car? Answer:\"\\n# Generate output\\ninputs = blip_processor(image, text=prompt, \\nreturn_tensors=\"pt\").to(device, torch.float16)\\ngenerated_ids = model.generate(**inputs, max_new_tokens=30)\\ngenerated_text = blip_processor.batch_decode(\\n    generated_ids, skip_special_tokens=True\\n)\\ngenerated_text = generated_text[0].strip()\\ngenerated_text', 'This gives us the following answer:\\n$1,000,000\\n$1,000,000 is highly specific! This shows more chat-like behavior from\\nBLIP-2, which allows for some interesting conversations.\\nFinally, we can make this process a bit smoother by creating an interactive\\nchatbot using ipywidgets, an extension for Jupyter notebooks that\\nallows us to make interactive buttons, input text, etc:\\nfrom IPython.display import HTML, display\\nimport ipywidgets as widgets\\ndef text_eventhandler(*args):\\n  question = args[0][\"new\"]\\n  if question:\\n    args[0][\"owner\"].value = \"\"\\n    # Create prompt\\n    if not memory:\\n      prompt = \" Question: \" + question + \" Answer:\"\\n    else:\\n      template = \"Question: {} Answer: {}.\"\\n      prompt = \" \".join(\\n          [\\n              template.format(memory[i][0], memory[i][1]) \\n              for i in range(len(memory))\\n          ]\\n      ) + \" Question: \" + question + \" Answer:\"\\n    # Generate text\\n    inputs = blip_processor(image, text=prompt, \\nreturn_tensors=\"pt\")\\n    inputs = inputs.to(device, torch.float16)\\n    generated_ids = model.generate(**inputs, max_new_tokens=100)\\n    generated_text = blip_processor.batch_decode(\\n        generated_ids, \\n        skip_special_tokens=True\\n    )', '    generated_text = generated_text[0].strip().split(\"Question\")\\n[0]\\n    # Update memory\\n    memory.append((question, generated_text))\\n    # Assign to output\\n    output.append_display_data(HTML(\"<b>USER:</b> \" + question))\\n    output.append_display_data(HTML(\"<b>BLIP-2:</b> \" + \\ngenerated_text))\\n    output.append_display_data(HTML(\"<br>\"))\\n# Prepare widgets\\nin_text = widgets.Text()\\nin_text.continuous_update = False\\nin_text.observe(text_eventhandler, \"value\")\\noutput = widgets.Output()\\nmemory = []\\n# Display chat box\\ndisplay(\\n    widgets.VBox(\\n        children=[output, in_text],\\n        layout=widgets.Layout(display=\"inline-flex\", \\nflex_flow=\"column-reverse\"),\\n    )\\n)', 'It seems that we can continue the conversation and ask a bunch of\\nquestions. Using this chat-based approach, we essentially created a chatbot\\nthat can reason about images!\\nSummary\\nIn this chapter, we explored various methods for making LLMs multimodal\\nby bridging the gap between textual and visual representations. We started\\nby discussing Transformers for vision, which are models that convert\\nimages into numerical representations. This was achieved through the use\\nof image encoders and patch embeddings, which allow the model to process\\nimages at various scales.\\nWe then explored the creation of embedding models that can convert both\\nimages and text to numerical representations using CLIP. We saw how CLIP\\nuses contrastive learning to align image and text embeddings in a shared\\nspace, allowing for tasks like zero-shot classification, clustering, and', 'search. The chapter also introduced OpenCLIP, an open source variant of\\nCLIP that is easy to use for multimodal embedding tasks.\\nFinally, we explored how text generation models could be made multimodal\\nand dived into the BLIP-2 model. The core idea of these multimodal text\\ngeneration models involves projecting visual features from input images to\\ntext embeddings that can be used by LLMs. We saw how this model could\\nbe used for image captioning and multimodal chat-based prompting, where\\nboth modalities are combined to generate responses. Overall, this chapter\\nhighlighted the power of multimodality in LLMs and demonstrated its\\napplications in various areas such as image captioning, search, and chat-\\nbased prompting.\\nIn Part III of the book, we will cover training and fine-tuning techniques. In\\nChapter 10, we will explore how to create and fine-tune a text embedding\\nmodel, which is a core technology that drives many language modeling\\napplications. This next chapter serves as an introduction into both training\\nand fine-tuning language models.\\n1  Jason Wei et al. “Emergent abilities of large language models.” arXiv preprint\\narXiv:2206.07682 (2022).\\n2  Alexey Dosovitskiy et al. “An image is worth 16x16 words: Transformers for image\\nrecognition at scale.” arXiv preprint arXiv:2010.11929 (2020).\\n3  Alec Radford et al. “Learning transferable visual models from natural language supervision.”\\nInternational Conference on Machine Learning. PMLR, 2021.\\n4  Robin Rombach et al. “High-resolution image synthesis with latent diffusion models.”\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\\n5  Junnan Li et al. “BLIP-2: Bootstrapping language-image pretraining with frozen image\\nencoders and large language models.” International Conference on Machine Learning. PMLR,\\n2023.\\n6  Haotian Liu et al. “Visual instruction tuning.” Advances in Neural Information Processing\\nSystems 36 (2024).\\n7  Hugo Laurençon et al. “What matters when building vision-language models?” arXiv preprint\\narXiv:2405.02246 (2024).', '8  Roy Schafer. Psychoanalytic Interpretation in Rorschach Testing: Theory and Application\\n(1954).\\nOceanofPDF.com', 'Part III. Training and Fine-\\nTuning Language Models\\nOceanofPDF.com', 'Chapter 10. Creating Text\\nEmbedding Models\\nText embedding models lie at the foundation of many powerful natural\\nlanguage processing applications. They lay the groundwork for empowering\\nalready impressive technologies such as text generation models. We have\\nalready used embedding models throughout this book in a number of\\napplications, such as supervised classification, unsupervised classification,\\nsemantic search, and even giving memory to text generation models like\\nChatGPT.\\nIt is nearly impossible to overstate the importance of embedding models in\\nthe field as they are the driving power behind so many applications. As\\nsuch, in this chapter, we will discuss a variety of ways that we can create\\nand fine-tune an embedding model to increase its representative and\\nsemantic power.\\nLet’s start by discovering what embedding models are and how they\\ngenerally work.\\nEmbedding Models\\nEmbeddings and embedding models have already been discussed in quite a\\nnumber of chapters (Chapters 4, 5, and 8) thereby demonstrating their\\nusefulness. Before going into training such a model, let’s recap what we\\nhave learned with embedding models.\\nUnstructured textual data by itself is often quite hard to process. They are\\nnot values we can directly process, visualize, and create actionable results\\nfrom. We first have to convert this textual data to something that we can\\neasily process: numeric representations. This process is often referred to as', 'embedding the input to output usable vectors, namely embeddings, as\\nshown in Figure 10-1.\\nFigure 10-1. We use an embedding model to convert textual input, such as documents, sentences, and\\nphrases, to numerical representations, called embeddings.\\nThis process of embedding the input is typically performed by an LLM,\\nwhich we refer to as an embedding model. The main purpose of such a\\nmodel is to be as accurate as possible in representing the textual data as an\\nembedding.\\nHowever, what does it mean to be accurate in representation? Typically, we\\nwant to capture the semantic nature—the meaning—of documents. If we\\ncan capture the core of what the document communicates, we hope to have\\ncaptured what the document is about. In practice, this means that we expect\\nvectors of documents that are similar to one another to be similar, whereas\\nthe embeddings of documents that each discuss something entirely different\\nshould be dissimilar. We’ve seen this idea of semantic similarity several\\ntimes already in this book, and it is visualized in Figure 10-2. This figure is\\na simplified example. While two-dimensional visualization helps illustrate\\nthe proximity and similarity of embeddings, these embeddings typically\\nreside in high-dimensional spaces.', 'Figure 10-2. The idea of semantic similarity is that we expect textual data with similar meanings to\\nbe closer to each other in n-dimensional space (two dimensions are illustrated here).\\nAn embedding model, however, can be trained for a number of purposes.\\nFor example, when we are building a sentiment classifier, we are more\\ninterested in the sentiment of texts than their semantic similarity. As\\nillustrated in Figure 10-3, we can fine-tune the model such that documents\\nare closer in n-dimensional space based on their sentiment rather than their\\nsemantic nature.\\nEither way, an embedding model aims to learn what makes certain\\ndocuments similar to one another and we can guide this process. By\\npresenting the model with enough examples of semantically similar\\ndocuments, we can steer toward semantics whereas using examples of\\nsentiment would steer it in that direction.', 'Figure 10-3. In addition to semantic similarity, an embedding model can be trained to focus on\\nsentiment similarity. In this figure, negative reviews (red) are close to one another and dissimilar to\\npositive reviews (green).\\nThere are many ways in which we can train, fine-tune, and guide\\nembedding models, but one of the strongest and most widely used\\ntechniques is called contrastive learning.\\nWhat Is Contrastive Learning?\\nOne major technique for both training and fine-tuning text embedding\\nmodels is called contrastive learning. Contrastive learning is a technique\\nthat aims to train an embedding model such that similar documents are\\ncloser in vector space while dissimilar documents are further apart. If this\\nsounds familiar, it’s because it’s very similar to the word2vec method from\\nChapter 2. We have seen this notion previously in Figures 10-2 and 10-3.\\nThe underlying idea of contrastive learning is that the best way to learn and\\nmodel similarity/dissimilarity between documents is by feeding a model\\nexamples of similar and dissimilar pairs. In order to accurately capture the\\nsemantic nature of a document, it often needs to be contrasted with another\\ndocument for a model to learn what makes it different or similar. This', 'contrasting procedure is quite powerful and relates to the context in which\\ndocuments are written. This high-level procedure is demonstrated in\\nFigure 10-4.\\nFigure 10-4. Contrastive learning aims to teach an embedding model whether documents are similar\\nor dissimilar. It does so by presenting groups of documents to a model that are similar or dissimilar\\nto a certain degree.\\nAnother way to look at contrastive learning is through the nature of\\nexplanations. A nice example of this is an anecdotal story of a reporter\\nasking a robber “Why did you rob a bank?” to which he answers, “Because\\nthat is where the money is.”1  Although a factually correct answer, the intent\\nof the question was not why he robs banks specifically but why he robs at\\nall. This is called contrastive explanation and refers to understanding a\\nparticular case, “Why P?” in contrast to alternatives, “Why P and not Q?”2 \\nIn the example, the question could be interpreted in a number of ways and\\nmay be best modeled by providing an alternative: “Why did you rob a bank\\n(P) instead of obeying the law (Q)?”\\nThe importance of alternatives to the understanding of a question also\\napplies to how an embedding learns through contrastive learning. By\\nshowing a model similar and dissimilar pairs of documents, it starts to learn\\nwhat makes something similar/dissimilar and more importantly, why.\\nFor example, you could teach a model to understand what a dog is by\\nletting it find features such as “tail,” “nose,” “four legs,” etc. This learning\\nprocess can be quite difficult since features are often not well-defined and\\ncan be interpreted in a number of ways. A being with a “tail,” “nose,” and\\n“four legs” can also be a cat. To help the model steer toward what we are\\ninterested in, we essentially ask it, “Why is this a dog and not a cat?” By\\nproviding the contrast between two concepts, it starts to learn the features', 'that define the concept but also the features that are not related. We get\\nmore information when we frame a question as a contrast. We further\\nillustrate this concept of contrastive explanation in Figure 10-5.\\nFigure 10-5. When we feed an embedding model different contrasts (degrees of similarity), it starts to\\nlearn what makes things different from one another and thereby the distinctive characteristics of\\nconcepts.\\nNOTE\\nOne of the earliest and most popular examples of contrastive learning in NLP is actually\\nword2vec, as we discussed in Chapters 1 and 2. The model learns word representations\\nby training on individual words in a sentence. A word close to a target word in a\\nsentence will be constructed as a positive pair whereas randomly sampled words\\nconstitute dissimilar pairs. In other words, positive examples of neighboring words are\\ncontrasted with randomly selected words that are not neighbors. Although not widely\\nknown, it is one of the first major breakthroughs in NLP that leverages contrastive\\nlearning with neural networks.\\nThere are many ways we can apply contrastive learning to create text\\nembedding models but the most well-known technique and framework is\\nsentence-transformers.\\nSBERT\\nAlthough there are many forms of contrastive learning, one framework that\\nhas popularized the technique within the natural language processing\\ncommunity is sentence-transformers.3  Its approach fixes a major', 'problem with the original BERT implementation for creating sentence\\nembeddings, namely its computational overhead. Before sentence-\\ntransformers, sentence embeddings often used an architectural\\nstructure called cross-encoders with BERT.\\nA cross-encoder allows two sentences to be passed to the Transformer\\nnetwork simultaneously to predict the extent to which the two sentences are\\nsimilar. It does so by adding a classification head to the original architecture\\nthat can output a similarity score. However, the number of computations\\nrises quickly when you want to find the highest pair in a collection of\\n10,000 sentences. That would require n·(n−1)/2 = 49,995,000 inference\\ncomputations and therefore generates significant overhead. Moreover, a\\ncross-encoder generally does not generate embeddings, as shown in\\nFigure 10-6. Instead, it outputs a similarity score between the input\\nsentences.\\nA solution to this overhead is to generate embeddings from a BERT model\\nby averaging its output layer or using the [CLS] token. This, however, has\\nshown to be worse than simply averaging word vectors, like GloVe.4 \\nFigure 10-6. The architecture of a cross-encoder. Both sentences are concatenated, separated with a\\n<SEP> token, and fed to the model simultaneously.', 'Instead, the authors of sentence-transformers approached the\\nproblem differently and searched for a method that is fast and creates\\nembeddings that can be compared semantically. The result is an elegant\\nalternative to the original cross-encoder architecture. Unlike a cross-\\nencoder, in sentence-transformers the classification head is\\ndropped, and instead mean pooling is used on the final output layer to\\ngenerate an embedding. This pooling layer averages the word embeddings\\nand gives back a fixed dimensional output vector. This ensures a fixed-size\\nembedding.\\nThe training for sentence-transformers uses a Siamese\\narchitecture. In this architecture, as visualized in Figure 10-7, we have two\\nidentical BERT models that share the same weights and neural architecture.\\nThese models are fed the sentences from which embeddings are generated\\nthrough the pooling of token embeddings. Then, models are optimized\\nthrough the similarity of the sentence embeddings. Since the weights are\\nidentical for both BERT models, we can use a single model and feed it the\\nsentences one after the other.', 'Figure 10-7. The architecture of the original sentence-transformers model, which leverages\\na Siamese network, also called a bi-encoder.\\nThe optimization process of these pairs of sentences is done through loss\\nfunctions, which can have a major impact on the model’s performance.\\nDuring training, the embeddings for each sentence are concatenated', 'together with the difference between the embeddings. Then, this resulting\\nembedding is optimized through a softmax classifier.\\nThe resulting architecture is also referred to as a bi-encoder or SBERT for\\nsentence-BERT. Although a bi-encoder is quite fast and creates accurate\\nsentence representations, cross-encoders generally achieve better\\nperformance than a bi-encoder but do not generate embeddings.\\nThe bi-encoder, like a cross-encoder, leverages contrastive learning; by\\noptimizing the (dis)similarity between pairs of sentences, the model will\\neventually learn the things that make the sentences what they are.\\nTo perform contrastive learning, we need two things. First, we need data\\nthat constitutes similar/dissimilar pairs. Second, we will need to define how\\nthe model defines and optimizes similarity.\\nCreating an Embedding Model\\nThere are many methods through which an embedding model can be\\ncreated but generally, we look toward contrastive learning. This is an\\nimportant aspect of many embedding models as the process allows it to\\nefficiently learn semantic representations.\\nHowever, this is not a free process. We will need to understand how to\\ngenerate contrastive examples, how to train the model, and how to properly\\nevaluate it.\\nGenerating Contrastive Examples\\nWhen pretraining your embedding model, you will often see data being\\nused from natural language inference (NLI) datasets. NLI refers to the task\\nof investigating whether, for a given premise, it entails the hypothesis\\n(entailment), contradicts it (contradiction), or neither (neutral).\\nFor example, when the premise is “He is in the cinema watching Coco” and\\nthe hypothesis “He is watching Frozen at home,” then these statements are\\ncontradictions. In contrast, when the premise is “He is in the cinema', 'watching Coco” and the hypothesis “In the movie theater he is watching the\\nDisney movie Coco,” then these statements are considered entailment. This\\nprinciple is illustrated in Figure 10-8.\\nFigure 10-8. We can leverage the structure of NLI datasets to generate negative examples\\n(contradiction) and positive examples (entailments) for contrastive learning.\\nIf you look closely at entailment and contradiction, then they describe the\\nextent to which two inputs are similar to one another. As such, we can use\\nNLI datasets to generate negative examples (contradictions) and positive\\nexamples (entailments) for contrastive learning.\\nThe data that we are going to be using throughout creating and fine-tuning\\nembedding models is derived from the General Language Understanding\\nEvaluation benchmark (GLUE). This GLUE benchmark consists of nine\\nlanguage understanding tasks to evaluate and analyze model performance.\\nOne of these tasks is the Multi-Genre Natural Language Inference (MNLI)\\ncorpus, which is a collection of 392,702 sentence pairs annotated with\\nentailment (contradiction, neutral, entailment). We will be using a subset of\\nthe data, 50,000 annotated sentence pairs, to create a minimal example that\\ndoes not need to be trained for hours on end. Do note, though, that the\\nsmaller the dataset, the more unstable training or fine-tuning an embedding\\nmodel is. If possible, larger datasets are preferred assuming it is still quality\\ndata:\\nfrom datasets import load_dataset\\n# Load MNLI dataset from GLUE\\n# 0 = entailment, 1 = neutral, 2 = contradiction\\ntrain_dataset = load_dataset(\\n    \"glue\", \"mnli\", split=\"train\"', ').select(range(50_000))\\ntrain_dataset = train_dataset.remove_columns(\"idx\")\\nNext, we take a look at an example:\\ndataset[2]\\n{\\'premise\\': \\'One of our number will carry out your instructions \\nminutely.\\',\\n \\'hypothesis\\': \\'A member of my team will execute your orders \\nwith immense precision.\\',\\n \\'label\\': 0}\\nThis shows an example of an entailment between the premise and the\\nhypothesis as they are positively related and have near identical meanings.\\nTrain Model\\nNow that we have our dataset with training examples, we will need to\\ncreate our embedding model. We typically choose an existing sentence-\\ntransformers model and fine-tune that model, but in this example, we\\nare going to train an embedding from scratch.\\nThis means that we will have to define two things. First, a pretrained\\nTransformer model that serves as embedding individual words. We will use\\nthe BERT base model (uncased) as it is a great introduction model.\\nHowever, many others exist that also have been evaluated using\\nsentence-transformers. Most notably, microsoft/mpnet-\\nbase often gives good results when used as a word embedding model.\\nfrom sentence_transformers import SentenceTransformer\\n# Use a base model\\nembedding_model = SentenceTransformer(\\'bert-base-uncased\\')', 'NOTE\\nBy default, all layers of an LLM in sentence-transformers are trainable.\\nAlthough it is possible to freeze certain layers, it is generally not advised since the\\nperformance is often better when unfreezing all layers.\\nNext, we will need to define a loss function over which we will optimize\\nthe model. As mentioned at the beginning of this section, one of the first\\ninstances of sentence-transformers uses softmax loss. For\\nillustrative purposes, we are going to be using that for now, but we will go\\ninto more performant losses later on:\\nfrom sentence_transformers import losses\\n# Define the loss function. In softmax loss, we will also need to \\nexplicitly set the number of labels.\\ntrain_loss = losses.SoftmaxLoss(\\n    model=embedding_model,\\n    \\nsentence_embedding_dimension=embedding_model.get_sentence_embeddi\\nng_dimension(),\\n    num_labels=3\\n)\\nBefore we train our model, we define an evaluator to evaluate the model’s\\nperformance during training, which also determines the best model to save.\\nWe can perform evaluation of the performance of our model using the\\nSemantic Textual Similarity Benchmark (STSB). It is a collection of\\nhuman-labeled sentence pairs, with similarity scores between 1 and 5.\\nWe use this dataset to explore how well our model scores on this semantic\\nsimilarity task. Moreover, we process the STSB data to make sure all values\\nare between 0 and 1:\\nfrom sentence_transformers.evaluation import \\nEmbeddingSimilarityEvaluator', '# Create an embedding similarity evaluator for STSB\\nval_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\\nevaluator = EmbeddingSimilarityEvaluator(\\n    sentences1=val_sts[\"sentence1\"],\\n    sentences2=val_sts[\"sentence2\"],\\n    scores=[score/5 for score in val_sts[\"label\"]],\\n    main_similarity=\"cosine\",\\n)\\nNow that we have our evaluator, we create\\nSentenceTransformerTrainingArguments, similar to training\\nwith Hugging Face Transformers (as we will explore in the next chapter):\\nfrom sentence_transformers.training_args import \\nSentenceTransformerTrainingArguments\\n# Define the training arguments\\nargs = SentenceTransformerTrainingArguments(\\n    output_dir=\"base_embedding_model\",\\n    num_train_epochs=1,\\n    per_device_train_batch_size=32,\\n    per_device_eval_batch_size=32,\\n    warmup_steps=100,\\n    fp16=True,\\n    eval_steps=100,\\n    logging_steps=100,\\n)\\nOf note are the following arguments:\\nnum_train_epochs\\nThe number of training rounds. We keep this at 1 for faster training but\\nit is generally advised to increase this value.\\nper_device_train_batch_size\\nThe number of samples to process simultaneously on each device (e.g.,\\nGPU or CPU) during evaluation. Higher values generally means faster\\ntraining.', 'per_device_eval_batch_size\\nThe number of samples to process simultaneously on each device (e.g.,\\nGPU or CPU) during evaluation. Higher values generally means faster\\nevaluation.\\nwarmup_steps\\nThe number of steps during which the learning rate will be linearly\\nincreased from zero to the initial learning rate defined for the training\\nprocess. Note that we did not specify a custom learning rate for this\\ntraining process.\\nfp16\\nBy enabling this parameter we allow for mixed precision training,\\nwhere computations are performed using 16-bit floating-point numbers\\n(FP16) instead of the default 32-bit (FP32). This reduces memory usage\\nand potentially increases the training speed.\\nNow that we have defined our data, embedding model, loss, and evaluator,\\nwe can start training our model. We can do that using\\nSentenceTransformerTrainer:\\nfrom sentence_transformers.trainer import \\nSentenceTransformerTrainer\\n# Train embedding model\\ntrainer = SentenceTransformerTrainer(\\n    model=embedding_model,\\n    args=args,\\n    train_dataset=train_dataset,\\n    loss=train_loss,\\n    evaluator=evaluator\\n)\\ntrainer.train()', \"After training our model, we can use the evaluator to get the performance\\non this single task:\\n# Evaluate our trained model\\nevaluator(embedding_model)\\n{'pearson_cosine': 0.5982288436666162,\\n 'spearman_cosine': 0.6026682018489217,\\n 'pearson_manhattan': 0.6100690915500567,\\n 'spearman_manhattan': 0.617732600131989,\\n 'pearson_euclidean': 0.6079280934202278,\\n 'spearman_euclidean': 0.6158926913905742,\\n 'pearson_dot': 0.38364924527804595,\\n 'spearman_dot': 0.37008497926991796,\\n 'pearson_max': 0.6100690915500567,\\n 'spearman_max': 0.617732600131989}\\nWe get several different distance measures. The one we are interested in\\nmost is 'pearson_cosine', which is the cosine similarity between\\ncentered vectors. It is a value between 0 and 1 where a higher value\\nindicates higher degrees of similarity. We get a value of 0.59, which we\\nconsider a baseline throughout this chapter.\\nTIP\\nLarger batch sizes tend to work better with multiple negative rankings (MNR) loss as a\\nlarger batch makes the task more difficult. The reason for this is that the model needs to\\nfind the best matching sentence from a larger set of potential pairs of sentences. You can\\nadapt the code to try out different batch sizes and get a feeling of its effects.\\nIn-Depth Evaluation\\nA good embedding model is more than just a good score on the STSB\\nbenchmark! As we observed earlier, the GLUE benchmark has a number of\\ntasks for which we can evaluate our embedding model. However, there exist\\nmany more benchmarks that allow for the evaluation of embedding models.\\nTo unify this evaluation procedure, the Massive Text Embedding\", 'Benchmark (MTEB) was developed.5  The MTEB spans 8 embedding tasks\\nthat cover 58 datasets and 112 languages.\\nTo publicly compare state-of-the-art embedding models, a leaderboard was\\ncreated with the scores of each embedding model across all tasks:\\nfrom mteb import MTEB\\n# Choose evaluation task\\nevaluation = MTEB(tasks=[\"Banking77Classification\"])\\n# Calculate results\\nresults = evaluation.run(model)\\n{\\'Banking77Classification\\': {\\'mteb_version\\': \\'1.1.2\\',\\n \\'dataset_revision\\': \\n\\'0fd18e25b25c072e09e0d92ab615fda904d66300\\',\\n \\'mteb_dataset_name\\': \\'Banking77Classification\\',\\n \\'test\\': {\\'accuracy\\': 0.4926298701298701,\\n \\'f1\\': 0.49083335791288685,\\n \\'accuracy_stderr\\': 0.010217785746224237,\\n \\'f1_stderr\\': 0.010265814957074591,\\n \\'main_score\\': 0.4926298701298701,\\n \\'evaluation_time\\': 31.83}}}\\nThis gives us several evaluation metrics for this specific task that we can\\nuse to explore its performance.\\nThe great thing about this evaluation benchmark is not only the diversity of\\nthe tasks and languages but that even the evaluation time is saved. Although\\nmany embedding models exist, we typically want those that are both\\naccurate and have low latency. The tasks for which embedding models are\\nused, like semantic search, often benefit from and require fast inference.\\nSince testing your model on the entire MTEB can take a couple of hours\\ndepending on your GPU, we will use the STSB benchmark throughout this\\nchapter instead for illustration purposes.', 'TIP\\nWhenever you are done training and evaluating your model, it is important to restart the\\nnotebook. This will clear your VRAM up for the next training examples throughout this\\nchapter. By restarting the notebook, we can be sure that all VRAM is cleared.\\nLoss Functions\\nWe trained our model using softmax loss to illustrate how one of the first\\nsentence-transformers models was trained. However, not only is\\nthere a large variety of loss functions to choose from, but softmax loss is\\ngenerally not advised as there are more performant losses.\\nInstead of going through every single loss function out there, there are two\\nloss functions that are typically used and seem to perform generally well,\\nnamely:\\nCosine similarity\\nMultiple negatives ranking (MNR) loss\\nNOTE\\nThere are many more loss functions to choose from than just those discussed here. For\\nexample, a loss like MarginMSE works great for training or fine-tuning a cross-encoder.\\nThere are a number of interesting loss functions implemented in the sentence-\\ntransformers framework.\\nCosine similarity\\nThe cosine similarity loss is an intuitive and easy-to-use loss that works\\nacross many different use cases and datasets. It is typically used in semantic\\ntextual similarity tasks. In these tasks, a similarity score is assigned to the\\npairs of texts over which we optimize the model.\\nInstead of having strictly positive or negative pairs of sentences, we assume\\npairs of sentences that are similar or dissimilar to a certain degree.', 'Typically, this value lies between 0 and 1 to indicate dissimilarity and\\nsimilarity, respectively (Figure 10-9).\\nFigure 10-9. Cosine similarity loss aims to minimize the cosine distance between semantically\\nsimilar sentences and to maximize the distance between semantically dissimilar sentences.\\nCosine similarity loss is straightforward—it calculates the cosine similarity\\nbetween the two embeddings of the two texts and compares that to the\\nlabeled similarity score. The model will learn to recognize the degree of\\nsimilarity between sentences.\\nCosine similarity loss intuitively works best using data where you have\\npairs of sentences and labels that indicate their similarity between 0 and 1.\\nTo use this loss with our NLI dataset, we need to convert the entailment (0),\\nneutral (1), and contradiction (2) labels to values between 0 and 1. The', 'entailment represents a high similarity between the sentences, so we give it\\na similarity score of 1. In contrast, since both neutral and contradiction\\nrepresent dissimilarity, we give these labels a similarity score of 0:\\nfrom datasets import Dataset, load_dataset\\n# Load MNLI dataset from GLUE\\n# 0 = entailment, 1 = neutral, 2 = contradiction\\ntrain_dataset = load_dataset(\\n    \"glue\", \"mnli\", split=\"train\"\\n).select(range(50_000))\\ntrain_dataset = train_dataset.remove_columns(\"idx\")\\n# (neutral/contradiction)=0 and (entailment)=1\\nmapping = {2: 0, 1: 0, 0:1}\\ntrain_dataset = Dataset.from_dict({\\n    \"sentence1\": train_dataset[\"premise\"],\\n    \"sentence2\": train_dataset[\"hypothesis\"],\\n    \"label\": [float(mapping[label]) for label in \\ntrain_dataset[\"label\"]]\\n})\\nAs before, we create our evaluator:\\nfrom sentence_transformers.evaluation import \\nEmbeddingSimilarityEvaluator\\n# Create an embedding similarity evaluator for stsb\\nval_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\\nevaluator = EmbeddingSimilarityEvaluator(\\n    sentences1=val_sts[\"sentence1\"],\\n    sentences2=val_sts[\"sentence2\"],\\n    scores=[score/5 for score in val_sts[\"label\"]],\\n    main_similarity=\"cosine\"\\n)\\nThen, we follow the same steps as before but select a different loss instead:\\nfrom sentence_transformers import losses, SentenceTransformer\\nfrom sentence_transformers.trainer import ', 'SentenceTransformerTrainer\\nfrom sentence_transformers.training_args import \\nSentenceTransformerTrainingArguments\\n# Define model\\nembedding_model = SentenceTransformer(\"bert-base-uncased\")\\n# Loss function\\ntrain_loss = losses.CosineSimilarityLoss(model=embedding_model)\\n# Define the training arguments\\nargs = SentenceTransformerTrainingArguments(\\n    output_dir=\"cosineloss_embedding_model\",\\n    num_train_epochs=1,\\n    per_device_train_batch_size=32,\\n    per_device_eval_batch_size=32,\\n    warmup_steps=100,\\n    fp16=True,\\n    eval_steps=100,\\n    logging_steps=100,\\n)\\n# Train model\\ntrainer = SentenceTransformerTrainer(\\n    model=embedding_model,\\n    args=args,\\n    train_dataset=train_dataset,\\n    loss=train_loss,\\n    evaluator=evaluator\\n)\\ntrainer.train()\\nEvaluating the model after training gives us the following score:\\n# Evaluate our trained model\\nevaluator(embedding_model)\\n{\\'pearson_cosine\\': 0.7222322163831805,\\n \\'spearman_cosine\\': 0.7250508271229599,\\n \\'pearson_manhattan\\': 0.7338163436711481,\\n \\'spearman_manhattan\\': 0.7323479193408869,\\n \\'pearson_euclidean\\': 0.7332716434966307,\\n \\'spearman_euclidean\\': 0.7316999722750905,\\n \\'pearson_dot\\': 0.660366792336156,\\n \\'spearman_dot\\': 0.6624167554844425,', \" 'pearson_max': 0.7338163436711481,\\n 'spearman_max': 0.7323479193408869}\\nA Pearson cosine score of 0.72 is a big improvement compared to the\\nsoftmax loss example, which scored 0.59. This demonstrates the impact the\\nloss function can have on performance.\\nMake sure to restart your notebook so we can explore a more common and\\nperformant loss, namely multiple negatives ranking loss.\\nMultiple negatives ranking loss\\nMultiple negatives ranking (MNR) loss,6  often referred to as InfoNCE7  or\\nNTXentLoss,8  is a loss that uses either positive pairs of sentences or triplets\\nthat contain a pair of positive sentences and an additional unrelated\\nsentence. This unrelated sentence is called a negative and represents the\\ndissimilarity between the positive sentences.\\nFor example, you might have pairs of question/answer, image/image\\ncaption, paper title/paper abstract, etc. The great thing about these pairs is\\nthat we can be confident they are hard positive pairs. In MNR loss\\n(Figure 10-10), negative pairs are constructed by mixing a positive pair\\nwith another positive pair. In the example of a paper title and abstract, you\\nwould generate a negative pair by combining the title of a paper with a\\ncompletely different abstract. These negatives are called in-batch negatives\\nand can also be used to generate the triplets.\", 'Figure 10-10. Multiple negatives ranking loss aims to minimize the distance between related pairs of\\ntext, such as questions and answers, and maximize the distance between unrelated pairs, such as\\nquestions and unrelated answers.\\nAfter having generated these positive and negative pairs, we calculate their\\nembeddings and apply cosine similarity. These similarity scores are then\\nused to answer the question, are these pairs negative or positive? In other\\nwords, it is treated as a classification task and we can use cross-entropy loss\\nto optimize the model.\\nTo make these triplets we start with an anchor sentence (i.e., labeled as the\\n“premise”), which is used to compare other sentences. Then, using the\\nMNLI dataset, we only select sentence pairs that are positive (i.e., labeled\\nas “entailment”). To add negative sentences, we randomly sample sentences\\nas the “hypothesis.”', 'import random\\nfrom tqdm import tqdm\\nfrom datasets import Dataset, load_dataset\\n# # Load MNLI dataset from GLUE\\nmnli = load_dataset(\"glue\", \"mnli\", \\nsplit=\"train\").select(range(50_000))\\nmnli = mnli.remove_columns(\"idx\")\\nmnli = mnli.filter(lambda x: True if x[\"label\"] == 0 else False)\\n# Prepare data and add a soft negative\\ntrain_dataset = {\"anchor\": [], \"positive\": [], \"negative\": []}\\nsoft_negatives = mnli[\"hypothesis\"]\\nrandom.shuffle(soft_negatives)\\nfor row, soft_negative in tqdm(zip(mnli, soft_negatives)):\\n    train_dataset[\"anchor\"].append(row[\"premise\"])\\n    train_dataset[\"positive\"].append(row[\"hypothesis\"])\\n    train_dataset[\"negative\"].append(soft_negative)\\ntrain_dataset = Dataset.from_dict(train_dataset)\\nSince we only selected sentences labeled with “entailment,” the number of\\nrows reduced quite a a bit from 50,000 to 16,875 rows.\\nLet’s define the evaluator:\\nfrom sentence_transformers.evaluation import \\nEmbeddingSimilarityEvaluator\\n# Create an embedding similarity evaluator for stsb\\nval_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\\nevaluator = EmbeddingSimilarityEvaluator(\\n    sentences1=val_sts[\"sentence1\"],\\n    sentences2=val_sts[\"sentence2\"],\\n    scores=[score/5 for score in val_sts[\"label\"]],\\n    main_similarity=\"cosine\"\\n)\\nWe then train as before but with MNR loss instead:\\nfrom sentence_transformers import losses, SentenceTransformer\\nfrom sentence_transformers.trainer import \\nSentenceTransformerTrainer', 'from sentence_transformers.training_args import \\nSentenceTransformerTrainingArguments\\n# Define model\\nembedding_model = SentenceTransformer(\\'bert-base-uncased\\')\\n# Loss function\\ntrain_loss = \\nlosses.MultipleNegativesRankingLoss(model=embedding_model)\\n# Define the training arguments\\nargs = SentenceTransformerTrainingArguments(\\n    output_dir=\"mnrloss_embedding_model\",\\n    num_train_epochs=1,\\n    per_device_train_batch_size=32,\\n    per_device_eval_batch_size=32,\\n    warmup_steps=100,\\n    fp16=True,\\n    eval_steps=100,\\n    logging_steps=100,\\n)\\n# Train model\\ntrainer = SentenceTransformerTrainer(\\n    model=embedding_model,\\n    args=args,\\n    train_dataset=train_dataset,\\n    loss=train_loss,\\n    evaluator=evaluator\\n)\\ntrainer.train()\\nLet’s see how this dataset and loss function compare to our previous\\nexamples:\\n# Evaluate our trained model\\nevaluator(embedding_model)\\n{\\'pearson_cosine\\': 0.8093892326162132,\\n \\'spearman_cosine\\': 0.8121064796503025,\\n \\'pearson_manhattan\\': 0.8215001523827565,\\n \\'spearman_manhattan\\': 0.8172161486524246,\\n \\'pearson_euclidean\\': 0.8210391407846718,\\n \\'spearman_euclidean\\': 0.8166537141010816,\\n \\'pearson_dot\\': 0.7473360302629125,', \" 'spearman_dot': 0.7345184137194012,\\n 'pearson_max': 0.8215001523827565,\\n 'spearman_max': 0.8172161486524246}\\nCompared to our previously trained model with softmax loss (0.72), our\\nmodel with MNR loss (0.80) seems to be much more accurate!\\nTIP\\nLarger batch sizes tend to be better with MNR loss as a larger batch makes the task\\nmore difficult. The reason for this is that the model needs to find the best matching\\nsentence from a larger set of potential pairs of sentences. You can adapt the code to try\\nout different batch sizes and get a feeling of the effects.\\nThere is a downside to how we used this loss function. Since negatives are\\nsampled from other question/answer pairs, these in-batch or “easy”\\nnegatives that we used could potentially be completely unrelated to the\\nquestion. As a result, the embedding model’s task of then finding the right\\nanswer to a question becomes quite easy. Instead, we would like to have\\nnegatives that are very related to the question but not the right answer.\\nThese negatives are called hard negatives. Since this would make the task\\nmore difficult for the embedding model as it has to learn more nuanced\\nrepresentations, the embedding model’s performance generally improves\\nquite a bit.\\nA good example of a hard negative is the following. Let’s assume we have\\nthe following question: “How many people live in Amsterdam?” A related\\nanswer to this question would be: “Almost a million people live in\\nAmsterdam.” To generate a good hard negative, we ideally want the answer\\nto contain something about Amsterdam and the number of people living in\\nthis city. For example: “More than a million people live in Utrecht, which is\\nmore than Amsterdam.” This answer relates to the question but is not the\\nactual answer, so this would be a good hard negative. Figure 10-11\\nillustrates the differences between easy and hard negatives.\", 'Figure 10-11. An easy negative is typically unrelated to both the question and answer. A semi-hard\\nnegative has some similarities to the topic of the question and answer but is somewhat unrelated. A\\nhard negative is very similar to the question but is generally the wrong answer.\\nGathering negatives can roughly be divided into the following three\\nprocesses:\\nEasy negatives\\nThrough randomly sampling documents as we did before.\\nSemi-hard negatives\\nUsing a pretrained embedding model, we can apply cosine similarity on\\nall sentence embeddings to find those that are highly related. Generally,\\nthis does not lead to hard negatives since this method merely finds\\nsimilar sentences, not question/answer pairs.\\nHard negatives\\nThese often need to be either manually labeled (for instance, by\\ngenerating semi-hard negatives) or you can use a generative model to\\neither judge or generate sentence pairs.', \"Make sure to restart your notebook so we can explore the different methods\\nof fine-tuning embedding models.\\nFine-Tuning an Embedding Model\\nIn the previous section, we went through the basics of training an\\nembedding model from scratch and saw how we could leverage loss\\nfunctions to further optimize its performance. This approach, although quite\\npowerful, requires creating an embedding model from scratch. This process\\ncan be quite costly and time-consuming.\\nInstead, the sentence-transformers framework allows nearly all\\nembedding models to be used as a base for fine-tuning. We can choose an\\nembedding model that was already trained on a large amount of data and\\nfine-tune it for our specific data or purpose.\\nThere are several ways to fine-tune your model, depending on the data\\navailability and domain. We will go through two such methods and\\ndemonstrate the strength of leveraging pretrained embedding models.\\nSupervised\\nThe most straightforward way to fine-tune an embedding model is to repeat\\nthe process of training our model as we did before but replace the 'bert-\\nbase-uncased' with a pretrained sentence-transformers\\nmodel. There are many to choose from but generally, all-MiniLM-L6-\\nv2 performs well across many use cases and due to its small size is quite\\nfast.\\nWe use the same data as we used to train our model in the MNR loss\\nexample but instead use a pretrained embedding model to fine-tune. As\\nalways, let’s start by loading the data and creating the evaluator:\\nfrom datasets import load_dataset\\nfrom sentence_transformers.evaluation import \\nEmbeddingSimilarityEvaluator\", '# Load MNLI dataset from GLUE\\n# 0 = entailment, 1 = neutral, 2 = contradiction\\ntrain_dataset = load_dataset(\\n    \"glue\", \"mnli\", split=\"train\"\\n).select(range(50_000))\\ntrain_dataset = train_dataset.remove_columns(\"idx\")\\n# Create an embedding similarity evaluator for stsb\\nval_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\\nevaluator = EmbeddingSimilarityEvaluator(\\n    sentences1=val_sts[\"sentence1\"],\\n    sentences2=val_sts[\"sentence2\"],\\n    scores=[score/5 for score in val_sts[\"label\"]],\\n    main_similarity=\"cosine\"\\n)\\nThe training steps are similar to our previous examples but instead of using\\n\\'bert-base-uncased\\', we can use a pretrained embedding model\\ninstead:\\nfrom sentence_transformers import losses, SentenceTransformer\\nfrom sentence_transformers.trainer import \\nSentenceTransformerTrainer\\nfrom sentence_transformers.training_args import \\nSentenceTransformerTrainingArguments\\n# Define model\\nembedding_model = SentenceTransformer(\\'sentence-transformers/all-\\nMiniLM-L6-v2\\')\\n# Loss function\\ntrain_loss = \\nlosses.MultipleNegativesRankingLoss(model=embedding_model)\\n# Define the training arguments\\nargs = SentenceTransformerTrainingArguments(\\n    output_dir=\"finetuned_embedding_model\",\\n    num_train_epochs=1,\\n    per_device_train_batch_size=32,\\n    per_device_eval_batch_size=32,\\n    warmup_steps=100,\\n    fp16=True,\\n    eval_steps=100,', \"    logging_steps=100,\\n)\\n# Train model\\ntrainer = SentenceTransformerTrainer(\\n    model=embedding_model,\\n    args=args,\\n    train_dataset=train_dataset,\\n    loss=train_loss,\\n    evaluator=evaluator\\n)\\ntrainer.train()\\nEvaluating this model gives us the following score:\\n# Evaluate our trained model\\nevaluator(embedding_model)\\n{'pearson_cosine': 0.8509553350510896,\\n 'spearman_cosine': 0.8484676559567688,\\n 'pearson_manhattan': 0.8503896832470704,\\n 'spearman_manhattan': 0.8475760325664419,\\n 'pearson_euclidean': 0.8513115442079158,\\n 'spearman_euclidean': 0.8484676559567688,\\n 'pearson_dot': 0.8489553386816947,\\n 'spearman_dot': 0.8484676559567688,\\n 'pearson_max': 0.8513115442079158,\\n 'spearman_max': 0.8484676559567688}\\nAlthough a score of 0.85 is the highest we have seen thus far, the pretrained\\nmodel that we used for fine-tuning was already trained on the full MNLI\\ndataset, whereas we only used 50,000 examples. It might seem redundant\\nbut this example demonstrates how to fine-tune a pretrained embedding\\nmodel on your own data.\", \"TIP\\nInstead of using a pretrained BERT model like 'bert-base-uncased' or a\\npossible out-of-domain model like 'all-mpnet-base-v2', you can also perform\\nmasked language modeling on the pretrained BERT model to first adapt it to your\\ndomain. Then, you can use this fine-tuned BERT model as the base for training your\\nembedding model. This is a form of domain adaptation. In the next chapter, we will\\napply masked language modeling on a pretrained model.\\nNote that the main difficulty of training or fine-tuning your model is finding\\nthe right data. With these models, we not only want to have very large\\ndatasets, but the data in itself needs to be of high quality. Developing\\npositive pairs is generally straightforward but adding hard negative pairs\\nsignificantly increases the difficulty of creating quality data.\\nAs always, restart your notebook to free up VRAM for the following\\nexamples.\\nAugmented SBERT\\nA disadvantage of training or fine-tuning these embedding models is that\\nthey often require substantial training data. Many of these models are\\ntrained with more than a billion sentence pairs. Extracting such a high\\nnumber of sentence pairs for your use case is generally not possible as in\\nmany cases, there are only a couple of thousand labeled data points\\navailable.\\nFortunately, there is a way to augment your data such that an embedding\\nmodel can be fine-tuned when there is only a little labeled data available.\\nThis procedure is referred to as Augmented SBERT.9 \\nIn this procedure, we aim to augment the small amount of labeled data such\\nthat they can be used for regular training. It makes use of the slow and more\\naccurate cross-encoder architecture (BERT) to augment and label a larger\\nset of input pairs. These newly labeled pairs are then used for fine-tuning a\\nbi-encoder (SBERT).\", 'As shown in Figure 10-12, Augmented SBERT involves the following\\nsteps:\\n1. Fine-tune a cross-encoder (BERT) using a small, annotated dataset\\n(gold dataset).\\n2. Create new sentence pairs.\\n3. Label new sentence pairs with the fine-tuned cross-encoder (silver\\ndataset).\\n4. Train a bi-encoder (SBERT) on the extended dataset (gold + silver\\ndataset).\\nHere, a gold dataset is a small but fully annotated dataset that holds the\\nground truth. A silver dataset is also fully annotated but is not necessarily\\nthe ground truth as it was generated through predictions of the cross-\\nencoder.\\nFigure 10-12. Augmented SBERT works through training a cross-encoder on a small gold dataset,\\nthen using that to label an unlabeled dataset to generate a larger silver dataset. Finally, both the\\ngold and silver datasets are used to train the bi-encoder.\\nBefore we get into the preceding steps, let’s first prepare the data. Instead of\\nour original 50,000 documents, we take a subset of 10,000 documents to\\nsimulate a setting where we have limited annotated data. As we did in our\\nexample with cosine similarity loss, give entailment a score of 1 whereas\\nneutral and contradiction get a score of 0:\\nimport pandas as pd', 'from tqdm import tqdm\\nfrom datasets import load_dataset, Dataset\\nfrom sentence_transformers import InputExample\\nfrom sentence_transformers.datasets import NoDuplicatesDataLoader\\n# Prepare a small set of 10000 documents for the cross-encoder\\ndataset = load_dataset(\"glue\", \"mnli\", \\nsplit=\"train\").select(range(10_000))\\nmapping = {2: 0, 1: 0, 0:1}\\n# Data loader\\ngold_examples = [\\n    InputExample(texts=[row[\"premise\"], row[\"hypothesis\"]], \\nlabel=mapping[row[\"label\"]])\\n    for row in tqdm(dataset)\\n]\\ngold_dataloader = NoDuplicatesDataLoader(gold_examples, \\nbatch_size=32)\\n# Pandas DataFrame for easier data handling\\ngold = pd.DataFrame(\\n    {\\n    \"sentence1\": dataset[\"premise\"],\\n    \"sentence2\": dataset[\"hypothesis\"],\\n    \"label\": [mapping[label] for label in dataset[\"label\"]]\\n    }\\n)\\nThis is the gold dataset since it is labeled and represents our ground truth.\\nUsing this gold dataset, we train our cross-encoder (step 1):\\nfrom sentence_transformers.cross_encoder import CrossEncoder\\n# Train a cross-encoder on the gold dataset\\ncross_encoder = CrossEncoder(\"bert-base-uncased\", num_labels=2)\\ncross_encoder.fit(\\n    train_dataloader=gold_dataloader,\\n    epochs=1,\\n    show_progress_bar=True,\\n    warmup_steps=100,\\n    use_amp=False\\n)', 'After training our cross-encoder, we use the remaining 400,000 sentence\\npairs (from our original dataset of 50,000 sentence pairs) as our silver\\ndataset (step 2):\\n# Prepare the silver dataset by predicting labels with the cross-\\nencoder\\nsilver = load_dataset(\\n    \"glue\", \"mnli\", split=\"train\"\\n).select(range(10_000, 50_000))\\npairs = list(zip(silver[\"premise\"], silver[\"hypothesis\"]))\\nTIP\\nIf you do not have any additional unlabeled sentence pairs, you can randomly sample\\nthem from your original gold dataset. To illustrate, you can create a new sentence pair\\nby taking the premise from one row and the hypothesis from another. This allows you to\\neasily generate 10 times as many sentence pairs that can be labeled with the cross-\\nencoder.\\nThis strategy, however, likely generates significantly more dissimilar than similar pairs.\\nInstead, we can use a pretrained embedding model to embed all candidate sentence pairs\\nand retrieve the top-k sentences for each input sentence using semantic search. This\\nrough reranking process allows us to focus on sentence pairs that are likely to be more\\nsimilar. Although the sentences are still chosen based on an approximation since the\\npretrained embedding model was not trained on our data, it is much better than random\\nsampling.\\nNote that we assume that these sentence pairs are unlabeled in this example.\\nWe will use our fine-tuned cross-encoder to label these sentence pairs (step\\n3):\\nimport numpy as np\\n# Label the sentence pairs using our fine-tuned cross-encoder\\noutput = cross_encoder.predict(\\n    pairs, apply_softmax=True, \\nshow_progress_bar=True\\n)\\nsilver = pd.DataFrame(', '    {\\n        \"sentence1\": silver[\"premise\"], \\n        \"sentence2\": silver[\"hypothesis\"],\\n        \"label\": np.argmax(output, axis=1)\\n    }\\n)\\nNow that we have a silver and gold dataset, we simply combine them and\\ntrain our embedding model as we did before:\\n# Combine gold + silver\\ndata = pd.concat([gold, silver], ignore_index=True, axis=0)\\ndata = data.drop_duplicates(subset=[\"sentence1\", \"sentence2\"], \\nkeep=\"first\")\\ntrain_dataset = Dataset.from_pandas(data, preserve_index=False)\\nAs always, we need to define our evaluator:\\nfrom sentence_transformers.evaluation import \\nEmbeddingSimilarityEvaluator\\n# Create an embedding similarity evaluator for stsb\\nval_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\\nevaluator = EmbeddingSimilarityEvaluator(\\n    sentences1=val_sts[\"sentence1\"],\\n    sentences2=val_sts[\"sentence2\"],\\n    scores=[score/5 for score in val_sts[\"label\"]],\\n    main_similarity=\"cosine\"\\n)\\nWe train the model the same as before except now we use the augmented\\ndataset:\\nfrom sentence_transformers import losses, SentenceTransformer\\nfrom sentence_transformers.trainer import \\nSentenceTransformerTrainer\\nfrom sentence_transformers.training_args import \\nSentenceTransformerTrainingArguments\\n# Define model', 'embedding_model = SentenceTransformer(\"bert-base-uncased\")\\n# Loss function\\ntrain_loss = losses.CosineSimilarityLoss(model=embedding_model)\\n# Define the training arguments\\nargs = SentenceTransformerTrainingArguments(\\n    output_dir=\"augmented_embedding_model\",\\n    num_train_epochs=1,\\n    per_device_train_batch_size=32,\\n    per_device_eval_batch_size=32,\\n    warmup_steps=100,\\n    fp16=True,\\n    eval_steps=100,\\n    logging_steps=100,\\n)\\n# Train model\\ntrainer = SentenceTransformerTrainer(\\n    model=embedding_model,\\n    args=args,\\n    train_dataset=train_dataset,\\n    loss=train_loss,\\n    evaluator=evaluator\\n)\\ntrainer.train()\\nFinally, we evaluate the model:\\nevaluator(embedding_model)\\n{\\'pearson_cosine\\': 0.7101597020018693,\\n \\'spearman_cosine\\': 0.7210536464320728,\\n \\'pearson_manhattan\\': 0.7296749443525249,\\n \\'spearman_manhattan\\': 0.7284184255293913,\\n \\'pearson_euclidean\\': 0.7293097297208753,\\n \\'spearman_euclidean\\': 0.7282830906742256,\\n \\'pearson_dot\\': 0.6746605824703588,\\n \\'spearman_dot\\': 0.6754486790570754,\\n \\'pearson_max\\': 0.7296749443525249,\\n \\'spearman_max\\': 0.7284184255293913}', 'The original cosine similarity loss example had a score of 0.72 with the full\\ndataset. Using only 20% of that data, we managed to get a score of 0.71!\\nThis method allows us to increase the size of datasets that you already have\\navailable without the need to manually label hundreds of thousands of\\nsentence pairs. You can test the quality of your silver data by also training\\nyour embedding model only on the gold dataset. The difference in\\nperformance indicates how much your silver dataset potentially adds to the\\nquality of the model.\\nYou can restart your notebook a final time for the last example, namely\\nunsupervised learning.\\nUnsupervised Learning\\nTo create an embedding model, we typically need labeled data. However,\\nnot all real-world datasets come with a nice set of labels that we can use.\\nWe instead look for techniques to train the model without any\\npredetermined labels—unsupervised learning. Many approaches exist, like\\nSimple Contrastive Learning of Sentence Embeddings (SimCSE),10 \\nContrastive Tension (CT),11  Transformer-based Sequential Denoising Auto-\\nEncoder (TSDAE),12  and Generative Pseudo-Labeling (GPL).13 \\nIn this section, we will focus on TSDAE, as it has shown great performance\\non unsupervised tasks as well as domain adaptation.\\nTransformer-Based Sequential Denoising Auto-Encoder\\nTSDAE is a very elegant approach to creating an embedding model with\\nunsupervised learning. The method assumes that we have no labeled data at\\nall and does not require us to artificially create labels.\\nThe underlying idea of TSDAE is that we add noise to the input sentence by\\nremoving a certain percentage of words from it. This “damaged” sentence is\\nput through an encoder, with a pooling layer on top of it, to map it to a\\nsentence embedding. From this sentence embedding, a decoder tries to', 'reconstruct the original sentence from the “damaged” sentence but without\\nthe artificial noise. The main concept here is that the more accurate the\\nsentence embedding is, the more accurate the reconstructed sentence will\\nbe.\\nThis method is very similar to masked language modeling, where we try to\\nreconstruct and learn certain masked words. Here, instead of reconstructing\\nmasked words, we try to reconstruct the entire sentence.\\nAfter training, we can use the encoder to generate embeddings from text\\nsince the decoder is only used for judging whether the embeddings can\\naccurately reconstruct the original sentence (Figure 10-13).', 'Figure 10-13. TSDAE randomly removes words from an input sentence that is passed through an\\nencoder to generate a sentence embedding. From this sentence embedding, the original sentence is\\nreconstructed.', 'Since we only need a bunch of sentences without any labels, training this\\nmodel is straightforward. We start by downloading an external tokenizer,\\nwhich is used for the denoising procedure:\\n# Download additional tokenizer\\nimport nltk\\nnltk.download(\"punkt\")\\nThen, we create flat sentences from our data and remove any labels that we\\nhave to mimic an unsupervised setting:\\nfrom tqdm import tqdm\\nfrom datasets import Dataset, load_dataset\\nfrom sentence_transformers.datasets import \\nDenoisingAutoEncoderDataset\\n# Create a flat list of sentences\\nmnli = load_dataset(\"glue\", \"mnli\", \\nsplit=\"train\").select(range(25_000))\\nflat_sentences = mnli[\"premise\"] + mnli[\"hypothesis\"]\\n# Add noise to our input data\\ndamaged_data = \\nDenoisingAutoEncoderDataset(list(set(flat_sentences)))\\n# Create dataset\\ntrain_dataset = {\"damaged_sentence\": [], \"original_sentence\": []}\\nfor data in tqdm(damaged_data):\\n    train_dataset[\"damaged_sentence\"].append(data.texts[0])\\n    train_dataset[\"original_sentence\"].append(data.texts[1])\\ntrain_dataset = Dataset.from_dict(train_dataset)\\nThis creates a dataset of 50,000 sentences. When we inspect the data, notice\\nthat the first sentence is the damaged sentence and the second sentence the\\noriginal:\\ntrain_dataset[0]\\n{\\'damaged_sentence\\': \\'Grim jaws are.\\',', ' \\'original_sentence\\': \\'Grim faces and hardened jaws are not \\npeople-friendly.\\'}\\nThe first sentence shows the “noisy” data whereas the second shows the\\noriginal input sentence. After creating our data, we define our evaluator as\\nbefore:\\nfrom sentence_transformers.evaluation import \\nEmbeddingSimilarityEvaluator\\n# Create an embedding similarity evaluator for stsb\\nval_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\\nevaluator = EmbeddingSimilarityEvaluator(\\n    sentences1=val_sts[\"sentence1\"],\\n    sentences2=val_sts[\"sentence2\"],\\n    scores=[score/5 for score in val_sts[\"label\"]],\\n    main_similarity=\"cosine\"\\n)\\nNext, we run the training as before but with the [CLS] token as the pooling\\nstrategy instead of the mean pooling of the token embeddings. In the\\nTSDAE paper, this was shown to be more effective since mean pooling\\nloses the position information, which is not the case when using the [CLS]\\ntoken:\\nfrom sentence_transformers import models, SentenceTransformer\\n# Create your embedding model\\nword_embedding_model = models.Transformer(\"bert-base-uncased\")\\npooling_model = \\nmodels.Pooling(word_embedding_model.get_word_embedding_dimension(\\n), \"cls\")\\nembedding_model = SentenceTransformer(modules=\\n[word_embedding_model, pooling_model])\\nUsing our sentence pairs, we will need a loss function that attempts to\\nreconstruct the original sentence using the noise sentence, namely\\nDenoisingAutoEncoderLoss. By doing so, it will learn how to', 'accurately represent the data. It is similar to masking but without knowing\\nwhere the actual masks are.\\nMoreover, we tie the parameters of both models. Instead of having separate\\nweights for the encoder’s embedding layer and the decoder’s output layer,\\nthey share the same weights. This means that any updates to the weights in\\none layer will be reflected in the other layer as well:\\nfrom sentence_transformers import losses\\n# Use the denoising auto-encoder loss\\ntrain_loss = losses.DenoisingAutoEncoderLoss(\\n    embedding_model, tie_encoder_decoder=True\\n)\\ntrain_loss.decoder = train_loss.decoder.to(\"cuda\")\\nFinally, training our model works the same as we have seen several times\\nbefore but we lower the batch size as memory increases with this loss\\nfunction:\\nfrom sentence_transformers.trainer import \\nSentenceTransformerTrainer\\nfrom sentence_transformers.training_args import \\nSentenceTransformerTrainingArguments\\n# Define the training arguments\\nargs = SentenceTransformerTrainingArguments(\\n    output_dir=\"tsdae_embedding_model\",\\n    num_train_epochs=1,\\n    per_device_train_batch_size=16,\\n    per_device_eval_batch_size=16,\\n    warmup_steps=100,\\n    fp16=True,\\n    eval_steps=100,\\n    logging_steps=100,\\n)\\n# Train model\\ntrainer = SentenceTransformerTrainer(\\n    model=embedding_model,\\n    args=args,', \"    train_dataset=train_dataset,\\n    loss=train_loss,\\n    evaluator=evaluator\\n)\\ntrainer.train()\\nAfter training, we evaluate our model to explore how well such an\\nunsupervised technique performs:\\n# Evaluate our trained model\\nevaluator(embedding_model)\\n{'pearson_cosine': 0.6991809700971775,\\n 'spearman_cosine': 0.713693213167873,\\n 'pearson_manhattan': 0.7152343356643568,\\n 'spearman_manhattan': 0.7201441944880915,\\n 'pearson_euclidean': 0.7151142243297436,\\n 'spearman_euclidean': 0.7202291660769805,\\n 'pearson_dot': 0.5198066451871277,\\n 'spearman_dot': 0.5104025515225046,\\n 'pearson_max': 0.7152343356643568,\\n 'spearman_max': 0.7202291660769805}\\nAfter fitting our model, we got a score of 0.70, which is quite impressive\\nconsidering we did all this training with unlabeled data.\\nUsing TSDAE for Domain Adaptation\\nWhen you have very little or no labeled data available, you typically use\\nunsupervised learning to create your text embedding model. However,\\nunsupervised techniques are generally outperformed by supervised\\ntechniques and have difficulty learning domain-specific concepts.\\nThis is where domain adaptation comes in. Its goal is to update existing\\nembedding models to a specific textual domain that contains different\\nsubjects from the source domain. Figure 10-14 demonstrates how domains\\ncan differ in content. The target domain, or out-domain, generally contains\\nwords and subjects that were not found in the source domain or in-domain.\", 'Figure 10-14. In domain adaptation, the aim is to create and generalize an embedding model from\\none domain to another.\\nOne method for domain adaptation is called adaptive pretraining. You start\\nby pretraining your domain-specific corpus using an unsupervised\\ntechnique, such as the previously discussed TSDAE or masked language\\nmodeling. Then, as illustrated in Figure 10-15, you fine-tune that model\\nusing a training dataset that can be either outside or in your target domain.\\nAlthough data from the target domain is preferred, out-domain data also\\nworks since we started with unsupervised training on the target domain.', 'Figure 10-15. Domain adaptation can be performed with adaptive pretraining and adaptive fine-\\ntuning.\\nUsing everything you have learned in this chapter, you should be able to\\nreproduce this pipeline! First, you can start with TSDAE to train an\\nembedding model on your target domain and then fine-tune it using either\\ngeneral supervised training or Augmented SBERT.\\nSummary\\nIn this chapter, we looked at creating and fine-tuning embedding models\\nthrough various tasks. We discussed the concept of embeddings and their\\nrole in representing textual data in a numerical format. We then explored the\\nfoundational technique of many embedding models, namely contrastive\\nlearning, which learns primarily from (dis)similar pairs of documents.\\nUsing a popular embedding framework, sentence-transformers, we\\nthen created embedding models using a pretrained BERT model while\\nexploring different loss functions, such as cosine similarity loss and MNR\\nloss. We discussed how the collection of (dis)similar pairs or triples of\\ndocuments is vital to the performance of the resulting model.\\nIn the sections that followed, we explored techniques for fine-tuning\\nembedding models. Both supervised and unsupervised techniques were', 'discussed such as Augmented SBERT and TSDAE for domain adaptation.\\nCompared to creating an embedding model, fine-tuning generally needs less\\ndata and is a great way to adapt existing embedding models to your domain.\\nIn the next chapter, methods for fine-tuning representations for\\nclassification will be discussed. Both BERT models and embedding models\\nwill make an appearance as well as a wide range of fine-tuning techniques.\\n1  Alan Garfinkel. Forms of Explanation: Rethinking the Questions in Social Theory. Yale\\nUniversity Press (1982).\\n2  Tim Miller. “Contrastive explanation: A structural-model approach.” The Knowledge\\nEngineering Review 36 (2021): e14.\\n3  Nils Reimers and Iryna Gurevych. “Sentence-BERT: Sentence embeddings using Siamese\\nBERT-networks.” arXiv preprint arXiv:1908.10084 (2019).\\n4  Jeffrey Pennington, Richard Socher, and Christopher D. Manning. “GloVe: Global vectors for\\nword representation.” Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP). 2014.\\n5  Niklas Muennighoff et al. “MTEB: Massive Text Embedding Benchmark.” arXiv preprint\\narXiv:2210.07316 (2022).\\n6  Matthew Henderson et al. “Efficient natural language response suggestion for smart reply.”\\narXiv preprint arXiv:1705.00652 (2017).\\n7  Aaron van den Oord, Yazhe Li, and Oriol Vinyals. “Representation learning with contrastive\\npredictive coding.” arXiv preprint arXiv:1807.03748 (2018).\\n8  Ting Chen et al. “A simple framework for contrastive learning of visual representations.”\\nInternational Conference on Machine Learning. PMLR, 2020.\\n9  Nandan Thakur et al. “Augmented SBERT: Data augmentation method for improving bi-\\nencoders for pairwise sentence scoring tasks.” arXiv preprint arXiv:2010.08240 (2020).\\n10  Tianyu Gao, Xingcheng Yao, and Danqi Chen. “SimCSE: Simple contrastive learning of\\nsentence embeddings.” arXiv preprint arXiv:2104.08821 (2021).\\n11  Fredrik Carlsson et al. “Semantic re-tuning with Contrastive Tension.” International\\nConference on Learning Representations, 2021. 2021.\\n12  Kexin Wang, Nils Reimers, and Iryna Gurevych. “TSDAE: Using Transformer-based\\nSequential Denoising Auto-Encoder for unsupervised sentence embedding learning.” arXiv\\npreprint arXiv:2104.06979 (2021).', '13  Kexin Wang et al. “GPL: Generative Pseudo Labeling for unsupervised domain adaptation of\\ndense retrieval.” arXiv preprint arXiv:2112.07577 (2021).\\nOceanofPDF.com', 'Chapter 11. Fine-Tuning\\nRepresentation Models for\\nClassification\\nIn Chapter 4, we used pretrained models to classify our text. We kept the\\npretrained models as they were without any modifications to them. This\\nmight make you wonder, what happens if we were to fine-tune them?\\nIf we have sufficient data, fine-tuning tends to lead to some of the best-\\nperforming models possible. In this chapter, we will go through several\\nmethods and applications for fine-tuning BERT models. “Supervised\\nClassification” demonstrates the general process of fine-tuning a\\nclassification model. Then, in “Few-Shot Classification”, we look at SetFit,\\nwhich is a method for efficiently fine-tuning a high-performing model using\\na small number of training examples. In “Continued Pretraining with\\nMasked Language Modeling”, we will explore how to continue training a\\npretrained model. Lastly, classification on a token level is explored in\\n“Named-Entity Recognition”.\\nWe will focus on nongenerative tasks, as generative models will be covered\\nin Chapter 12.\\nSupervised Classification\\nIn Chapter 4, we explored supervised classification tasks by leveraging\\npretrained representation models that were either trained to predict\\nsentiment (task-specific model) or to generate embeddings (embedding\\nmodel), as shown in Figure 11-1.', 'Figure 11-1. In Chapter 4, we used pretrained models to perform classification without updating\\ntheir weight. These models were kept “frozen.”\\nBoth models were kept frozen (nontrainable) to showcase the potential of\\nleveraging pretrained models for classification tasks. The embedding model\\nuses a separate trainable classification head (classifier) to predict the\\nsentiment of movie reviews.\\nIn this section, we will take a similar approach but allow both the model\\nand the classification head to be updated during training. As shown in\\nFigure 11-2, instead of using an embedding model, we will fine-tune a\\npretrained BERT model to create a task-specific model similar to the one\\nwe used in Chapter 2. Compared to the embedding model approach, we will\\nfine-tune both the representation model and the classification head as a\\nsingle architecture.', 'Figure 11-2. Compared to the “frozen” architecture, we instead train both the pretrained BERT\\nmodel and the classification head. A backward pass will start at the classification head and go\\nthrough BERT.\\nTo do so, instead of freezing the model, we allow it to be trainable and\\nupdate its parameters during training. As illustrated in Figure 11-3, we will\\nuse a pretrained BERT model and add a neural network as a classification\\nhead, both of which will be fine-tuned for classification.\\nFigure 11-3. The architecture of a task-specific model. It contains a pretrained representation model\\n(e.g., BERT) with an additional classification head for the specific task.', 'In practice, this means that the pretrained BERT model and the\\nclassification head are updated jointly. Instead of independent processes,\\nthey learn from one another and allow for more accurate representations.\\nFine-Tuning a Pretrained BERT Model\\nWe will be using the same dataset we used in Chapter 4 to fine-tune our\\nmodel, namely the Rotten Tomatoes dataset, which contains 5,331 positive\\nand 5,331 negative movie reviews from Rotten Tomatoes:\\nfrom datasets import load_dataset\\n# Prepare data and splits\\ntomatoes = load_dataset(\"rotten_tomatoes\")\\ntrain_data, test_data = tomatoes[\"train\"], tomatoes[\"test\"]\\nThe first step in our classification task is to select the underlying model we\\nwant to use. We use \"bert-base-cased\", which was pretrained on the\\nEnglish Wikipedia as well as a large dataset consisting of unpublished\\nbooks.1 \\nWe define the number of labels that we want to predict beforehand. This is\\nnecessary to create the feedforward neural network that is applied on top of\\nour pretrained model:\\nfrom transformers import AutoTokenizer, \\nAutoModelForSequenceClassification\\n# Load model and tokenizer\\nmodel_id = \"bert-base-cased\"\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\n    model_id, num_labels=2\\n)\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\nNext, we will tokenize our data:', 'from transformers import DataCollatorWithPadding\\n# Pad to the longest sequence in the batch\\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\\ndef preprocess_function(examples):\\n   \"\"\"Tokenize input data\"\"\"\\n   return tokenizer(examples[\"text\"], truncation=True)\\n# Tokenize train/test data\\ntokenized_train = train_data.map(preprocess_function, \\nbatched=True)\\ntokenized_test = test_data.map(preprocess_function, batched=True)\\nBefore creating the Trainer, we will want to prepare a special\\nDataCollator. A DataCollator is a class that helps us build batches\\nof data but also allows us to apply data augmentation.\\nDuring this process of tokenization, and as shown in Chapter 9, we will add\\npadding to the input text to create equally sized representations. We use\\nDataCollatorWithPadding for that.\\nOf course, an example would not be complete without defining some\\nmetrics:\\nimport numpy as np\\nfrom datasets import load_metric\\ndef compute_metrics(eval_pred):\\n   \"\"\"Calculate F1 score\"\"\"\\n   logits, labels = eval_pred\\n   predictions = np.argmax(logits, axis=-1)\\n   load_f1 = load_metric(\"f1\")\\n   f1 = load_f1.compute(predictions=predictions, \\nreferences=labels)[\"f1\"]\\n   return {\"f1\": f1}\\nWith compute_metrics we can define any number of metrics that we\\nare interested in and that can be printed out or logged during training. This', 'is especially helpful during training as it allows for detecting overfitting\\nbehavior.\\nNext, we instantiate our Trainer:\\nfrom transformers import TrainingArguments, Trainer\\n# Training arguments for parameter tuning\\ntraining_args = TrainingArguments(\\n   \"model\",\\n   learning_rate=2e-5,\\n   per_device_train_batch_size=16,\\n   per_device_eval_batch_size=16,\\n   num_train_epochs=1,\\n   weight_decay=0.01,\\n   save_strategy=\"epoch\",\\n   report_to=\"none\"\\n)\\n# Trainer which executes the training process\\ntrainer = Trainer(\\n   model=model,\\n   args=training_args,\\n   train_dataset=tokenized_train,\\n   eval_dataset=tokenized_test,\\n   tokenizer=tokenizer,\\n   data_collator=data_collator,\\n   compute_metrics=compute_metrics,\\n)\\nThe TrainingArguments class defines hyperparameters we want to\\ntune, such as the learning rate and how many epochs (rounds) we want to\\ntrain. The Trainer is used to execute the training process.\\nFinally, we can train our model and evaluate it:\\ntrainer.evaluate()\\n{\\'eval_loss\\': 0.3663691282272339,\\n \\'eval_f1\\': 0.8492366412213741,\\n \\'eval_runtime\\': 4.5792,\\n \\'eval_samples_per_second\\': 232.791,', \" 'eval_steps_per_second': 14.631,\\n 'epoch': 1.0}\\nWe get an F1 score of 0.85, which is quite a bit higher than the task-specific\\nmodel we used in Chapter 4, which resulted in an F1 score of 0.80. It shows\\nthat fine-tuning a model yourself can be more advantageous than using a\\npretrained model. It only costs us a couple of minutes to train.\\nFreezing Layers\\nTo further showcase the importance of training the entire network, the next\\nexample will demonstrate how you can use Hugging Face Transformers to\\nfreeze certain layers of your network.\\nWe will freeze the main BERT model and allow only updates to pass\\nthrough the classification head. This will be a great comparison as we will\\nkeep everything the same, except for freezing specific layers.\\nTo start, let’s reinitialize our model so we can start from scratch:\\n# Load model and tokenizer\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\n    model_id, num_labels=2\\n)\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\nOur pretrained BERT model contains a lot of layers that we can potentially\\nfreeze. Inspecting these layers gives insight into the structure of the network\\nand what we might want to freeze:\\n# Print layer names\\nfor name, param in model.named_parameters():\\n    print(name)\\nbert.embeddings.word_embeddings.weight\\nbert.embeddings.position_embeddings.weight\\nbert.embeddings.token_type_embeddings.weight\\nbert.embeddings.LayerNorm.weight\", 'bert.embeddings.LayerNorm.bias\\nbert.encoder.layer.0.attention.self.query.weight\\nbert.encoder.layer.0.attention.self.query.bias\\n...\\nbert.encoder.layer.11.output.LayerNorm.weight\\nbert.encoder.layer.11.output.LayerNorm.bias\\nbert.pooler.dense.weight\\nbert.pooler.dense.bias\\nclassifier.weight\\nclassifier.bias\\nThere are 12 (0–11) encoder blocks consisting of attention heads, dense\\nnetworks, and layer normalization. We further illustrate this architecture in\\nFigure 11-4 to demonstrate everything that could be potentially frozen. On\\ntop of that, we have our classification head.\\nFigure 11-4. The basic architecture of BERT with the additional classification head.', 'We could choose to only freeze certain layers to speed up computing but\\nstill allow the main model to learn from the classification task. Generally,\\nwe want frozen layers to be followed by trainable layers.\\nWe are going to freeze everything except for the classification head as we\\ndid in Chapter 2:\\nfor name, param in model.named_parameters():\\n     # Trainable classification head\\n     if name.startswith(\"classifier\"):\\n        param.requires_grad = True\\n      # Freeze everything else\\n     else:\\n        param.requires_grad = False\\nAs shown in Figure 11-5, we have frozen everything except for the\\nfeedforward neural network, which is our classification head.', 'Figure 11-5. We fully freeze all encoder blocks and embedding layers such that the BERT model does\\nnot learn new representations during fine-tuning.\\nNow that we have successfully frozen everything but the classification\\nhead, we can move on to train our model:\\nfrom transformers import TrainingArguments, Trainer\\n# Trainer which executes the training process\\ntrainer = Trainer(\\n   model=model,\\n   args=training_args,\\n   train_dataset=tokenized_train,\\n   eval_dataset=tokenized_test,\\n   tokenizer=tokenizer,\\n   data_collator=data_collator,\\n   compute_metrics=compute_metrics,\\n)\\ntrainer.train()\\nYou might notice that training has become much faster. That is because we\\nare only training the classification head, which provides us with a', \"significant speedup compared to fine-tuning the entire model:\\ntrainer.evaluate()\\n{'eval_loss': 0.6821751594543457,\\n 'eval_f1': 0.6331058020477816,\\n 'eval_runtime': 4.0175,\\n 'eval_samples_per_second': 265.337,\\n 'eval_steps_per_second': 16.677,\\n 'epoch': 1.0}\\nWhen we evaluate the model, we only get an F1 score of 0.63, which is\\nquite a bit lower compared to our original 0.85 score. Instead of freezing\\nnearly all layers, let’s freeze everything up until encoder block 10 as\\nillustrated in Figure 11-6, and see how it affects performance. A major\\nbenefit is that this reduces computation but still allows updates to flow\\nthrough part of the pretrained model:\", 'Figure 11-6. We freeze the first 10 encoder blocks of our BERT model. Everything else is trainable\\nand will be fine-tuned.\\n# Load model\\nmodel_id = \"bert-base-cased\"\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\n    model_id, num_labels=2\\n)\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\n# Encoder block 11 starts at index 165 and\\n# we freeze everything before that block\\nfor index, (name, param) in enumerate(model.named_parameters()):    \\n    if index < 165:\\n        param.requires_grad = False\\n# Trainer which executes the training process\\ntrainer = Trainer(\\n   model=model,\\n   args=training_args,\\n   train_dataset=tokenized_train,', \"   eval_dataset=tokenized_test,\\n   tokenizer=tokenizer,\\n   data_collator=data_collator,\\n   compute_metrics=compute_metrics,\\n)\\ntrainer.train()\\nAfter training, we evaluate the results:\\ntrainer.evaluate()\\n{'eval_loss': 0.40812647342681885,\\n 'eval_f1': 0.8,\\n 'eval_runtime': 3.7125,\\n 'eval_samples_per_second': 287.137,\\n 'eval_steps_per_second': 18.047,\\n 'epoch': 1.0}\\nWe got an F1 score of 0.8, which is much higher than our previous score of\\n0.63 when freezing all layers. It demonstrates that although we generally\\nwant to train as many layers as possible, you can get away with training less\\nif you do not have the necessary computing power.\\nTo further illustrate this effect, we tested the effect of iteratively freezing\\nencoder blocks and fine-tuning them as we did thus far. As shown in\\nFigure 11-7, training only the first five encoder blocks (red vertical line) is\\nenough to almost reach the performance of training all encoder blocks.\", 'Figure 11-7. The effect of freezing certain encoder blocks on the performance of the model. Training\\nmore blocks leads to improved performance but stabilizes early on.\\nNOTE\\nWhen you are training for multiple epochs, the difference (in training time and\\nresources) between freezing and not freezing often becomes larger. It is therefore\\nadvised to play around with a balance that works for you.\\nFew-Shot Classification\\nFew-shot classification is a technique within supervised classification\\nwhere you have a classifier learn target labels based on only a few labeled\\nexamples. This technique is great when you have a classification task but do\\nnot have many labeled data points readily available. In other words, this\\nmethod allows you to label a few high-quality data points per class on\\nwhich to train the model. This idea of using a few labeled data points for\\ntraining your model is shown in Figure 11-8.', 'Figure 11-8. In few-shot classification, we only use a few labeled data points to learn from.\\nSetFit: Efficient Fine-Tuning with Few Training Examples\\nTo perform few-shot text classification, we use an efficient framework\\ncalled SetFit.2  It is built on top of the architecture of sentence-\\ntransformers to generate high-quality textual representations that are\\nupdated during training. Only a few labeled examples are needed for this\\nframework to be competitive with fine-tuning a BERT-like model on a\\nlarge, labeled dataset as we explored in the previous example.\\nThe underlying algorithm of SetFit consists of three steps:\\n1. Sampling training data\\nBased on in-class and out-class selection of labeled data it generates\\npositive (similar) and negative (dissimilar) pairs of sentences\\n2. Fine-tuning embeddings\\nFine-tuning a pretrained embedding model based on the previously\\ngenerated training data\\n3. Training a classifier\\nCreate a classification head on top of the embedding model and train it\\nusing the previously generated training data', 'Before fine-tuning an embedding model, we need to generate training data.\\nThe model assumes the training data to be samples of positive (similar) and\\nnegative (dissimilar) pairs of sentences. However, when we are dealing with\\na classification task, our input data is generally not labeled as such.\\nSay, for example, we have the training dataset in Figure 11-9 that classifies\\ntext into two categories: text about programming languages, and text about\\npets.\\nFigure 11-9. Data in two classes: text about programming languages and text about pets.\\nIn step 1, SetFit handles this problem by generating the necessary data\\nbased on in-class and out-class selection as we illustrate in Figure 11-10.\\nFor example, when we have 16 sentences about sports, we can create 16 *\\n(16 – 1) / 2 = 120 pairs that we label as positive pairs. We can use this\\nprocess to generate negative pairs by collecting pairs from different classes.\\nFigure 11-10. Step 1: sampling training data. We assume sentences within a class are similar and\\ncreate positive pairs while sentences in different classes become negative pairs.', 'In step 2, we can use the generated sentence pairs to fine-tune the\\nembedding model. This leverages a method called contrastive learning to\\nfine-tune a pretrained BERT model. As we reviewed in Chapter 10,\\ncontrastive learning allows accurate sentence embeddings to be learned\\nfrom pairs of similar (positive) and dissimilar (negative) sentences.\\nSince we generated these pairs in the previous step, we can use them to\\nfine-tune a SentenceTransformers model. Although we have\\ndiscussed contrastive learning before, we again illustrate the method in\\nFigure 11-11 as a refresher.\\nFigure 11-11. Step 2: Fine-tuning a SentenceTransformers model. Using contrastive learning,\\nembeddings are learned from positive and negative sentence pairs.', 'The goal of fine-tuning this embedding model is that it can create\\nembeddings that are tuned to the classification task. The relevance of the\\nclasses, and their relative meaning, are distilled into the embeddings\\nthrough fine-tuning the embedding model.\\nIn step 3, we generate embeddings for all sentences and use those as the\\ninput of a classifier. We can use the fine-tuned\\nSentenceTransformers model to convert our sentences into\\nembeddings that we can use as features. The classifier learns from our fine-\\ntuned embeddings to accurately predict unseen sentences. This last step is\\nillustrated in Figure 11-12.\\nFigure 11-12. Step 3: Training a classifier. The classifier can be any scikit-learn model or a\\nclassification head.', 'When we put all the steps together, we get an efficient and elegant pipeline\\nfor performing classification when you only have a few labels per class. It\\ncleverly makes use of the idea that we have labeled data, although not in the\\nway that we would like it. The three steps together are illustrated in\\nFigure 11-13 to give a single overview of the entire procedure.\\nFirst, sentence pairs are generated based on in-class and out-class selection.\\nSecond, the sentence pairs are used to fine-tune a pretrained\\nSentenceTransformer model. Third, the sentences are embedded with\\nthe fine-tuned model on which a classifier is trained to predict the classes.\\nFigure 11-13. The three main steps of SetFit.\\nFine-Tuning for Few-Shot Classification\\nWe previously trained on a dataset containing roughly 8,500 movie reviews.\\nHowever, since this is a few-shot setting, we will only sample 16 examples\\nper class. With two classes, we will only have 32 documents to train on\\ncompared to the 8,500 movie reviews we used before!\\nfrom setfit import sample_dataset\\n# We simulate a few-shot setting by sampling 16 examples per \\nclass', 'sampled_train_data = sample_dataset(tomatoes[\"train\"], \\nnum_samples=16)\\nAfter sampling the data, we choose a pretrained\\nSentenceTransformer model to fine-tune. The official documentation\\ncontains an overview of pretrained SentenceTransformer models\\nfrom which we are going to be using \"sentence-\\ntransformers/all-mpnet-base-v2\". It is one of the best-\\nperforming models on the MTEB leaderboard, which shows the\\nperformance of embedding models across a variety of tasks:\\nfrom setfit import SetFitModel\\n# Load a pretrained SentenceTransformer model\\nmodel = SetFitModel.from_pretrained(\"sentence-transformers/all-\\nmpnet-base-v2\")\\nAfter loading in the pretrained SentenceTransformer model, we can\\nstart defining our SetFitTrainer. By default, a logistic regression\\nmodel is chosen as the classifier to train.\\nSimilar to what we did with Hugging Face Transformers, we can use the\\ntrainer to define and play around with relevant parameters. For example, we\\nset the num_epochs to 3 so that contrastive learning will be performed for\\nthree epochs:\\nfrom setfit import TrainingArguments as SetFitTrainingArguments\\nfrom setfit import Trainer as SetFitTrainer\\n# Define training arguments\\nargs = SetFitTrainingArguments(\\n    num_epochs=3, # The number of epochs to use for contrastive \\nlearning\\n    num_iterations=20  # The number of text pairs to generate\\n)\\nargs.eval_strategy = args.evaluation_strategy\\n# Create trainer\\ntrainer = SetFitTrainer(', '    model=model,\\n    args=args,\\n    train_dataset=sampled_train_data,\\n    eval_dataset=test_data,\\n    metric=\"f1\"\\n)\\nWe only need to call train to start the training loop. When we do, we\\nshould get the following output:\\n# Training loop\\ntrainer.train()\\n***** Running training *****\\n  Num unique pairs = 1280\\n  Batch size = 16\\n  Num epochs = 3\\n  Total optimization steps = 240\\nNotice that the output mentions that 1,280 sentence pairs were generated for\\nfine-tuning the SentenceTransformer model. As a default, 20\\nsentence pair combinations are generated for each sample in our data,\\nwhich would be 20 * 32 = 680 samples. We will have to multiply this value\\nby 2 for each positive and negative pair generated, 680 * 2 = 1,280 sentence\\npairs. Generating 1,280 sentence pairs is quite impressive considering we\\nonly had 32 labeled sentences to start with!', 'TIP\\nWhen we do not specifically define a classification head, by default a logistic regression\\nis used. If we would like to specify a classification head ourselves, we can do so by\\nspecifying the following model in SetFitTrainer:\\n# Load a SetFit model from Hub\\nmodel = SetFitModel.from_pretrained(\\n    \"sentence-transformers/all-mpnet-base-v2\",\\n    use_differentiable_head=True,\\n    head_params={\"out_features\": num_classes},\\n)\\n# Create trainer\\ntrainer = SetFitTrainer(\\n    model=model,\\n    ...\\n)\\nHere, num_classes refers to the number of classes that we want to predict.\\nNext, let’s evaluate the model to get a feeling of its performance:\\n# Evaluate the model on our test data\\ntrainer.evaluate()\\n{\\'f1\\': 0.8363988383349468}\\nWith only 32 labeled documents, we get an F1 score of 0.85. Considering\\nthat the model was trained on a tiny subset of the original data, this is very\\nimpressive! Moreover, in Chapter 2, we got the same performance but\\ninstead trained a logistic regression model on the embeddings of the full\\ndata. Thus, this pipeline demonstrates the potential of taking the time to\\nlabel just a few instances.', 'TIP\\nNot only can SetFit perform few-shot classification tasks, but it also has support for\\nwhen you have no labels at all, also called zero-shot classification. SetFit generates\\nsynthetic examples from the label names to resemble the classification task and then\\ntrains a SetFit model on them. For example, if the target labels are “happy” and “sad,”\\nthen synthetic data could be “The example is happy” and “This example is sad.”\\nContinued Pretraining with Masked\\nLanguage Modeling\\nIn the examples thus far, we leveraged a pretrained model and fine-tuned it\\nto perform classification. This process describes a two-step process: first\\npretraining a model (which was already done for us) and then fine-tuning it\\nfor a particular task. We illustrate this process in Figure 11-14.\\nFigure 11-14. To fine-tune the model on a target task—for example, classification—we either start\\nwith pretraining a BERT model or use a pretrained one.', 'This two-step approach is typically used throughout many applications. It\\nhas its limitations when faced with domain-specific data. The pretrained\\nmodel is often trained on very general data, like Wikipedia pages, and\\nmight not be tuned to your domain-specific words.\\nInstead of adopting this two-step approach, we can squeeze another step\\nbetween them, namely continue pretraining an already pretrained BERT\\nmodel. In other words, we can simply continue training the BERT model\\nusing masked language modeling (MLM) but instead use data from our\\ndomain. It is like going from a general BERT model to a BioBERT model\\nspecialized for the medical domain, to a fine-tuned BioBERT model to\\nclassify medication.\\nThis will update the subword representations to be more tuned toward\\nwords it would not have seen before. This process is illustrated in\\nFigure 11-15 and demonstrates how this additional step updates a masked\\nlanguage modeling task. Continuing pretraining on a pretrained BERT\\nmodel has been shown to improve the performance of models in\\nclassification tasks and is a worthwhile addition to the fine-tuning pipeline.3 \\nFigure 11-15. Instead of a two-step approach, we can add another step that continues to pretrain the\\npretrained model before fine-tuning it on the target task. Notice how the masks were filled with\\nabstract concepts in 1 while they were filled with movie-specific concepts in 2.', 'Instead of having to pretrain an entire model from scratch, we can simply\\ncontinue pretraining before fine-tuning it for classification. This also helps\\nthe model to adapt to a certain domain or even the lingo of a specific\\norganization. The genealogy of models a company might want to adopt is\\nfurther illustrated in Figure 11-16.\\nFigure 11-16. The three-step approach illustrated for specific use cases.\\nIn this example, we will demonstrate how to apply step 2 and continue\\npretraining an already pretrained BERT model. We use the same data that\\nwe started with, namely the Rotten Tomatoes reviews.\\nWe start by loading the \"bert-base-cased\" model we have used thus\\nfar and prepare it for MLM:\\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\\n# Load model for masked language modeling (MLM)\\nmodel = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")\\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\\nWe need to tokenize the raw sentences. We will also remove the labels since\\nthis is not a supervised task:\\ndef preprocess_function(examples):', '   return tokenizer(examples[\"text\"], truncation=True)\\n# Tokenize data\\ntokenized_train = train_data.map(preprocess_function, \\nbatched=True)\\ntokenized_train = tokenized_train.remove_columns(\"label\")\\ntokenized_test = test_data.map(preprocess_function, batched=True)\\ntokenized_test = tokenized_test.remove_columns(\"label\")\\nPreviously, we used DataCollatorWithPadding, which dynamically\\npads the input it receives.\\nInstead, we will have a DataCollator that will perform the masking of\\ntokens for us. There are two methods that are generally used for this: token\\nand whole-word masking. With token masking, we randomly mask 15% of\\nthe tokens in a sentence. It might happen that part of a word will be masked.\\nTo enable masking of the entire word, we could apply whole-word masking,\\nas illustrated in Figure 11-17.\\nFigure 11-17. Different methods for randomly masking tokens.\\nGenerally, predicting whole words tends to be more complicated than\\ntokens, which makes the model perform better as it needs to learn more\\naccurate and precise representations during training. However, it tends to\\ntake a bit more time to converge. We will be going with token masking in\\nthis example using DataCollatorForLanguageModeling for faster', 'convergence. However, we can use whole-word masking by replacing\\nDataCollatorForLanguageModeling with\\nDataCollatorForWholeWordMask. Lastly, we set the probability\\nthat a token is masked in a given sentence to 15% (mlm_probability):\\nfrom transformers import DataCollatorForLanguageModeling\\n# Masking Tokens\\ndata_collator = DataCollatorForLanguageModeling(\\n    tokenizer=tokenizer, \\n    mlm=True, \\n    mlm_probability=0.15\\n)\\nNext, we will create the Trainer for running the MLM task and specify\\ncertain parameters:\\n# Training arguments for parameter tuning\\ntraining_args = TrainingArguments(\\n   \"model\",\\n   learning_rate=2e-5,\\n   per_device_train_batch_size=16,\\n   per_device_eval_batch_size=16,\\n   num_train_epochs=10,\\n   weight_decay=0.01,\\n   save_strategy=\"epoch\",\\n   report_to=\"none\"\\n)\\n# Initialize Trainer\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=tokenized_train,\\n    eval_dataset=tokenized_test,\\n    tokenizer=tokenizer,\\n    data_collator=data_collator\\n)', 'Several parameters are worth noting. We train for 20 epochs and keep the\\ntask short. You can experiment with the learning rate and weight decay to\\nascertain whether they assist in fine-tuning the model.\\nBefore we start our training loop we will first save our pretrained tokenizer.\\nThe tokenizer is not updated during training so there is no need to save it\\nafter training. We will, however, save our model after we continue\\npretraining:\\n# Save pre-trained tokenizer\\ntokenizer.save_pretrained(\"mlm\")\\n# Train model\\ntrainer.train()\\n# Save updated model\\nmodel.save_pretrained(\"mlm\")\\nThis gives us an updated model in the mlm folder. To evaluate its\\nperformance we would normally fine-tune the model on a variety of tasks.\\nFor our purposes, however, we can run some masking tasks to see if it has\\nlearned from its continued training.\\nWe will do so by loading in our pretrained model before we continue\\npretraining. Using the sentence \"What a horrible [MASK]!\" the\\nmodel will predict which word would be in place of \"[MASK]\":\\nfrom transformers import pipeline\\n# Load and create predictions\\nmask_filler = pipeline(\"fill-mask\", model=\"bert-base-cased\")\\npreds = mask_filler(\"What a horrible [MASK]!\")\\n# Print results\\nfor pred in preds:\\n    print(f\">>> {pred[\"sequence\"]}\")\\n>>> What a horrible idea!\\n>>> What a horrible dream!', '>>> What a horrible thing!\\n>>> What a horrible day!\\n>>> What a horrible thought!\\nThe output demonstrates concepts like “idea,” “dream,” and “day,” which\\ndefinitely make sense. Next, let’s see what our updated model predicts:\\n# Load and create predictions\\nmask_filler = pipeline(\"fill-mask\", model=\"mlm\")\\npreds = mask_filler(\"What a horrible [MASK]!\")\\n# Print results\\nfor pred in preds:\\n    print(f\">>> {pred[\"sequence\"]}\")\\n>>> What a horrible movie!\\n>>> What a horrible film!\\n>>> What a horrible mess!\\n>>> What a horrible comedy!\\n>>> What a horrible story!\\nA horrible movie, film, mess, etc. clearly shows us that the model is more\\nbiased toward the data that we fed it compared to the pretrained model.\\nThe next step would be to fine-tune this model on the classification task that\\nwe did at the beginning of this chapter. Simply load the model as follows\\nand you are good to go:\\nfrom transformers import AutoModelForSequenceClassification\\n# Fine-tune for classification\\nmodel = AutoModelForSequenceClassification.from_pretrained(\"mlm\", \\nnum_labels=2)\\ntokenizer = AutoTokenizer.from_pretrained(\"mlm\")', 'Named-Entity Recognition\\nIn this section, we will delve into the process of fine-tuning a pretrained\\nBERT model specifically for NER (named-entity recognition). Instead of\\nclassifying entire documents, this procedure allows for the classification of\\nindividual tokens and/or words, including people and locations. This is\\nespecially helpful for de-identification and anonymization tasks when there\\nis sensitive data.\\nNER shares similarities with the classification example we explored at the\\nbeginning of this chapter. Nevertheless, a key distinction lies in the\\npreprocessing and classification of data. Given that we are focusing on\\nclassifying individual words instead of entire documents, we must\\npreprocess the data to consider this granular structure. Figure 11-18\\nprovides a visual representation of this word-level approach.\\nFigure 11-18. Fine-tuning a BERT model for NER allows for the detection of named entities, such as\\npeople or locations.\\nFine-tuning the pretrained BERT model follows a similar architecture akin\\nto what we observed with document classification. However, there is a\\nfundamental shift in the classification approach. Rather than relying on the\\naggregation or pooling of token embeddings, the model now makes\\npredictions for individual tokens in a sequence. It is crucial to emphasize\\nthat our word-level classification task does not entail classifying entire\\nwords, but rather the tokens that collectively constitute those words.', 'Figure 11-19 provides a visual representation of this token-level\\nclassification.\\nFigure 11-19. During the fine-tuning process of a BERT model, individual tokens are classified\\ninstead of words or entire documents.\\nPreparing Data for Named-Entity Recognition\\nIn this example, we will use the English version of the CoNLL-2003\\ndataset, which contains several different types of named entities (person,\\norganization, location, miscellaneous, and no entity) and has roughly\\n14,000 training samples.4 \\n# The CoNLL-2003 dataset for NER\\ndataset = load_dataset(\"conll2003\", trust_remote_code=True)', 'TIP\\nWhile researching datasets to use for this example, there were a few more that we\\nwanted to share. wnut_17 is a task that focuses on emerging and rare entities, those\\nthat are more difficult to spot. Furthermore, the tner/mit_movie_trivia and\\ntner/mit_restaurant datasets are quite fun to use.\\ntner/mit_movie_trivia is for detecting entities like actor, plot, and soundtrack\\nwhereas tner/mit_restaurant aims to detect entities such as amenity, dish, and\\ncuisine.5 \\nLet’s inspect the structure of the data with an example:\\nexample = dataset[\"train\"][848]\\nexample\\n{\\'id\\': \\'848\\',\\n \\'tokens\\': [\\'Dean\\',\\n  \\'Palmer\\',\\n  \\'hit\\',\\n  \\'his\\',\\n  \\'30th\\',\\n  \\'homer\\',\\n  \\'for\\',\\n  \\'the\\',\\n  \\'Rangers\\',\\n  \\'.\\'],\\n \\'pos_tags\\': [22, 22, 38, 29, 16, 21, 15, 12, 23, 7],\\n \\'chunk_tags\\': [11, 12, 21, 11, 12, 12, 13, 11, 12, 0],\\n \\'ner_tags\\': [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]}\\nThis dataset provides us with labels for each word given in a sentence.\\nThese labels can be found in the ner_tags key, which refers to the\\nfollowing possible entities:\\nlabel2id = {\\n    \"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-ORG\": 3, \"I-ORG\": 4, \\n    \"B-LOC\": 5, \"I-LOC\": 6, \"B-MISC\": 7, \"I-MISC\": 8\\n}', \"id2label = {index: label for label, index in label2id.items()}\\nlabel2id\\n{'O': 0,\\n 'B-PER': 1,\\n 'I-PER': 2,\\n 'B-ORG': 3,\\n 'I-ORG': 4,\\n 'B-LOC': 5,\\n 'I-LOC': 6,\\n 'B-MISC': 7,\\n 'I-MISC': 8}\\nThese entities correspond to specific categories: a person (PER),\\norganization (ORG), location (LOC), miscellaneous entities (MISC), and no\\nentity (O). Note that these entities are prefixed with either a B (beginning)\\nor an I (inside). If two tokens that follow each other are part of the same\\nphrase, then the start of that phrase is indicated with B, which is followed\\nby an I to show that they belong to each other and are not independent\\nentities.\\nThis process is further illustrated in Figure 11-20. In the figure, since\\n“Dean” is the start of the phrase and “Palmer” is the end, we know that\\n“Dean Palmer” is a person and that “Dean” and “Palmer” are not individual\\npeople.\\nFigure 11-20. By indicating the start and end of the phrase with the same entity, we can recognize\\nentities of entire phrases.\\nOur data is preprocessed and split up into words but not yet tokens. To do\\nso, we will tokenize it further with the tokenizer of the pretrained model we\\nused throughout this chapter, namely bert-base-cased:\", 'from transformers import AutoModelForTokenClassification\\n# Load tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\\n# Load model\\nmodel = AutoModelForTokenClassification.from_pretrained(\\n    \"bert-base-cased\", \\n    num_labels=len(id2label), \\n    id2label=id2label, \\n    label2id=label2id\\n)\\nLet’s explore how the tokenizer would process our example:\\n# Split individual tokens into sub-tokens\\ntoken_ids = tokenizer(example[\"tokens\"], is_split_into_words=True)\\n[\"input_ids\"]\\nsub_tokens = tokenizer.convert_ids_to_tokens(token_ids)\\nsub_tokens\\n[\\'[CLS]\\',\\n \\'Dean\\',\\n \\'Palmer\\',\\n \\'hit\\',\\n \\'his\\',\\n \\'30th\\',\\n \\'home\\',\\n \\'##r\\',\\n \\'for\\',\\n \\'the\\',\\n \\'Rangers\\',\\n \\'.\\',\\n \\'[SEP]\\']\\nThe tokenizer added the [CLS] and [SEP] tokens as we learned in\\nChapters 2 and 3. Note that the word \\'homer\\' was further split up into the\\ntokens \\'home\\' and \\'##r\\'. This creates a bit of a problem for us since we\\nhave labeled data at the word level but not at the token level. This can be', 'resolved by aligning the labels with their subtoken counterparts during\\ntokenization.\\nLet’s consider the word \\'Maarten\\', which has the label B-PER to signal\\nthat this is a person. If we pass that word through the tokenizer, it splits the\\nword up into the tokens \\'Ma\\', \\'##arte\\', and \\'##n\\'. We cannot use the\\nB-PER entity for all tokens as that would signal that the three tokens are all\\nindependent people. Whenever an entity is split into tokens, the first token\\nshould have B (for beginning) and the following should be I (for inner).\\nTherefore, \\'Ma\\' will get the B-PER to signal the start of a phrase, and\\n\\'##arte\\', and \\'##n\\' will get the I-PER to signal they belong to a\\nphrase. This alignment process is illustrated in Figure 11-21.\\nFigure 11-21. The alignment process of labeling tokenized input.\\nWe create a function, align_labels, that will tokenize the input and\\nalign these tokens with their updated labels during tokenization:\\ndef align_labels(examples):\\n    token_ids = tokenizer(\\n        examples[\"tokens\"], \\n        truncation=True, \\n        is_split_into_words=True\\n    )', '    labels = examples[\"ner_tags\"]\\n    updated_labels = []\\n    for index, label in enumerate(labels):\\n        \\n        # Map tokens to their respective word\\n        word_ids = token_ids.word_ids(batch_index=index)  \\n        previous_word_idx = None\\n        label_ids = []\\n        for word_idx in word_ids: \\n            # The start of a new word\\n            if word_idx != previous_word_idx:\\n                \\n                previous_word_idx = word_idx\\n                updated_label = -100 if word_idx is None else \\nlabel[word_idx]\\n                label_ids.append(updated_label)\\n            # Special token is -100\\n            elif word_idx is None:\\n                label_ids.append(-100)\\n            # If the label is B-XXX we change it to I-XXX\\n            else:\\n                updated_label = label[word_idx]\\n                if updated_label % 2 == 1:\\n                    updated_label += 1\\n                label_ids.append(updated_label)\\n        updated_labels.append(label_ids)\\n    token_ids[\"labels\"] = updated_labels\\n    return token_ids\\ntokenized = dataset.map(align_labels, batched=True)\\nLooking at our example, note that additional labels (-100) were added for\\nthe [CLS] and [SEP] tokens:\\n# Difference between original and updated labels', 'print(f\"Original: {example[\"ner_tags\"]}\")\\nprint(f\"Updated: {tokenized[\"train\"][848][\"labels\"]}\")\\nOriginal: [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]\\nUpdated: [-100, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, -100]\\nNow that we have tokenized and aligned the labels, we can start thinking\\nabout defining our evaluation metrics. This is also different from what we\\nhave seen before. Instead of a single prediction per document, we now have\\nmultiple predictions per document, namely per token.\\nWe will make use of the evaluate package by Hugging Face to create a\\ncompute_metrics function that allows us to evaluate performance on a\\ntoken level:\\nimport evaluate \\n# Load sequential evaluation\\nseqeval = evaluate.load(\"seqeval\")\\ndef compute_metrics(eval_pred):\\n    # Create predictions\\n    logits, labels = eval_pred\\n    predictions = np.argmax(logits, axis=2)\\n    true_predictions = []\\n    true_labels = []\\n    # Document-level iteration\\n    for prediction, label in zip(predictions, labels):\\n      # Token-level iteration\\n      for token_prediction, token_label in zip(prediction, label):\\n        # We ignore special tokens\\n        if token_label != -100:\\n          true_predictions.append([id2label[token_prediction]])\\n          true_labels.append([id2label[token_label]])\\n    results = seqeval.compute(\\n    predictions=true_predictions, references=true_labels', ')\\n    return {\"f1\": results[\"overall_f1\"]}\\nFine-Tuning for Named-Entity Recognition\\nWe are nearly there. Instead of DataCollatorWithPadding, we need\\na collator that works with classification on a token level, namely\\nDataCollatorForTokenClassification:\\nfrom transformers import DataCollatorForTokenClassification\\n# Token-classification DataCollator\\ndata_collator = \\nDataCollatorForTokenClassification(tokenizer=tokenizer)\\nNow that we have loaded our model, the rest of the steps are similar to\\nprevious training procedures in this chapter. We define a trainer with\\nspecific arguments that we can tune and create a Trainer:\\n# Training arguments for parameter tuning\\ntraining_args = TrainingArguments(\\n   \"model\",\\n   learning_rate=2e-5,\\n   per_device_train_batch_size=16,\\n   per_device_eval_batch_size=16,\\n   num_train_epochs=1,\\n   weight_decay=0.01,\\n   save_strategy=\"epoch\",\\n   report_to=\"none\"\\n)\\n# Initialize Trainer\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=tokenized[\"train\"],\\n    eval_dataset=tokenized[\"test\"],\\n    tokenizer=tokenizer,\\n    data_collator=data_collator,\\n    compute_metrics=compute_metrics,', ')\\ntrainer.train()\\nWe then evaluate the model that we created:\\n# Evaluate the model on our test data\\ntrainer.evaluate()\\nLastly, let’s save the model and use it in a pipeline for inference. This\\nallows us to check certain data so we can manually inspect what happens\\nduring inference and if we are satisfied with the output:\\nfrom transformers import pipeline\\n# Save our fine-tuned model\\ntrainer.save_model(\"ner_model\")\\n# Run inference on the fine-tuned model\\ntoken_classifier = pipeline(\\n    \"token-classification\", \\n    model=\"ner_model\", \\n)\\ntoken_classifier(\"My name is Maarten.\")\\n[{\\'entity\\': \\'B-PER\\',\\n  \\'score\\': 0.99534035,\\n  \\'index\\': 4,\\n  \\'word\\': \\'Ma\\',\\n  \\'start\\': 11,\\n  \\'end\\': 13},\\n {\\'entity\\': \\'I-PER\\',\\n  \\'score\\': 0.9928328,\\n  \\'index\\': 5,\\n  \\'word\\': \\'##arte\\',\\n  \\'start\\': 13,\\n  \\'end\\': 17},\\n {\\'entity\\': \\'I-PER\\',\\n  \\'score\\': 0.9954301,\\n  \\'index\\': 6,\\n  \\'word\\': \\'##n\\',\\n  \\'start\\': 17,\\n  \\'end\\': 18}]', 'In the sentence \"My name is Maarten\", the word \"Maarten\" and\\nits subtokens were correctly identified as a person!\\nSummary\\nIn this chapter, we explored several tasks for fine-tuning pretrained\\nrepresentation models on specific classification tasks. We started by\\ndemonstrating how to fine-tune a pretrained BERT model and extended the\\nexamples by freezing certain layers of its architectures.\\nWe experimented with a few-shot classification technique called SetFit,\\nwhich involves fine-tuning a pretrained embedding model together with a\\nclassification head using limited labeled data. Using only a few labeled data\\npoints, this model generated similar performance to the models we explored\\nin earlier chapters.\\nNext, we delved into the concept of continued pretraining, where we used a\\npretrained BERT model as a starting point and continued training it using\\ndifferent data. The underlying process, masked language modeling, is not\\nonly used for creating a representation model but can also be used to\\ncontinue pretraining models.\\nFinally, we looked at named-entity recognition, a task that involves\\nidentifying specific entities such as people and places in unstructured text.\\nCompared to previous examples, this classification was done on a word\\nlevel rather than on a document level.\\nIn the next chapter, we continue with the field of fine-tuning language\\nmodels but focus on generative models instead. Using a two-step process,\\nwe will explore how to fine-tune a generative model to properly follow\\ninstructions and then fine-tune it for human preference.\\n1  Jacob Devlin et al. “BERT: Pre-training of deep bidirectional transformers for language\\nunderstanding.” arXiv preprint arXiv:1810.04805 (2018).', '2  Lewis Tunstall et al. “Efficient few-shot learning without prompts.” arXiv preprint\\narXiv:2209.11055 (2022).\\n3  Chi Sun et al. “How to fine-tune GERT for text classification?” Chinese Computational\\nLinguistics: 18th China National Conference, CCL 2019, Kunming, China, October 18–20,\\n2019, proceedings 18. Springer International Publishing, 2019.\\n4  Erik F. Sang and Fien De Meulder. “Introduction to the CoNLL-2003 shared task: Language-\\nindependent named entity recognition.” arXiv preprint cs/0306050 (2003).\\n5  Jingjing Liu et al. “Asgard: A portable architecture for multilingual dialogue systems.” 2013\\nIEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013.\\nOceanofPDF.com', 'Chapter 12. Fine-Tuning\\nGeneration Models\\nIn this chapter, we will take a pretrained text generation model and go over\\nthe process of fine-tuning it. This fine-tuning step is key in producing high-\\nquality models and an important tool in our toolbox to adapt a model to a\\nspecific desired behavior. Fine-tuning allows us to adapt a model to a\\nspecific dataset or domain.\\nThroughout this chapter, we will guide you among the two most common\\nmethods for fine-tuning text generation models, supervised fine-tuning and\\npreference tuning. We will explore the transformative potential of fine-\\ntuning pretrained text generation models to make them more effective tools\\nfor your application.\\nThe Three LLM Training Steps: Pretraining,\\nSupervised Fine-Tuning, and Preference\\nTuning\\nThere are three common steps that lead to creating a high-quality LLM:\\n1. Language modeling\\nThe first step in creating a high-quality LLM is to pretrain it on one or\\nmore massive text datasets (Figure 12-1). During training, it attempts to\\npredict the next token to accurately learn linguistic and semantic\\nrepresentations found in the text. As we saw before in Chapters 3 and\\n11, this is called language modeling and is a self-supervised method.\\nThis produces a base model, also commonly referred to as a pretrained\\nor foundation model. Base models are a key artifact of the training', 'process but are harder for the end user to deal with. This is why the next\\nstep is important.\\nFigure 12-1. During language modeling, the LLM aims to predict the next token based on an\\ninput. This is a process without labels.\\n2. Fine-tuning 1 (supervised fine-tuning)\\nLLMs are more useful if they respond well to instructions and try to\\nfollow them. When humans ask the model to write an article, they\\nexpect the model to generate the article and not list other instructions\\nfor example (which is what a base model might do).\\nWith supervised fine-tuning (SFT), we can adapt the base model to\\nfollow instructions. During this fine-tuning process, the parameters of\\nthe base model are updated to be more in line with our target task, like\\nfollowing instructions. Like a pretrained model, it is trained using next-\\ntoken prediction but instead of only predicting the next token, it does so\\nbased on a user input (Figure 12-2).\\nFigure 12-2. During supervised fine-tuning, the LLM aims to predict the next token based on an\\ninput that has additional labels. In a sense, the label is the user’s input.', 'SFT can also be used for other tasks, like classification, but is often\\nused to go from a base generative model to an instruction (or chat)\\ngenerative model.\\n3. Fine-tuning 2 (preference tuning)\\nThe final step further improves the quality of the model and makes it\\nmore aligned with the expected behavior of AI safety or human\\npreferences. This is called preference tuning. Preference tuning is a\\nform of fine-tuning and, as the name implies, aligns the output of the\\nmodel to our preferences, which are defined by the data that we give it.\\nLike SFT, it can improve upon the original model but has the added\\nbenefit of distilling preference of output in its training process. These\\nthree steps are illustrated in Figure 12-3 and demonstrate the process of\\nstarting from an untrained architecture and ending with a preference-\\ntuned LLM.\\nFigure 12-3. The three steps of creating a high-quality LLM.\\nIn this chapter, we use a base model that was already trained on massive\\ndatasets and explore how we can fine-tune it using both fine-tuning\\nstrategies. For each method, we start with the theoretical underpinnings\\nbefore using them in practice.\\nSupervised Fine-Tuning (SFT)\\nThe purpose of pretraining a model on large datasets is that it is able to\\nreproduce language and its meaning. During this process, the model learns\\nto complete input phrases as shown in Figure 12-4.', 'Figure 12-4. A base or pretrained LLM was trained to predict the next word(s).\\nThis example also illustrates that the model was not trained to follow\\ninstructions and instead will attempt to complete a question rather than\\nanswer it (Figure 12-5).\\nFigure 12-5. A base LLM will not follow instructions but instead attempts to predict each next word.\\nIt may even create new questions.\\nWe can use this base model and adapt it to certain use cases, such as\\nfollowing instructions, by fine-tuning it.\\nFull Fine-Tuning\\nThe most common fine-tuning process is full fine-tuning. Like pretraining\\nan LLM, this process involves updating all parameters of a model to be in\\nline with your target task. The main difference is that we now use a smaller\\nbut labeled dataset whereas the pretraining process was done on a large\\ndataset without any labels (Figure 12-6).', 'Figure 12-6. Compared to language modeling (pretraining), full fine-tuning uses a smaller but\\nlabeled dataset.\\nYou can use any labeled data for full fine-tuning, making it also a great\\ntechnique for learning domain-specific representations. To make our LLM\\nfollow instructions, we will need question-response data. This data, as\\nshown in Figure 12-7, is queries by the user with corresponding answers.', 'Figure 12-7. Instruction data with instructions by a user and corresponding answers. The\\ninstructions can contain many different tasks.\\nDuring full fine-tuning, the model takes the input (instructions) and applies\\nnext-token prediction on the output (response). In turn, instead of\\ngenerating new questions, it will follow instructions.\\nParameter-Efficient Fine-Tuning (PEFT)\\nUpdating all parameters of a model has a large potential of increasing its\\nperformance but comes with several disadvantages. It is costly to train, has\\nslow training times, and requires significant storage. To resolve these\\nissues, attention has been given to parameter-efficient fine-tuning (PEFT)\\nalternatives that focus on fine-tuning pretrained models at higher\\ncomputational efficiency.', 'Adapters\\nAdapters are a core component of many PEFT-based techniques. The\\nmethod proposes a set of additional modular components inside the\\nTransformer that can be fine-tuned to improve the model’s performance on\\na specific task without having to fine-tune all the model weights. This saves\\na lot of time and compute.\\nAdapters are described in the paper “Parameter-efficient transfer learning\\nfor NLP”, which showed that fine-tuning 3.6% of the parameters of BERT\\nfor a task can yield comparable performance to fine-tuning all the model’s\\nweights.1  On the GLUE benchmark, the authors show they reach within\\n0.4% of the performance of full fine-tuning. In a single Transformer block,\\nthe paper’s proposed architecture places adapters after the attention layer\\nand the feedforward neural network as illustrated in Figure 12-8.', 'Figure 12-8. Adapters add a small number of weights in certain places in the network that can be\\nfine-tuned efficiently while leaving the majority of model weights frozen.\\nIt’s not enough to only alter one Transformer block, however, so these\\ncomponents are part of every block in the model, as Figure 12-9 shows.', 'Figure 12-9. Adapter components span the various Transformer blocks in the model.\\nSeeing all the adapter’s components across the model like this enables us to\\nsee individual adapters as shown in Figure 12-10, which is a collection of\\nthese components spanning all the blocks of the model. Adapter 1 can be a\\nspecialist in, say, medical text classification, while Adapter 2 can specialize', 'in named-entity recognition (NER). You can download specialized adapters\\nfrom https://oreil.ly/XraXg.\\nFigure 12-10. Adapters that specialize in specific tasks can be swapped into the same architecture (if\\nthey share the same original model architecture and weights).\\nThe paper “AdapterHub: A framework for adapting transformers”\\nintroduced the Adapter Hub as a central repository for sharing adapters.2  A\\nlot of these earlier adapters were more focused on BERT architectures.\\nMore recently, the concept has been applied to text generation Transformers\\nin papers like “LLaMA-Adapter: Efficient fine-tuning of language models\\nwith zero-init attention”.3 \\nLow-Rank Adaptation (LoRA)\\nAs an alternative to adapters, low-rank adaptation (LoRA) was introduced\\nand is at the time of writing is a widely used and effective technique for', 'PEFT. LoRA is a technique that (like adapters) only requires updating a\\nsmall set of parameters. As illustrated in Figure 12-11, it creates a small\\nsubset of the base model to fine-tune instead of adding layers to the model.4 \\nFigure 12-11. LoRA requires only fine-tuning a small set of parameters that can be kept separately\\nfrom the base LLM.\\nLike adapters, this subset allows for much quicker fine-tuning since we\\nonly need to update a small part of the base model. We create this subset of\\nparameters by approximating large matrices that accompany the original\\nLLM with smaller matrices. We can then use those smaller matrices as a\\nreplacement and fine-tune them instead of the original large matrices. Take\\nfor example the 10 × 10 matrix we see in Figure 12-12.', 'Figure 12-12. A major bottleneck of LLMs is their massive weight matrices. Only one of these may\\nhave 150 million parameters and each Transformer block would have its version of these.\\nWe can come up with two smaller matrices, which when multiplied,\\nreconstruct a 10 × 10 matrix. This is a major efficiency win because instead\\nof using 100 weights (10 times 10) we now only have 20 weights (10 plus\\n10), as we can see in Figure 12-13.\\nFigure 12-13. Decomposing a large weight matrix into two smaller matrices leads to a compressed,\\nlow-rank version of the matrix that can be fine-tuned more efficiently.', 'During training, we only need to update these smaller matrices instead of\\nthe full weight changes. The updated change matrices (smaller matrices) are\\nthen combined with the full (frozen) weights as illustrated in Figure 12-14.\\nFigure 12-14. Compared to full fine-tuning, LoRA aims to update a small representation of the\\noriginal weights during training.\\nBut you might suspect that performance would drop. And you would be\\nright. But where does this trade-off make sense?\\nPapers like “Intrinsic dimensionality explains the effectiveness of language\\nmodel fine-tuning” demonstrate that language models “have a very low\\nintrinsic dimension.”5  This means that we can find small ranks that\\napproximate even the massive matrices of an LLM. A 175B model like\\nGPT-3, for example, would have a weight matrix of 12,288 × 12,288 inside\\neach of its 96 Transformer blocks. That’s 150 million parameters. If we can\\nsuccessfully adapt that matrix into rank 8, that would only require two\\n12,288 × 2 matrices resulting in 197K parameters per block. These are\\nmajor savings in speed, storage, and compute as explained further in the\\npreviously referenced LoRA paper.\\nThis smaller representation is quite flexible in that you can select which\\nparts of the base model to fine-tune. For instance, we can only fine-tune the\\nQuery and Value weight matrices in each Transformer layer.', 'Compressing the model for (more) efficient training\\nWe can make LoRA even more efficient by reducing the memory\\nrequirements of the model’s original weights before projecting them into\\nsmaller matrices. The weights of an LLM are numeric values with a given\\nprecision, which can be expressed by the number of bits like float64 or\\nfloat32. As illustrated in Figure 12-15, if we lower the amount of bits to\\nrepresent a value, we get a less accurate result. However, if we lower the\\nnumber of bits we also lower the memory requirements of that model.\\nFigure 12-15. Attempting to represent pi with float 32-bit and float 16-bit representations. Notice the\\nlowered accuracy when we halve the number of bits.\\nWith quantization, we aim to lower the number of bits while still accurately\\nrepresenting the original weight values. However, as shown in Figure 12-\\n16, when directly mapping higher precision values to lower precision\\nvalues, multiple higher precision values might end up being represented by\\nthe same lower precision values.', 'Figure 12-16. Quantizing weights that are close to one another results in the same reconstructed\\nweights thereby removing any differentiating factor.\\nInstead, the authors of QLoRA, a quantized version of LoRA, found a way\\nto go from a higher number of bits to a lower value and vice versa without\\ndifferentiating too much from the original weights.6 \\nThey used blockwise quantization to map certain blocks of higher precision\\nvalues to lower precision values. Instead of directly mapping higher\\nprecision to lower precision values, additional blocks are created that allow\\nfor quantizing similar weights. As shown in Figure 12-17, this results in\\nvalues that can be accurately represented with lower precision.\\nFigure 12-17. Blockwise quantization can accurately represent weights in lower precision through\\nquantization blocks.', 'A nice property of neural networks is that their values are generally\\nnormally distributed between –1 and 1. This property allows us to bin the\\noriginal weights to lower bits based on their relative density, as illustrated in\\nFigure 12-18. The mapping between weights is more efficient as it takes\\ninto account the relative frequency of weights. This also reduces issues with\\noutliers.\\nFigure 12-18. Using distribution-aware blocks we can prevent values close to one another from being\\nrepresented with the same quantized value.\\nCombined with the blockwise quantization, this normalization procedure\\nallows for accurate representation of high precision values by low precision\\nvalues with only a small decrease in the performance of the LLM. As a\\nresult, we can go from a 16-bit float representation to a measly 4-bit\\nnormalized float representation. A 4-bit representation significantly reduces\\nthe memory requirements of the LLM during training. Note that the\\nquantization of LLMs in general is also helpful for inference as quantized\\nLLMs are smaller in size and therefore require less VRAM.\\nThere are more elegant methods to further optimize this like double\\nquantization and paged optimizers, which you can read about more in the', 'QLoRA paper discussed earlier. For a complete and highly visual guide to\\nquantization, see this blog post.\\nInstruction Tuning with QLoRA\\nNow that we have explored how QLoRA works, let us put that knowledge\\ninto practice! In this section, we will fine-tune a completely open source\\nand smaller version of Llama, TinyLlama, to follow instructions using the\\nQLoRA procedure. Consider this model a base or pretrained model, one that\\nwas trained with language modeling but cannot yet follow instructions.\\nTemplating Instruction Data\\nTo have the LLM follow instructions, we will need to prepare instruction\\ndata that follows a chat template. This chat template, as illustrated in\\nFigure 12-19, differentiates between what the LLM has generated and what\\nthe user has generated.\\nFigure 12-19. The chat template that we use throughout this chapter.\\nWe chose this chat template to use throughout the examples since the chat\\nversion of TinyLlama uses the same format. The data that we are using is a\\nsmall subset of the UltraChat dataset.7  This dataset is a filtered version of\\nthe original UltraChat dataset that contains almost 200k conversations\\nbetween a user and an LLM.', 'We create a function, format_prompt, to make sure that the\\nconversations follow this template:\\nfrom transformers import AutoTokenizer\\nfrom datasets import load_dataset\\n# Load a tokenizer to use its chat template\\ntemplate_tokenizer = AutoTokenizer.from_pretrained(\\n    \"TinyLlama/TinyLlama-1.1BChat-v1.0\"\\n)\\ndef format_prompt(example):\\n    \"\"\"Format the prompt to using the <|user|> template TinyLLama \\nis using\"\"\"\\n    # Format answers\\n    chat = example[\"messages\"]\\n    prompt = template_tokenizer.apply_chat_template(chat, \\ntokenize=False)\\n    return {\"text\": prompt}\\n# Load and format the data using the template TinyLLama is using\\ndataset = (\\n    load_dataset(\"HuggingFaceH4/ultrachat_200k\", \\nsplit=\"test_sft\")\\n      .shuffle(seed=42)\\n      .select(range(3_000))\\n)\\ndataset = dataset.map(format_prompt)\\nWe select a subset of 3,000 documents to reduce the training time, but you\\ncan increase this value to get more accurate results.\\nUsing the \"text\" column, we can explore these formatted prompts:\\n# Example of formatted prompt\\nprint(dataset[\"text\"][2576])\\n<|user|>\\nGiven the text: Knock, knock. Who\\'s there? Hike.\\nCan you continue the joke based on the given text material ', '\"Knock, knock. Who\\'s there? Hike\"?</s>\\n<|assistant|>\\nSure! Knock, knock. Who\\'s there? Hike. Hike who? Hike up your \\npants, it\\'s cold outside!</s>\\n<|user|>\\nCan you tell me another knock-knock joke based on the same text \\nmaterial \"Knock, knock. Who\\'s there? Hike\"?</s>\\n<|assistant|>\\nOf course! Knock, knock. Who\\'s there? Hike. Hike who? Hike your \\nway over here and let\\'s go for a walk!</s>\\nModel Quantization\\nNow that we have our data, we can start loading in our model. This is where\\nwe apply the Q in QLoRA, namely quantization. We use the\\nbitsandbytes package to compress the pretrained model to a 4-bit\\nrepresentation.\\nIn BitsAndBytesConfig, you can define the quantization scheme. We\\nfollow the steps used in the original QLoRA paper and load the model in 4-\\nbit (load_in_4bit) with a normalized float representation\\n(bnb_4bit_quant_type) and double quantization\\n(bnb_4bit_use_double_quant):\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, \\nBitsAndBytesConfig\\nmodel_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-\\n3T\"\\n# 4-bit quantization configuration - Q in QLoRA\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,  # Use 4-bit precision model loading\\n    bnb_4bit_quant_type=\"nf4\",  # Quantization type\\n    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\\n    bnb_4bit_use_double_quant=True,  # Apply nested quantization\\n)\\n# Load the model to train on the GPU', 'model = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=\"auto\",\\n    # Leave this out for regular SFT\\n    quantization_config=bnb_config,\\n)\\nmodel.config.use_cache = False\\nmodel.config.pretraining_tp = 1\\n# Load LLaMA tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(model_name, \\ntrust_remote_code=True)\\ntokenizer.pad_token = \"<PAD>\"\\ntokenizer.padding_side = \"left\"\\nThis quantization procedure allows us to decrease the size of the original\\nmodel while retaining most of the original weights’ precision. Loading the\\nmodel now only uses ~1 GB VRAM compared to the ~4 GB of VRAM it\\nwould need without quantization. Note that during fine-tuning, more\\nVRAM will be necessary so it does not cap out on the ~1 GB VRAM\\nneeded to load the model.\\nLoRA Configuration\\nNext, we will need to define our LoRA configuration using the peft\\nlibrary, which represents hyperparameters of the fine-tuning process:\\nfrom peft import LoraConfig, prepare_model_for_kbit_training, \\nget_peft_model\\n# Prepare LoRA Configuration\\npeft_config = LoraConfig(\\n    lora_alpha=32,  # LoRA Scaling\\n    lora_dropout=0.1,  # Dropout for LoRA Layers\\n    r=64,  # Rank\\n    bias=\"none\",\\n    task_type=\"CAUSAL_LM\",\\n    target_modules=  # Layers to target\\n     [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \\n\"o_proj\", \"down_proj\"]', ')\\n# Prepare model for training\\nmodel = prepare_model_for_kbit_training(model)\\nmodel = get_peft_model(model, peft_config)\\nThere are several parameters worth mentioning:\\nr\\nThis is the rank of the compressed matrices (recall this from Figure 12-\\n13) Increasing this value will also increase the sizes of compressed\\nmatrices leading to less compression and thereby improved\\nrepresentative power. Values typically range between 4 and 64.\\nlora_alpha\\nControls the amount of change that is added to the original weights. In\\nessence, it balances the knowledge of the original model with that of the\\nnew task. A rule of thumb is to choose a value twice the size of r.\\ntarget_modules\\nControls which layers to target. The LoRA procedure can choose to\\nignore specific layers, like specific projection layers. This can speed up\\ntraining but reduce performance and vice versa.\\nPlaying around with the parameters is a worthwhile experiment to get an\\nintuitive understanding of values that work and those that do not. You can\\nfind an amazing resource of additional tips on LoRA fine-tuning in the\\nAhead of AI newsletter by Sebastian Raschka.', 'NOTE\\nThis example demonstrates an efficient form of fine-tuning your model. If you want to\\nperform full fine-tuning instead, you can remove the quantization_config\\nparameter when loading the model and skip the creation of peft_config. By\\nremoving those, we would go from “Instruction tuning with QLoRA” to “full instruction\\ntuning.”\\nTraining Configuration\\nLastly, we need to configure our training parameters as we did in\\nChapter 11:\\nfrom transformers import TrainingArguments\\noutput_dir = \"./results\"\\n# Training arguments\\ntraining_arguments = TrainingArguments(\\n    output_dir=output_dir,\\n    per_device_train_batch_size=2,\\n    gradient_accumulation_steps=4,\\n    optim=\"paged_adamw_32bit\",\\n    learning_rate=2e-4,\\n    lr_scheduler_type=\"cosine\",\\n    num_train_epochs=1,\\n    logging_steps=10,\\n    fp16=True,\\n    gradient_checkpointing=True\\n)\\nThere are several parameters worth mentioning:\\nnum_train_epochs\\nThe total number of training rounds. Higher values tend to degrade\\nperformance so we generally like to keep this low.\\nlearning_rate', 'Determines the step size at each iteration of weight updates. The authors\\nof QLoRA found that higher learning rates work better for larger models\\n(>33B parameters).\\nlr_scheduler_type\\nA cosine-based scheduler to adjust the learning rate dynamically. It will\\nlinearly increase the learning rate, starting from zero, until it reaches the\\nset value. After that, the learning rate is decayed following the values of\\na cosine function.\\noptim\\nThe paged optimizers used in the original QLoRA paper.\\nOptimizing these parameters is a difficult task and there are no set\\nguidelines for doing so. It requires experimentation to figure out what\\nworks best for specific datasets, model sizes, and target tasks.\\nNOTE\\nAlthough this section describes instruction tuning, we could also use QLoRA to fine-\\ntune an instruction model. For instance, we could fine-tune a chat model to generate\\nspecific SQL code or to create JSON output that adheres to a specific format. As long as\\nyou have the data available (with appropriate query-response items), QLoRA is a great\\ntechnique for nudging an existing chat model to be more appropriate for your use case.\\nTraining\\nNow that we have prepared all our models and parameters, we can start\\nfine-tuning our model. We load in SFTTrainer and simply run\\ntrainer.train():\\nfrom trl import SFTTrainer', '# Set supervised fine-tuning parameters\\ntrainer = SFTTrainer(\\n    model=model,\\n    train_dataset=dataset,\\n    dataset_text_field=\"text\",\\n    tokenizer=tokenizer,\\n    args=training_arguments,\\n    max_seq_length=512,\\n    # Leave this out for regular SFT\\n    peft_config=peft_config,\\n)\\n# Train model\\ntrainer.train()\\n# Save QLoRA weights\\ntrainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\")\\nDuring training the loss will be printed every 10 steps according to the\\nlogging_steps parameter. If you are using the free GPU provided by\\nGoogle Colab, which is the Tesla T4 at the time of writing, then training\\nmight take up to an hour. A good time to take a break!\\nMerge Weights\\nAfter we have trained our QLoRA weights, we still need to combine them\\nwith the original weights to use them. We reload the model in 16 bits,\\ninstead of the quantized 4 bits, to merge the weights. Although the tokenizer\\nwas not updated during training, we save it to the same folder as the model\\nfor easier access:\\nfrom peft import AutoPeftModelForCausalLM\\nmodel = AutoPeftModelForCausalLM.from_pretrained(\\n    \"TinyLlama-1.1B-qlora\",\\n    low_cpu_mem_usage=True,\\n    device_map=\"auto\",\\n)', '# Merge LoRA and base model\\nmerged_model = model.merge_and_unload()\\nAfter merging the adapter with the base model, we can use it with the\\nprompt template that we defined earlier:\\nfrom transformers import pipeline\\n# Use our predefined prompt template\\nprompt = \"\"\"<|user|>\\nTell me something about Large Language Models.</s>\\n<|assistant|>\\n\"\"\"\\n# Run our instruction-tuned model\\npipe = pipeline(task=\"text-generation\", model=merged_model, \\ntokenizer=tokenizer)\\nprint(pipe(prompt)[0][\"generated_text\"])\\nLarge Language Models (LLMs) are artificial intelligence (AI) \\nmodels that learn language and understand what it means to say \\nthings in a particular language. They are trained on huge \\namounts of text…\\nThe aggregate output shows that the model now closely follows our\\ninstructions, which is not possible with the base model.\\nEvaluating Generative Models\\nEvaluating generative models poses a significant challenge. Generative\\nmodels are used across many diverse use cases, making it a challenge to\\nrely on a singular metric for judgment. Unlike more specialized models, a\\ngenerative model’s ability to solve mathematical questions does not\\nguarantee success in solving coding questions.\\nAt the same time, evaluating these models is vital, particularly in\\nproduction settings where consistency is important. Given their\\nprobabilistic nature, generative models do not necessarily generate\\nconsistent outputs; there is a need for robust evaluation.', 'In this section, we will explore a few common evaluation methods, but we\\nwant to emphasize the current lack of golden standards. No one metric is\\nperfect for all use cases.\\nWord-Level Metrics\\nOne common metrics category for comparing generative models is word-\\nlevel evaluation. These classic techniques compare a reference dataset with\\nthe generated tokens on a token(set) level. Common word-level metrics\\ninclude perplexity,8  ROUGE,9  BLEU,10  and BERTScore.11 \\nOf note is perplexity, which measures how well a language model predicts a\\ntext. Given input text, the model predicts how likely the next token is. With\\nperplexity, we assume a model performs better if it gives the next token a\\nhigh probability. In other words, the models should not be “perplexed”\\nwhen presented with a well-written document.\\nAs illustrated in Figure 12-20, when presented with the input “When a\\nmeasure becomes a,” the model is asked how probable the word “target” is\\nas the next word.\\nFigure 12-20. Next-word prediction is a central feature of many LLMs.\\nAlthough perplexity, and other word-level metrics, are useful metrics to\\nunderstand the confidence of the model, they are not a perfect measure.\\nThey do not account for consistency, fluency, creativity, or even correctness\\nof the generated text.\\nBenchmarks\\nA common method for evaluating generative models on language\\ngeneration and understanding tasks is on well-known and public\\nbenchmarks, such as MMLU,12  GLUE,13  TruthfulQA,14  GSM8k,15  and', 'HellaSwag.16  These benchmarks give us information about basic language\\nunderstanding but also complex analytical answering, like math problems.\\nAside from natural language tasks, some models specialize in other\\ndomains, like programming. These models tend to be evaluated on different\\nbenchmarks, such as HumanEval,17  which consists of challenging\\nprogramming tasks for the model to solve. Table 12-1 gives an overview of\\ncommon public benchmarks for generative models.', 'Table 12-1. Common public benchmarks for generative models\\nBenchmarkDescription Resources\\nMMLU The Massive Multitask\\nLanguage Understanding\\n(MMLU) benchmark tests the\\nmodel on 57 different tasks,\\nincluding classification,\\nquestion answering, and\\nsentiment analysis.\\nhttps://oreil.ly/nrG_g\\nGLUE The General Language\\nUnderstanding Evaluation\\n(GLUE) benchmark consists of\\nlanguage understanding tasks\\ncovering a wide degree of\\ndifficulty.\\nhttps://oreil.ly/LV_fb\\nTruthfulQA TruthfulQA measures the\\ntruthfulness of a model’s\\ngenerated text.\\nhttps://oreil.ly/i2Brj\\nGSM8k The GSM8k dataset contains\\ngrade-school math word\\nproblems. It is linguistically\\ndiverse and created by human\\nproblem writers.\\nhttps://oreil.ly/oOBXY\\nHellaSwag HellaSwag is a challenge\\ndataset for evaluating\\ncommon-sense inference. It\\nconsists of multiple-choice\\nquestions that the model needs\\nto answer. It can select one of\\nhttps://oreil.ly/aDvBP', 'BenchmarkDescription Resources\\nfour answer choices for each\\nquestion.\\nHumanEval The HumanEval benchmark is\\nused for evaluating generated\\ncode based on 164\\nprogramming problems.\\nhttps://oreil.ly/dlJIX\\nBenchmarks are a great way to get a basic understanding on how well a\\nmodel performs on a wide variety of tasks. A downside to public\\nbenchmarks is that models can be overfitted to these benchmarks to\\ngenerate the best responses. Moreover, these are still broad benchmarks and\\nmight not cover very specific use cases. Lastly, another downside is that\\nsome benchmarks require strong GPUs with a long running time (over\\nhours) to compute, which makes iteration difficult.\\nLeaderboards\\nWith so many different benchmarks, it is hard to choose which benchmark\\nbest suits your model. Whenever a model is released, you will often see it\\nevaluated on several benchmarks to showcase how it performs across the\\nboard.\\nAs such, leaderboards were developed containing multiple benchmarks. A\\ncommon leaderboard is the Open LLM Leaderboard, which, at the time of\\nwriting, includes six benchmarks, including HellaSwag, MMLU,\\nTruthfulQA, and GSM8k. Models that top the leaderboard, assuming they\\nwere not overfitted on the data, are generally regarded as the “best” model.\\nHowever, since these leaderboards often contain publicly available\\nbenchmarks, there is a risk of overfitting on the leaderboard.', 'Automated Evaluation\\nPart of evaluating a generative output is the quality of its text. For instance,\\neven if two models were to give the same correct answer to a question, the\\nway they derived that answer might be different. It is often not just about\\nthe final answer but also the construction of it. Similarly, although two\\nsummaries might be similar, one could be significantly shorter than another,\\nwhich is often important for a good summary.\\nTo evaluate the quality of the generated text above the correctness of the\\nfinal answer, LLM-as-a-judge was introduced.18  In essence, a separate\\nLLM is asked to judge the quality of the LLM to be evaluated. An\\ninteresting variant of this method is pairwise comparison. Two different\\nLLMs will generate an answer to a question and a third LLM will be the\\njudge to declare which is better.\\nAs a result, this methodology allows for automated evaluation of open-\\nended questions. A major advantage is that as LLMs improve, so do their\\ncapabilities to judge the quality of output. In other words, this evaluation\\nmethodology grows with the field.\\nHuman Evaluation\\nAlthough benchmarks are important, the gold standard of evaluation is\\ngenerally considered to be human evaluation. Even if an LLM scores well\\non broad benchmarks, it still might not score well on domain-specific tasks.\\nMoreover, benchmarks do not fully capture human preference and all\\nmethods discussed before are merely proxies for that.\\nA great example of a human-based evaluation technique is the Chatbot\\nArena.19  When you go to this leaderboard you are shown two (anonymous)\\nLLMs you can interact with. Any question or prompt you ask will be sent to\\nboth models and you will receive their output. Then, you can decide which\\noutput you prefer. This process allows for the community to vote on which\\nmodels they prefer without knowing which ones are presented. Only after\\nyou vote do you see which model generated which text.', 'At the time of writing, this method has generated over 800,000+ human\\nvotes that were used to compute a leaderboard. These votes are used to\\ncalculate the relative skill level of LLMs based on their win rates. For\\ninstance, if a low-ranked LLM beats a high-ranked LLM, its ranking\\nchanges significantly. In chess, this is referred to as the Elo rating system.\\nThis methodology therefore uses crowdsourced votes, which helps us\\nunderstand the quality of the LLM. However, it is still the aggregated\\nopinion of a wide variety of users, which might not relate to your use case.\\nAs a result, there is no one perfect method of evaluating LLMs. All\\nmentioned methodologies and benchmarks provide an important, although\\nlimited evaluation perspective. Our advice is to evaluate your LLM based\\non the intended use case. For coding, HumanEval would be more logical\\nthan GSM8k.\\nBut most importantly, we believe that you are the best evaluator. Human\\nevaluation remains the gold standard because it is up to you to decide\\nwhether the LLM works for your intended use case. As with the examples\\nin this chapter, we highly advise that you also try these models and perhaps\\ndevelop some questions yourself. For example, the authors of this book are\\nArabic (Jay Alammar) and Dutch (Maarten Grootendorst), and we often ask\\nquestions in our native language when approached with new models.\\nOne final note on this topic is a quote we hold dear:\\nWhen a measure becomes a target, it ceases to be a good measure.\\n—Goodhart’s Law20 \\nIn the context of LLMs, when using a specific benchmark, we tend to\\noptimize for that benchmark regardless of the consequences. For instance, if\\nwe focus purely on optimizing for generating grammatically correct\\nsentences, the model could learn to only output one sentence: “This is a\\nsentence.” It is grammatically correct but tells you nothing about its\\nlanguage understanding capabilities. Thus, the model may excel at a\\nspecific benchmark but potentially at the expense of other useful\\ncapabilities.', 'Preference-Tuning / Alignment / RLHF\\nAlthough our model can now follow instructions, we can further improve\\nits behavior by a final training phase that aligns it to how we expect it to\\nbehave in different scenarios. For instance, when asked “What is an LLM?”\\nwe might prefer an elaborate answer that describes the internals of an LLM\\ncompared to the answer “It is a large language model” without further\\nexplanations. How exactly do we align our (human) preference for one\\nanswer over the other with the output of an LLM?\\nTo start with, recall that an LLM takes a prompt and outputs a generation as\\nillustrated in Figure 12-21.\\nFigure 12-21. An LLM takes an input prompt and outputs a generation.\\nWe can ask a person (preference evaluator) to evaluate the quality of that\\nmodel generation. Say they assign it a certain score, like 4 (see Figure 12-\\n22).\\nFigure 12-22. Use a preference evaluator (human or otherwise) to evaluate the quality of the\\ngeneration.', 'Figure 12-23 shows a preference tuning step updating the model based on\\nthat score:\\nIf the score is high, the model is updated to encourage it to\\ngenerate more like this type of generation.\\nIf the score is low, the model is updated to discourage such\\ngenerations.\\nFigure 12-23. Preference tuning methods update the LLM based on the evaluation score.\\nAs always, we need many training examples. So can we automate the\\npreference evaluation? Yes, we can by training a different model called a\\nreward model.\\nAutomating Preference Evaluation Using\\nReward Models\\nTo automate preference evaluation, we need a step before the preference-\\ntuning step, namely to train a reward model, as shown in Figure 12-24.', 'Figure 12-24. We train a reward model before fine-tuning the LLM.\\nFigure 12-25 shows that to create a reward model, we take a copy of the\\ninstruction-tuned model and slightly change it so that instead of generating\\ntext, it now outputs a single score.\\nFigure 12-25. The LLM becomes a reward model by replacing its language modeling head with a\\nquality classification head.\\nThe Inputs and Outputs of a Reward Model\\nThe way we expect this reward model to work is that we give it a prompt\\nand a generation, and it outputs a single number indicating the\\npreference/quality of that generation in response to that prompt. Figure 12-\\n26 shows the reward model generating this single number.', 'Figure 12-26. Use a reward model trained on human preference to generate the completion quality\\nscore.\\nTraining a Reward Model\\nWe cannot directly use the reward model. It needs to first be trained to\\nproperly score generations. So let’s get a preference dataset that the model\\ncan learn from.', 'Reward model training dataset\\nOne common shape for preference datasets is for a training example to have\\na prompt, with one accepted generation and one rejected generation.\\n(Nuance: it’s not always a good versus bad generation; it can be that the two\\ngenerations are both good, but that one is better than the other). Figure 12-\\n27 shows an example preference training set with two training examples.\\nFigure 12-27. Preference tuning datasets are often made up of prompts with accepted and rejected\\ngenerations.\\nOne way to generate preference data is to present a prompt to the LLM and\\nhave it generate two different generations. As shown in Figure 12-28, we\\ncan ask human labelers which of the two they prefer.', 'Figure 12-28. Output two generations and ask a human labeler which one they prefer.\\nReward model training step\\nNow that we have the preference training dataset, we can proceed to train\\nthe reward model.\\nA simple step is that we use the reward model to:\\n1. Score the accepted generation\\n2. Score the rejected generation\\nFigure 12-29 shows the training objective: to ensure the accepted\\ngeneration has a higher score than the rejected generation.', 'Figure 12-29. The reward model aims to evaluate the quality scores of generations in response to a\\nprompt.\\nWhen we combine everything together as shown in Figure 12-30, we get\\nthe three stages to preference tuning:\\n1. Collect preference data\\n2. Train a reward model', '3. Use the reward model to fine-tune the LLM (operating as the\\npreference evaluator)\\nFigure 12-30. The three stages of preference tuning: collecting preference data, training a reward\\nmodel, and finally fine-tuning the LLM.\\nReward models are an excellent idea that can be further extended and\\ndeveloped. Llama 2, for example, trains two reward models: one that scores\\nhelpfulness and another that scores safety (Figure 12-31).\\nFigure 12-31. We can use multiple reward models to perform the scoring.\\nA common method to fine-tune the LLM with the trained reward model is\\nProximal Policy Optimization (PPO). PPO is a popular reinforcement\\ntechnique that optimizes the instruction-tuned LLM by making sure that the', 'LLM does not deviate too much from the expected rewards.21  It was even\\nused to train the original ChatGPT released in November 2022.\\nTraining No Reward Model\\nA disadvantage of PPO is that it is a complex method that needs to train at\\nleast two models, the reward model and the LLM, which can be more costly\\nthan perhaps necessary.\\nDirect Preference Optimization (DPO) is an alternative to PPO and does\\naway with the reinforcement-based learning procedure.22  Instead of using\\nthe reward model to judge the quality of a generation, we let the LLM itself\\ndo that. As illustrated in Figure 12-32, we use a copy of the LLM as the\\nreference model to judge the shift between the reference and trainable\\nmodel in the quality of the accepted generation and rejected generation.\\nFigure 12-32. Use the LLM itself as the reward model by comparing the output of a frozen model\\nwith the trainable model.\\nBy calculating this shift during training, we can optimize the likelihood of\\naccepted generations over rejected generations by tracking the difference in\\nthe reference model and the trainable model.', 'To calculate this shift and its related scores, the log probabilities of the\\nrejected generations and accepted generations are extracted from both\\nmodels. As illustrated in Figure 12-33, this process is performed at a token\\nlevel where the probabilities are combined to calculate the shift between the\\nreference and trainable models.\\nFigure 12-33. Scores are calculated by taking the probabilities of generation on a token level. The\\nshift in probabilities between the reference model and the trainable model is optimized. The accepted\\ngeneration follows the same procedure.\\nUsing these scores, we can optimize the parameters of the trainable model\\nto be more confident of generating the accepted generations and less\\nconfident of generating the rejected generations. Compared to PPO, the\\nauthors found DPO to be more stable during training and more accurate.\\nDue to its stability, we will be using it as our primary model for preference\\ntuning our previously instruction-tuned model.', 'Preference Tuning with DPO\\nWhen we use the Hugging Face stack, preference tuning is eerily similar to\\nthe instruction tuning we covered before with some slight differences. We\\nwill still be using TinyLlama but this time an instruction-tuned version that\\nwas first trained using full fine-tuning and then further aligned with DPO.\\nCompared to our initial instruction-tuned model, this LLM was trained on\\nmuch larger datasets.\\nIn this section, we will demonstrate how you can further align this model\\nusing DPO with reward-based datasets.\\nTemplating Alignment Data\\nWe will use a dataset that for each prompt contains an accepted generation\\nand a rejected generation. This dataset was in part generated by ChatGPT\\nwith scores on which output should be accepted and which rejected:\\nfrom datasets import load_dataset\\ndef format_prompt(example):\\n    \"\"\"Format the prompt to using the <|user|> template TinyLLama \\nis using\"\"\"\\n    # Format answers\\n    system = \"<|system|>\\\\n\" + example[\"system\"] + \"</s>\\\\n\"\\n    prompt = \"<|user|>\\\\n\" + example[\"input\"] + \"\\n</s>\\\\n<|assistant|>\\\\n\"\\n    chosen = example[\"chosen\"] + \"</s>\\\\n\"\\n    rejected = example[\"rejected\"] + \"</s>\\\\n\"\\n    return {\\n        \"prompt\": system + prompt,\\n        \"chosen\": chosen,\\n        \"rejected\": rejected,\\n    }\\n# Apply formatting to the dataset and select relatively short \\nanswers\\ndpo_dataset = load_dataset(', '    \"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\"\\n)\\ndpo_dataset = dpo_dataset.filter(\\n    lambda r: \\n        r[\"status\"] != \"tie\" and \\n        r[\"chosen_score\"] >= 8 and \\n        not r[\"in_gsm8k_train\"]\\n)\\ndpo_dataset = dpo_dataset.map(\\n    format_prompt,  remove_columns=dpo_dataset.column_names\\n)\\ndpo_dataset\\nNote that we apply additional filtering to further reduce the size of the data\\nto roughly 6,000 examples from the original 13,000 examples.\\nModel Quantization\\nWe load our base model and load it with the LoRA we created previously.\\nAs before, we quantize the model to reduce the necessary VRAM for\\ntraining:\\nfrom peft import AutoPeftModelForCausalLM\\nfrom transformers import BitsAndBytesConfig, AutoTokenizer\\n# 4-bit quantization configuration - Q in QLoRA\\nbnb_config = BitsAndBytesConfig(\\n    load_in_4bit=True,  # Use 4-bit precision model loading\\n    bnb_4bit_quant_type=\"nf4\",  # Quantization type\\n    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\\n    bnb_4bit_use_double_quant=True,  # Apply nested quantization\\n)\\n# Merge LoRA and base model\\nmodel = AutoPeftModelForCausalLM.from_pretrained(\\n    \"TinyLlama-1.1B-qlora\",\\n    low_cpu_mem_usage=True,\\n    device_map=\"auto\",\\n    quantization_config=bnb_config,\\n)\\nmerged_model = model.merge_and_unload()', '# Load LLaMA tokenizer\\nmodel_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-\\n3T\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name, \\ntrust_remote_code=True)\\ntokenizer.pad_token = \"<PAD>\"\\ntokenizer.padding_side = \"left\"\\nNext, we use the same LoRA configuration as before to perform the DPO\\ntraining:\\nfrom peft import LoraConfig, prepare_model_for_kbit_training, \\nget_peft_model\\n# Prepare LoRA configuration\\npeft_config = LoraConfig(\\n    lora_alpha=32,  # LoRA Scaling\\n    lora_dropout=0.1,  # Dropout for LoRA Layers\\n    r=64,  # Rank\\n    bias=\"none\",\\n    task_type=\"CAUSAL_LM\",\\n    target_modules=  # Layers to target\\n     [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \\n\"o_proj\", \"down_proj\"]\\n)\\n# prepare model for training\\nmodel = prepare_model_for_kbit_training(model)\\nmodel = get_peft_model(model, peft_config)\\nTraining Configuration\\nFor the sake of simplicity, we will use the same training arguments as we\\ndid before with one difference. Instead of running for a single epoch (which\\ncan take up to two hours), we run for 200 steps instead for illustration\\npurposes. Moreover, we added the warmup_ratio parameter, which\\nincreases the learning rate from 0 to the learning_rate value we set for\\nthe first 10% of steps. By maintaining a small learning rate at the start (i.e.,', 'warmup period), we allow the model to adjust to the data before applying\\nlarger learning rates, therefore avoiding harmful divergence:\\nfrom trl import DPOConfig\\noutput_dir = \"./results\"\\n# Training arguments\\ntraining_arguments = DPOConfig(\\n    output_dir=output_dir,\\n    per_device_train_batch_size=2,\\n    gradient_accumulation_steps=4,\\n    optim=\"paged_adamw_32bit\",\\n    learning_rate=1e-5,\\n    lr_scheduler_type=\"cosine\",\\n    max_steps=200,\\n    logging_steps=10,\\n    fp16=True,\\n    gradient_checkpointing=True,\\n    warmup_ratio=0.1\\n)\\nTraining\\nNow that we have prepared all our models and parameters, we can start\\nfine-tuning our model:\\nfrom trl import DPOTrainer\\n# Create DPO trainer\\ndpo_trainer = DPOTrainer(\\n    model,\\n    args=training_arguments,\\n    train_dataset=dpo_dataset,\\n    tokenizer=tokenizer,\\n    peft_config=peft_config,\\n    beta=0.1,\\n    max_prompt_length=512,\\n    max_length=512,\\n)\\n# Fine-tune model with DPO', 'dpo_trainer.train()\\n# Save adapter\\ndpo_trainer.model.save_pretrained(\"TinyLlama-1.1B-dpo-qlora\")\\nWe have created a second adapter. To merge both adapters, we iteratively\\nmerge the adapters with the base model:\\nfrom peft import PeftModel\\n# Merge LoRA and base model\\nmodel = AutoPeftModelForCausalLM.from_pretrained(\\n    \"TinyLlama-1.1B-qlora\",\\n    low_cpu_mem_usage=True,\\n    device_map=\"auto\",\\n)\\nsft_model = model.merge_and_unload()\\n# Merge DPO LoRA and SFT model\\ndpo_model = PeftModel.from_pretrained(\\n    sft_model,\\n    \"TinyLlama-1.1B-dpo-qlora\",\\n    device_map=\"auto\",\\n)\\ndpo_model = dpo_model.merge_and_unload()\\nThis combination of SFT+DPO is a great way to first fine-tune your model\\nto perform basic chatting and then align its answers with human preference.\\nHowever, it does come at a cost since we need to perform two training\\nloops and potentially tweak the parameters in two processes.\\nSince the release of DPO, new methods of aligning preferences have been\\ndeveloped. Of note is Odds Ratio Preference Optimization (ORPO), a\\nprocess that combines SFT and DPO into a single training process.23  It\\nremoves the need to perform two separate training loops, further\\nsimplifying the training process while allowing for the use of QLoRA.', 'Summary\\nIn this chapter, we explored different steps of fine-tuning pretrained LLMs.\\nWe performed fine-tuning by making use of parameter-efficient fine-tuning\\n(PEFT) through the low-rank adaptation (LoRA) technique. We explained\\nhow LoRA can be extended through quantization, a technique for reducing\\nmemory constraints when representing the parameters of the model and\\nadapters.\\nThe fine-tuning process we explored has two steps. In the first step, we\\nperformed supervised fine-tuning using instruction data on a pretrained\\nLLM, often called instruction tuning. This resulted in a model that has chat-\\nlike behavior and could closely follow instructions.\\nIn the second step, we further improved the model by fine-tuning it on\\nalignment data, data that represents what type of answers are preferred over\\nothers. This process, referred to as preference tuning, distills human\\npreference to the previously instruction-tuned model.\\nOverall, this chapter has shown the two major steps of fine-tuning a\\npretrained LLM and how that could lead to more accurate and informative\\noutputs.\\n1  Neil Houlsby et al. “Parameter-efficient transfer learning for NLP.” International Conference\\non Machine Learning. PMLR, 2019.\\n2  Jonas Pfeiffer et al. “AdapterHub: A framework for adapting transformers.” arXiv preprint\\narXiv:2007.07779 (2020).\\n3  Renrui Zhang et al. “Llama-adapter: Efficient fine-tuning of language models with zero-init\\nattention.” arXiv preprint arXiv:2303.16199 (2023).\\n4  Edward J. Hu et al. “LoR: Low-Rank Adaptation of large language models.” arXiv preprint\\narXiv:2106.09685 (2021).\\n5  Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. “Intrinsic dimensionality explains\\nthe effectiveness of language model fine-tuning.” arXiv preprint arXiv:2012.13255 (2020).\\n6  Tim Dettmers et al. “QLoRA: Efficient finetuning of quantized LLMs.” arXiv preprint\\narXiv:2305.14314 (2023).', '7  Ning Ding et al. “Enhancing chat language models by scaling high-quality instructional\\nconversations.” arXiv preprint arXiv:2305.14233 (2023).\\n8  Fred Jelinek et al. “Perplexity—a measure of the difficulty of speech recognition tasks.” The\\nJournal of the Acoustical Society of America 62.S1 (1977): S63.\\n9  Chin-Yew Lin. “ROUGE: A package for automatic evaluation of summaries.” Text\\nSummarization Branches Out, 74–81. 2004.\\n10  Kishore Papineni, et al. “Bleu: a method for automatic evaluation of machine translation.”\\nProceedings of the 40th Annual Meeting of the Association for Computational Linguistics.\\n2002.\\n11  Tianyi Zhang et al. “BERTscore: Evaluating text generation with BERT.” arXiv preprint\\narXiv:1904.09675 (2019).\\n12  Dan Hendrycks et al. “Measuring massive multitask language understanding.” arXiv preprint\\narXiv:2009.03300 (2020).\\n13  Alex Wang et al. “GLUE: A multi-task benchmark and analysis platform for natural language\\nunderstanding.” arXiv preprint arXiv:1804.07461 (2018).\\n14  Stephanie Lin, Jacob Hilton, and Owain Evans. “TruthfulQA: Measuring how models mimic\\nhuman falsehoods.” arXiv preprint arXiv:2109.07958 (2021).\\n15  Karl Cobbe et al. “Training verifiers to solve math word problems.” arXiv preprint\\narXiv:2110.14168 (2021).\\n16  Roman Zellers et al. “HellaSwag: Can a machine really finish your sentence?” arXiv preprint\\narXiv:1905.07830 (2019).\\n17  Mark Chen et al. “Evaluating large language models trained on code.” arXiv preprint\\narXiv:2107.03374 (2021).\\n18  Lianmin Zheng et al. “Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.”\\nAdvances in Neural Information Processing Systems 36 (2024).\\n19  Wei-Lin Chiang et al. “Chatbot Arena: An open platform for evaluating LLMs by human\\npreference.” arXiv preprint arXiv:2403.04132 (2024).\\n20  Mafilyn Strathern. “‘Improving ratings’: audit in the British University system.” European\\nReview 5.3 (1997): 305–321.\\n21  John Schulman et al. “Proximal Policy Optimization algorithms.” arXiv preprint\\narXiv:1707.06347 (2017).\\n22  Rafael Rafailov, et al. “Direct Preference Optimization: Your language model is secretly a\\nreward model.” arXiv preprint arXiv:2305.18290 (2023).', '23  Jiwoo Hong, Noah Lee, and James Thorne, “ORPO: Monolithic preference optimization\\nwithout reference model”. arXiv preprint arXiv:2403.07691 (2024).\\nOceanofPDF.com', 'Afterword \\nThank you to all who joined us on this fascinating journey through the\\nworld of large language models. We are grateful for your dedication to\\nlearning about these powerful models that have revolutionized language\\nprocessing.\\nThroughout this book, we have seen how LLMs work and how they can be\\nused to create a wide range of applications, from simple chatbots to more\\ncomplex systems like search engines. We have also explored various\\nmethods for fine-tuning pretrained LLMs on specific tasks, including\\nclassification, generation, and language representation. By mastering these\\ntechniques, readers will be able to unlock the potential of LLMs and create\\ninnovative solutions that can benefit from their capabilities. This knowledge\\nwill enable readers to stay ahead of the curve and adapt to new\\ndevelopments in the field.\\nAs we come to the end of this book, we want to emphasize that our\\nexploration of LLMs is only just the beginning. There are many more\\nexciting developments on the horizon, and we encourage you to continue\\nfollowing the advancements in the field. To help with this process, keep an\\neye out on the repository of this book as we continue to add resources.\\nWe hope that by reading this book, you gained a deeper understanding of\\nhow LLMs can be used in various applications and how they have the\\npotential to transform industries.\\nWith this book as your guide, we believe that you will be well-equipped to\\nnavigate the exciting landscape of LLMs and make meaningful\\ncontributions to this rapidly advancing field.\\nOceanofPDF.com', 'Index \\nA \\naccuracy\\nconfusion matrices, Using a Task-Specific Model\\noutput verification, Output Verification\\nadaptive pretraining, Using TSDAE for Domain Adaptation\\nagents, Agents: Creating a System of LLMs-ReAct in LangChain\\nagentic RAG, Agentic RAG\\nReAct in LangChain, ReAct in LangChain-ReAct in LangChain\\nstep-by-step reasoning, The Driving Power Behind Agents: Step-by-\\nstep Reasoning-The Driving Power Behind Agents: Step-by-step\\nReasoning\\nAI (artificial intelligence)\\naccelerated development of, An Introduction to Large Language\\nModels\\ndefined, What Is Language AI?\\nALBERT, Model Selection\\nalign_labels function, Preparing Data for Named-Entity Recognition\\nall-MiniLM-L6-v2 model, Supervised\\nall-mpnet-base-v2 model, Fine-Tuning for Few-Shot Classification\\nAnnoy, Nearest neighbor search versus vector databases', 'Anthropic Claude, Proprietary, Private Models\\nAPIs (application programming interfaces), Proprietary, Private Models\\nCohere, API Keys, Dense retrieval example\\nexternal, ChatGPT for Classification\\ngenerating embeddings, Supervised Classification\\nOpenAI, API Keys, ChatGPT for Classification\\nartificial intelligence (see AI)\\nArXiv, ArXiv’s Articles: Computation and Language\\nattention, Encoding and Decoding Context with Attention-Attention Is All\\nYou Need\\noverview of, Encoding and Decoding Context with Attention-\\nEncoding and Decoding Context with Attention\\nTransformer architecture, Attention Is All You Need-Attention Is All\\nYou Need\\nattention calculation, How attention is calculated-How attention is\\ncalculated\\nattention layer, The Components of the Forward Pass, Inside the\\nTransformer Block, The attention layer at a glance, Summary\\nFlash Attention, Flash Attention\\ngrouped-query attention, Multi-query and grouped-query\\nattention-Optimizing attention: From multi-head to multi-query to\\ngrouped query\\nlocal attention, Local/sparse attention\\nmulti-query attention, Multi-query and grouped-query attention-\\nOptimizing attention: From multi-head to multi-query to grouped', 'query\\noptimizing attention, Optimizing attention: From multi-head to\\nmulti-query to grouped query\\nself-attention and relevance scoring, Self-attention: Relevance\\nscoring-Self-attention: Combining information\\nsparse attention, Local/sparse attention\\nattention heads, How attention is calculated-How attention is calculated,\\nSummary\\naudience, in text-generation prompts, The Potential Complexity of a Prompt\\nAugmented SBERT, Augmented SBERT-Augmented SBERT\\nautoregressive architecture, Encoding and Decoding Context with\\nAttention, Attention Is All You Need, Generative Models: Decoder-Only\\nModels, The Inputs and Outputs of a Trained Transformer LLM,\\nLocal/sparse attention\\nB \\nbag-of-words model, Representing Language as a Bag-of-Words-\\nRepresenting Language as a Bag-of-Words\\nembeddings, Types of Embeddings\\ntopic modeling, From Text Clustering to Topic Modeling-BERTopic: A\\nModular Topic Modeling Framework, Adding a Special Lego Block\\nbenchmarks, in generative model evaluation, Benchmarks\\nBERT (Bidirectional Encoder Representations from Transformers),\\nRepresentation Models: Encoder-Only Models-Generative Models:\\nDecoder-Only Models', 'adoption by search engines, Semantic Search and Retrieval-\\nAugmented Generation\\nBERT-like models, Model Selection\\ncomparing to other trained tokenizers, BERT base model (uncased)\\n(2018)\\nfine-tuning pretrained BERT models, Fine-Tuning a Pretrained BERT\\nModel-Fine-Tuning a Pretrained BERT Model\\nmasked language modeling, Supervised\\nTransformer blocks versus, Local/sparse attention\\nBERTopic, BERTopic: A Modular Topic Modeling Framework-BERTopic:\\nA Modular Topic Modeling Framework\\nalgorithmic variants, BERTopic: A Modular Topic Modeling\\nFramework\\nmodularity of, BERTopic: A Modular Topic Modeling Framework\\nrepresentation blocks, Adding a Special Lego Block-The Text\\nGeneration Lego Block\\nKeyBERTInspired, KeyBERTInspired\\nmaximal marginal relevance, Maximal marginal relevance\\ntext generation, The Text Generation Lego Block-The Text\\nGeneration Lego Block\\nBERTScore, Word-Level Metrics\\nbias and fairness, Responsible LLM Development and Usage\\nBidirectional Encoder Representations from Transformers (see BERT)\\nbitsandbytes package, Model Quantization', 'BLEU, Word-Level Metrics\\nBLIP-2 (Bootstrapping Language-Image Pre-training for Unified Vision-\\nLanguage Understanding and Generation 2)\\nchat-based prompting, Use Case 2: Multimodal Chat-Based\\nPrompting-Use Case 2: Multimodal Chat-Based Prompting\\nimage captioning, Use Case 1: Image Captioning\\npreprocessing text, Preprocessing text\\nQ-Former, BLIP-2: Bridging the Modality Gap-BLIP-2: Bridging the\\nModality Gap\\nBM25 algorithm, Search the index\\nBPE (byte pair encoding), How Does the Tokenizer Break Down Text?,\\nTokenization methods\\nbyte tokens, Word Versus Subword Versus Character Versus Byte Tokens\\nC \\nc-TF-IDF, BERTopic: A Modular Topic Modeling Framework,\\nKeyBERTInspired, The Text Generation Lego Block\\ncapitalization, Tokenizer parameters\\ncaptioning, Use Case 1: Image Captioning-Use Case 1: Image Captioning\\ncentroid-based algorithms, Cluster the Reduced Embeddings\\nchains, Chain Prompting: Breaking up the Problem-Chain Prompting:\\nBreaking up the Problem, Chains: Extending the Capabilities of LLMs-A\\nChain with Multiple Prompts\\nchain-of-thought, Chain-of-Thought: Think Before Answering-Chain-\\nof-Thought: Think Before Answering', 'chaining single prompt, A Single Link in the Chain: Prompt Template-\\nA Single Link in the Chain: Prompt Template\\nsequential chaining of multiple prompts, A Chain with Multiple\\nPrompts-A Chain with Multiple Prompts\\ncharacter tokens, Word Versus Subword Versus Character Versus Byte\\nTokens\\nchat tokens, Phi-3 (and Llama 2)\\nchat-based prompting, Use Case 2: Multimodal Chat-Based Prompting-Use\\nCase 2: Multimodal Chat-Based Prompting\\nChatbot Arena, Human Evaluation\\nChatGPT, Model I/O: Loading Quantized Models with LangChain, Making\\nText Generation Models Multimodal, Reward model training step,\\nTemplating Alignment Data\\nrelease of, An Introduction to Large Language Models\\ntext classification, ChatGPT for Classification-ChatGPT for\\nClassification\\nchatgpt_generation function, ChatGPT for Classification\\nchat_history input variable, Conversation Buffer\\nclassification reports, Using a Task-Specific Model-Using a Task-Specific\\nModel\\nclassification step, embedding model, Supervised Classification\\nCLIP, CLIP: Connecting Text and Images-How Can CLIP Generate\\nMultimodal Embeddings?\\nconnecting text and images, CLIP: Connecting Text and Images', 'generating multimodal embeddings, How Can CLIP Generate\\nMultimodal Embeddings?-How Can CLIP Generate Multimodal\\nEmbeddings?\\nOpenCLIP, OpenCLIP-OpenCLIP\\nclosed-source LLMs, Proprietary, Private Models\\n[CLS] token, Representation Models: Encoder-Only Models, BERT base\\nmodel (uncased) (2018), Creating Contextualized Word Embeddings with\\nLanguage Models, OpenCLIP, SBERT, Transformer-Based Sequential\\nDenoising Auto-Encoder, Preparing Data for Named-Entity Recognition\\ncluster model, Cluster the Reduced Embeddings-Cluster the Reduced\\nEmbeddings\\nclustering (see text clustering)\\nCNNs (convolutional neural networks), Transformers for Vision\\nCohere, Example: Grounded Generation with an LLM API\\nCommand R+, Open Models, Agentic RAG\\ncreating accounts, API Keys, Dense retrieval example\\ngenerating embeddings, Supervised Classification\\nquery rewriting, Query rewriting\\nRerank endpoint, Reranking example\\ncompletion models, Generative Models: Decoder-Only Models\\ncompute_metrics function, Preparing Data for Named-Entity Recognition\\nconfusion matrices, Using a Task-Specific Model\\nCoNLL-2003 dataset, Preparing Data for Named-Entity Recognition', 'constrained sampling, Grammar: Constrained Sampling-Grammar:\\nConstrained Sampling\\ncontext\\nattention and, The attention layer at a glance\\nprompt engineering, The Potential Complexity of a Prompt\\ntraining datasets, The Word2vec Algorithm and Contrastive Training\\ncontext length\\ncompletion models, Generative Models: Decoder-Only Models\\ntoken processing limits, Parallel Token Processing and Context Size\\ncontext window, in completion models, Generative Models: Decoder-Only\\nModels\\ncontrastive learning\\ntext embedding models, What Is Contrastive Learning?-What Is\\nContrastive Learning?, Generating Contrastive Examples\\nword2vec algorithm and, The Word2vec Algorithm and Contrastive\\nTraining-The Word2vec Algorithm and Contrastive Training\\nContrastive Tension (CT), Unsupervised Learning\\nconversation buffer memory, Conversation Summary\\noverview of, Conversation Buffer-Conversation Buffer\\nwindowed, Windowed Conversation Buffer-Windowed Conversation\\nBuffer\\nconversation summary memory, Conversation Summary-Conversation\\nSummary\\nconvert_ids_to_tokens function, OpenCLIP', 'convolutional neural networks (CNNs), Transformers for Vision\\ncosine similarity, What If We Do Not Have Labeled Data?, Cosine\\nsimilarity-Cosine similarity\\nCountVectorizer, BERTopic: A Modular Topic Modeling Framework\\ncross-encoders, How reranking models work\\ncross-entropy loss, Multiple negatives ranking loss\\nCT (Contrastive Tension), Unsupervised Learning\\nD \\ndata outliers, Cluster the Reduced Embeddings\\nDataCollator class, Fine-Tuning a Pretrained BERT Model, Continued\\nPretraining with Masked Language Modeling\\ndatamapplot package, The Text Generation Lego Block\\nDBSCAN (Density-Based Spatial Clustering), Cluster the Reduced\\nEmbeddings\\nDeBERTa, Creating Contextualized Word Embeddings with Language\\nModels, Model Selection\\ndecoder-only models (see generative models)\\ndecoding strategy, Choosing a Single Token from the Probability\\nDistribution (Sampling/Decoding)-Choosing a Single Token from the\\nProbability Distribution (Sampling/Decoding), Summary\\ndense retrieval, Overview of Semantic Search and RAG, Dense Retrieval-\\nFine-tuning embedding models for dense retrieval\\ncaveats of, Caveats of dense retrieval\\nexample of, Dense retrieval example-Search the index', 'fine-tuning embedding models for, Fine-tuning embedding models for\\ndense retrieval\\nnearest neighbor search versus vector databases, Nearest neighbor\\nsearch versus vector databases\\ntext chunking, Chunking long texts-Multiple vectors per document\\ndensity-based algorithms, Cluster the Reduced Embeddings\\nDensity-Based Spatial Clustering (DBSCAN), Cluster the Reduced\\nEmbeddings\\ndimensionality reduction model, Reducing the Dimensionality of\\nEmbeddings-Reducing the Dimensionality of Embeddings\\nDistilBERT, Model Selection, Using a Task-Specific Model\\ndomain adaptation, Using TSDAE for Domain Adaptation\\ndo_sample parameter, Generating Your First Text\\nDPO (Direct Preference Optimization), Training No Reward Model-\\nTraining\\nfine-tuning, Training\\nmodel quantization, Model Quantization\\ntemplating alignment data, Templating Alignment Data\\ntraining configuration, Training Configuration\\nDSPy, Advanced Text Generation Techniques and Tools\\nE \\neasy negatives, Multiple negatives ranking loss\\nElo rating system, Human Evaluation', 'embeddings, Better Representations with Dense Vector Embeddings-Types\\nof Embeddings, Tokens and Embeddings, Token Embeddings-Training a\\nSong Embedding Model\\ndense retrieval, Overview of Semantic Search and RAG, Dense\\nRetrieval-Fine-tuning embedding models for dense retrieval\\nembedding models, defined, Text Classification with Representation\\nModels, Embedding Models\\nmultimodality, Multimodal Embedding Models-OpenCLIP\\nCLIP, CLIP: Connecting Text and Images-How Can CLIP\\nGenerate Multimodal Embeddings?\\nOpenCLIP, OpenCLIP-OpenCLIP\\noverview of, Better Representations with Dense Vector Embeddings-\\nBetter Representations with Dense Vector Embeddings, Embedding\\nModels-Embedding Models\\npositional embeddings, Positional Embeddings (RoPE)-Positional\\nEmbeddings (RoPE)\\nrecommendation systems, Embeddings for Recommendation Systems-\\nTraining a Song Embedding Model\\ntext classification tasks that leverage, Classification Tasks That\\nLeverage Embeddings-What If We Do Not Have Labeled Data?\\nsupervised classification, Supervised Classification-Supervised\\nClassification\\nzero-shot classification, What If We Do Not Have Labeled Data?-\\nWhat If We Do Not Have Labeled Data?\\ntext clustering pipeline, A Common Pipeline for Text Clustering-\\nInspecting the Clusters', 'cluster model, Cluster the Reduced Embeddings-Cluster the\\nReduced Embeddings\\ndimensionality reduction model, Reducing the Dimensionality of\\nEmbeddings-Reducing the Dimensionality of Embeddings\\nembedding model, Embedding Documents\\ninspecting clusters, Inspecting the Clusters-Inspecting the\\nClusters\\ntext embedding models, Text Embeddings (for Sentences and Whole\\nDocuments)-Text Embeddings (for Sentences and Whole Documents),\\nCreating Text Embedding Models-Summary\\ncontrastive learning, What Is Contrastive Learning?-What Is\\nContrastive Learning?\\ncreating, Creating an Embedding Model-Multiple negatives\\nranking loss\\nfine-tuning, Fine-Tuning an Embedding Model-Augmented\\nSBERT\\nSBERT, SBERT-SBERT\\nunsupervised learning, Unsupervised Learning\\ntoken embeddings, Token Embeddings-Creating Contextualized Word\\nEmbeddings with Language Models\\ncreating contextualized word embeddings, Creating\\nContextualized Word Embeddings with Language Models-\\nCreating Contextualized Word Embeddings with Language\\nModels\\ntokenizer’s vocabulary and, A Language Model Holds\\nEmbeddings for the Vocabulary of Its Tokenizer', 'types of, Types of Embeddings\\nword embeddings, Word Embeddings Beyond LLMs-The Word2vec\\nAlgorithm and Contrastive Training\\npretrained, Using pretrained Word Embeddings\\nword2vec algorithm and contrastive training, The Word2vec\\nAlgorithm and Contrastive Training-The Word2vec Algorithm\\nand Contrastive Training\\nencoder-decoder models, Using the Text-to-Text Transfer Transformer\\nencoder-only models (see representation models)\\nethics, validating output, Output Verification\\nexponential backoff, ChatGPT for Classification\\nF \\nF1 score, confusion matrices, Using a Task-Specific Model\\nFAISS, Nearest neighbor search versus vector databases\\nFalcon, Choosing a Text Generation Model\\nfeature extraction step, embedding model, Supervised Classification\\nfeedforward layer, Inside the Transformer Block\\nfeedforward neural networks, Attention Is All You Need, The feedforward\\nneural network at a glance, Summary, Fine-Tuning a Pretrained BERT\\nModel\\nfew-shot classification, Few-Shot Classification-Fine-Tuning for Few-Shot\\nClassification\\nfine-tuning for classification, Fine-Tuning for Few-Shot Classification-\\nFine-Tuning for Few-Shot Classification', 'SetFit, SetFit: Efficient Fine-Tuning with Few Training Examples-\\nSetFit: Efficient Fine-Tuning with Few Training Examples\\nfew-shot prompting, In-Context Learning: Providing Examples-In-Context\\nLearning: Providing Examples, Providing Examples-Providing Examples\\nfind_topics() function, BERTopic, BERTopic: A Modular Topic Modeling\\nFramework\\nfine-tuning\\nembedding models for dense retrieval, Fine-tuning embedding models\\nfor dense retrieval\\ngenerative models, Output Verification, Fine-Tuning Generation\\nModels-Summary\\nevaluating, Evaluating Generative Models-Human Evaluation\\npreference tuning, The Three LLM Training Steps: Pretraining,\\nSupervised Fine-Tuning, and Preference Tuning, Preference-\\nTuning / Alignment / RLHF-Training\\nsupervised fine-tuning, The Three LLM Training Steps:\\nPretraining, Supervised Fine-Tuning, and Preference Tuning-\\nMerge Weights\\ntraining steps, The Three LLM Training Steps: Pretraining,\\nSupervised Fine-Tuning, and Preference Tuning-The Three LLM\\nTraining Steps: Pretraining, Supervised Fine-Tuning, and\\nPreference Tuning\\noverview of, The Training Paradigm of Large Language Models\\nrepresentation models, Fine-Tuning Representation Models for\\nClassification-Summary\\nfew-shot classification, Few-Shot Classification-Fine-Tuning for\\nFew-Shot Classification', 'masked language modeling, Continued Pretraining with Masked\\nLanguage Modeling-Continued Pretraining with Masked\\nLanguage Modeling\\nnamed-entity recognition, Named-Entity Recognition-Fine-\\nTuning for Named-Entity Recognition\\nsupervised classification, Supervised Classification-Freezing\\nLayers\\nT5 model, Using the Text-to-Text Transfer Transformer\\ntext embedding models, Fine-Tuning an Embedding Model-\\nAugmented SBERT\\nAugmented SBERT, Augmented SBERT-Augmented SBERT\\nsupervised, Supervised-Supervised\\nFlan-T5 model, Flan-T5 (2022), Using the Text-to-Text Transfer\\nTransformer-Using the Text-to-Text Transfer Transformer\\nFlash Attention, Flash Attention, Summary\\nforward pass, Summary\\ncomponents of, The Components of the Forward Pass-The\\nComponents of the Forward Pass\\ndefined, The Inputs and Outputs of a Trained Transformer LLM\\nfoundation models, The Year of Generative AI\\nfp16 parameter, Train Model\\nfreezing layers, Train Model, Freezing Layers-Freezing Layers\\nfrozen (nontrainable) models, Text Classification with Representation\\nModels, Supervised Classification, Supervised Classification', 'G \\nĠ symbol, Preprocessing text\\nGalactica, Galactica\\nGeneral Language Understanding Evaluation (GLUE) benchmark,\\nGenerating Contrastive Examples, In-Depth Evaluation, Adapters,\\nBenchmarks\\ngenerated_text variable, Use Case 1: Image Captioning, Use Case 1: Image\\nCaptioning\\ngeneration_output variable, Downloading and Running an LLM\\ngenerative models, Generative Models: Decoder-Only Models-Generative\\nModels: Decoder-Only Models\\nevaluating, Evaluating Generative Models-Human Evaluation\\nautomated evaluation, Automated Evaluation\\nbenchmarks, Benchmarks\\nhuman evaluation, Human Evaluation\\nleaderboards, Leaderboards\\nword-level metrics, Word-Level Metrics\\nfine-tuning, Fine-Tuning Generation Models-Summary\\nevaluation, Evaluating Generative Models-Human Evaluation\\npreference tuning, The Three LLM Training Steps: Pretraining,\\nSupervised Fine-Tuning, and Preference Tuning, Preference-\\nTuning / Alignment / RLHF-Training\\nsupervised fine-tuning, The Three LLM Training Steps:\\nPretraining, Supervised Fine-Tuning, and Preference Tuning-\\nMerge Weights', 'training steps, The Three LLM Training Steps: Pretraining,\\nSupervised Fine-Tuning, and Preference Tuning-The Three LLM\\nTraining Steps: Pretraining, Supervised Fine-Tuning, and\\nPreference Tuning\\nreasoning, Reasoning with Generative Models-Tree-of-Thought:\\nExploring Intermediate Steps\\nself-consistency, Self-Consistency: Sampling Outputs\\ntree-of-thought, Tree-of-Thought: Exploring Intermediate Steps-\\nTree-of-Thought: Exploring Intermediate Steps\\nrepresentation models versus, Representation Models: Encoder-Only\\nModels\\ntext classification, Text Classification with Generative Models-\\nChatGPT for Classification\\nChatGPT, ChatGPT for Classification-ChatGPT for Classification\\nT5, Using the Text-to-Text Transfer Transformer-Using the Text-\\nto-Text Transfer Transformer\\ngenerative pre-trained transformers (see GPTs)\\nGenerative Pseudo-Labeling (GPL), Unsupervised Learning\\nGensim library, Using pretrained Word Embeddings\\nget_topic function, BERTopic: A Modular Topic Modeling Framework\\nget_topic_info() method, BERTopic: A Modular Topic Modeling\\nFramework\\nGGUF model, Grammar: Constrained Sampling, Model I/O: Loading\\nQuantized Models with LangChain\\nGitHub, Using Code Examples', 'GloVe, SBERT\\nGLUE (General Language Understanding Evaluation) benchmark,\\nGenerating Contrastive Examples, In-Depth Evaluation, Adapters,\\nBenchmarks\\ngold datasets, Augmented SBERT\\nGoodhart’s Law, Human Evaluation\\nGoogle Colab, Prerequisites, Hardware and Software Requirements,\\nLimited Resources Are All You Need, Generating Your First Text, Speeding\\nUp Generation by Caching Keys and Values, Training\\nGoogle Gemini, Retrieval-Augmented Generation (RAG)\\nGoogle Search, Semantic Search and Retrieval-Augmented Generation\\nGPL (Generative Pseudo-Labeling), Unsupervised Learning\\nGPT2Tokenizer, Preprocessing text\\nGPTs (generative pre-trained transformers), Generative Models: Decoder-\\nOnly Models, Prompt Engineering\\n(see also text generation)\\nGPT-1, Generative Models: Decoder-Only Models\\nGPT-2, An Introduction to Large Language Models, Generative\\nModels: Decoder-Only Models, Word Versus Subword Versus\\nCharacter Versus Byte Tokens, GPT-2 (2019)\\nGPT-3, Generative Models: Decoder-Only Models, The feedforward\\nneural network at a glance, Local/sparse attention, Low-Rank\\nAdaptation (LoRA)\\nGPT-3.5, The Year of Generative AI, ChatGPT for Classification, A\\nSingle Link in the Chain: Prompt Template, ReAct in LangChain', 'GPT-4, The Year of Generative AI, Proprietary, Private Models, How\\nTokenizers Prepare the Inputs to the Language Model, GPT-4 (2023),\\nThe feedforward neural network at a glance\\nGPUs\\nFlash Attention, Flash Attention\\nrequirements, Hardware and Software Requirements, Limited\\nResources Are All You Need\\nSRAM and HBM, Flash Attention\\ngrammar, Output Verification, Grammar: Constrained Sampling-Grammar:\\nConstrained Sampling\\ngreedy decoding, Choosing a Single Token from the Probability\\nDistribution (Sampling/Decoding)\\ngrounded generation, From Search to RAG-Example: Grounded Generation\\nwith an LLM API\\ngrouped-query attention, Multi-query and grouped-query attention-\\nOptimizing attention: From multi-head to multi-query to grouped query,\\nSummary\\nGSM8k, Benchmarks\\nGuardrails, Grammar: Constrained Sampling\\nGuidance, Grammar: Constrained Sampling\\nH \\nhallucination\\navoiding in in instruction-based prompting, Instruction-Based\\nPrompting', 'text generation models, Semantic Search and Retrieval-Augmented\\nGeneration\\nhard negatives, Multiple negatives ranking loss\\nharmful content, generating, Responsible LLM Development and Usage\\nHaystack, Advanced Text Generation Techniques and Tools\\nHBM (high bandwidth memory), Flash Attention\\nHDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications\\nwith Noise), Cluster the Reduced Embeddings, BERTopic: A Modular Topic\\nModeling Framework\\nHellaSwag, Benchmarks\\nhigh bandwidth memory (HBM), Flash Attention\\nHugging Face, Generating Your First Text, The Sentiment of Movie\\nReviews\\ncreating accounts, API Keys\\nevaluate package, Preparing Data for Named-Entity Recognition\\ntokenizers, The domain of the data\\nhuman evaluation, Inspecting the Clusters, Human Evaluation\\nHumanEval, Benchmarks\\nhybrid search, Caveats of dense retrieval, Reranking example\\nI \\nIdefics 2, BLIP-2: Bridging the Modality Gap\\nimages (see multimodality)', 'in-context learning, In-Context Learning: Providing Examples-In-Context\\nLearning: Providing Examples\\nindexes, Building the search index\\nInfoNCE, Multiple negatives ranking loss\\ninput_ids variable, Downloading and Running an LLM\\ninstruction-based prompting, Instruction-Based Prompting-Instruction-\\nBased Prompting\\nintellectual property, Responsible LLM Development and Usage\\nintuition-first philosophy, An Intuition-First Philosophy\\ninvoke function, A Single Link in the Chain: Prompt Template\\nJ \\nJupyter, Speeding Up Generation by Caching Keys and Values\\nK \\nk-means algorithm, Cluster the Reduced Embeddings, BERTopic: A\\nModular Topic Modeling Framework, BERTopic: A Modular Topic\\nModeling Framework\\nKeyBERTInspired, KeyBERTInspired, Maximal marginal relevance\\nkeyword search\\nreranking, Reranking example-Reranking example\\nverifying semantic search with, Search the index\\nkv (keys and values) cache, Speeding Up Generation by Caching Keys and\\nValues-Speeding Up Generation by Caching Keys and Values', 'L \\nLangChain, Advanced Text Generation Techniques and Tools\\n(see also chains)\\nloading quantized models with, Model I/O: Loading Quantized Models\\nwith LangChain-Model I/O: Loading Quantized Models with\\nLangChain\\nReAct in, ReAct in LangChain-ReAct in LangChain\\nLanguage AI (Language Artificial Intelligence), An Introduction to Large\\nLanguage Models-The Year of Generative AI\\ndefining, What Is Language AI?\\nrecent history of, A Recent History of Language AI-The Year of\\nGenerative AI\\nattention, Encoding and Decoding Context with Attention-\\nAttention Is All You Need\\nbag-of-words model, Representing Language as a Bag-of-Words-\\nRepresenting Language as a Bag-of-Words\\nembeddings, Better Representations with Dense Vector\\nEmbeddings-Types of Embeddings\\ngenerative models, Generative Models: Decoder-Only Models-\\nGenerative Models: Decoder-Only Models\\nrepresentation models, Representation Models: Encoder-Only\\nModels-Representation Models: Encoder-Only Models\\nYear of Generative AI, The Year of Generative AI-The Year of\\nGenerative AI\\nlanguage modeling, The Three LLM Training Steps: Pretraining,\\nSupervised Fine-Tuning, and Preference Tuning', 'language modeling head (LM head), The Components of the Forward Pass-\\nThe Components of the Forward Pass\\nlarge language models (see LLMs)\\nlatent Dirichlet allocation, From Text Clustering to Topic Modeling\\nLayerNorm, The Transformer Block\\nleaderboards, in generative model evaluation, Leaderboards\\nlearning_rate parameter, Training Configuration\\nLlama, Choosing a Text Generation Model\\nLlama 2, API Keys, Limited Resources Are All You Need, Phi-3 (and\\nLlama 2), Multi-query and grouped-query attention, Choosing a Text\\nGeneration Model, Making Text Generation Models Multimodal\\nllama-cpp-python library, Grammar: Constrained Sampling\\nLLaVA, BLIP-2: Bridging the Modality Gap\\nLLM-as-a-judge, RAG Evaluation\\nLLMs (large language models), An Introduction to Large Language\\nModels-Summary\\ncode examples and exercises, Using Code Examples\\nembeddings, Tokens and Embeddings, Token Embeddings-Training a\\nSong Embedding Model\\nrecommendation systems, Embeddings for Recommendation\\nSystems-Training a Song Embedding Model\\ntext embeddings, Text Embeddings (for Sentences and Whole\\nDocuments)-Text Embeddings (for Sentences and Whole\\nDocuments)', 'token embeddings, Token Embeddings-Creating Contextualized\\nWord Embeddings with Language Models\\nword embeddings, Word Embeddings Beyond LLMs-The\\nWord2vec Algorithm and Contrastive Training\\nfine-tuning generative models, Fine-Tuning Generation Models-\\nSummary\\nevaluation, Evaluating Generative Models-Human Evaluation\\npreference tuning, The Three LLM Training Steps: Pretraining,\\nSupervised Fine-Tuning, and Preference Tuning, Preference-\\nTuning / Alignment / RLHF-Training\\nsupervised fine-tuning, The Three LLM Training Steps:\\nPretraining, Supervised Fine-Tuning, and Preference Tuning-\\nMerge Weights\\ntraining steps, The Three LLM Training Steps: Pretraining,\\nSupervised Fine-Tuning, and Preference Tuning-The Three LLM\\nTraining Steps: Pretraining, Supervised Fine-Tuning, and\\nPreference Tuning\\nfine-tuning representation models, Fine-Tuning Representation Models\\nfor Classification-Summary\\nfew-shot classification, Few-Shot Classification-Fine-Tuning for\\nFew-Shot Classification\\nmasked language modeling, Continued Pretraining with Masked\\nLanguage Modeling-Continued Pretraining with Masked\\nLanguage Modeling\\nnamed-entity recognition, Named-Entity Recognition-Fine-\\nTuning for Named-Entity Recognition', 'supervised classification, Supervised Classification-Freezing\\nLayers\\ngenerating text, Generating Your First Text-Generating Your First Text\\ngenerative models, Generative Models: Decoder-Only Models-\\nGenerative Models: Decoder-Only Models\\nhardware and software requirements, Hardware and Software\\nRequirements, Limited Resources Are All You Need\\nhigh-level view, How Tokenizers Prepare the Inputs to the Language\\nModel\\nhistory of Language AI, A Recent History of Language AI-The Year of\\nGenerative AI\\ninterfacing with, Interfacing with Large Language Models-Open\\nSource Frameworks\\nclosed-source models, Proprietary, Private Models\\nopen models, Open Models-Open Source Frameworks\\nintuition-first philosophy, An Intuition-First Philosophy\\nmoving definition of, The Moving Definition of a “Large Language\\nModel”\\nmultimodality, Multimodal Large Language Models-Summary\\nembedding models, Multimodal Embedding Models-OpenCLIP\\ntext generation models, Making Text Generation Models\\nMultimodal-Use Case 2: Multimodal Chat-Based Prompting\\nVision Transformer, Transformers for Vision-Transformers for\\nVision\\nprompt engineering, Prompt Engineering-Summary', 'chain prompting, Chain Prompting: Breaking up the Problem-\\nChain Prompting: Breaking up the Problem\\nin-context learning, In-Context Learning: Providing Examples-In-\\nContext Learning: Providing Examples\\ninstruction-based prompting, Instruction-Based Prompting-\\nInstruction-Based Prompting\\noutput verification, Output Verification-Grammar: Constrained\\nSampling\\npotential complexity of prompts, The Potential Complexity of a\\nPrompt-The Potential Complexity of a Prompt\\nprompt components, The Basic Ingredients of a Prompt-The\\nBasic Ingredients of a Prompt\\nreasoning with generative models, Reasoning with Generative\\nModels-Tree-of-Thought: Exploring Intermediate Steps\\ntext generation models, Using Text Generation Models-top_p\\nrepresentation models, Representation Models: Encoder-Only Models-\\nRepresentation Models: Encoder-Only Models\\nresponsible development and usage of, Responsible LLM\\nDevelopment and Usage\\nretrieval-augmented generation, Overview of Semantic Search and\\nRAG, Retrieval-Augmented Generation (RAG)-RAG Evaluation\\nagentic RAG, Agentic RAG\\nconverting search system to, From Search to RAG\\nevaluating results, RAG Evaluation\\ngrounded generation, Example: Grounded Generation with an\\nLLM API', 'multi-hop RAG, Multi-hop RAG\\nmulti-query RAG, Multi-query RAG\\nquery rewriting, Query rewriting\\nquery routing, Query routing\\nwith local models, Example: RAG with Local Models, The RAG\\nprompt\\nsemantic search, Semantic Search and Retrieval-Augmented\\nGeneration-Scoring across multiple queries with mean average\\nprecision\\ndense retrieval, Overview of Semantic Search and RAG, Dense\\nRetrieval-Fine-tuning embedding models for dense retrieval\\nreranking, Overview of Semantic Search and RAG, Reranking-\\nHow reranking models work\\nretrieval evaluation metrics, Retrieval Evaluation Metrics-Scoring\\nacross multiple queries with mean average precision\\ntext classification, Text Classification-Summary\\nwith generative models, Text Classification with Generative\\nModels-ChatGPT for Classification\\nmovie reviews, The Sentiment of Movie Reviews, The Sentiment\\nof Movie Reviews\\nwith representation models, Text Classification with\\nRepresentation Models-What If We Do Not Have Labeled Data?\\ntext clustering, Text Clustering and Topic Modeling-Inspecting the\\nClusters\\ntext embedding models, Creating Text Embedding Models-Summary', 'contrastive learning, What Is Contrastive Learning?-What Is\\nContrastive Learning?\\ncreating, Creating an Embedding Model-Multiple negatives\\nranking loss\\nfine-tuning, Fine-Tuning an Embedding Model-Augmented\\nSBERT\\nSBERT, SBERT-SBERT\\nunsupervised learning, Unsupervised Learning\\ntext generation, Advanced Text Generation Techniques and Tools-\\nSummary\\nagents, Agents: Creating a System of LLMs-ReAct in LangChain\\nchains, Chains: Extending the Capabilities of LLMs-A Chain with\\nMultiple Prompts\\nmemory of conversations, Memory: Helping LLMs to Remember\\nConversations-Conversation Summary\\nmodel I/O, Model I/O: Loading Quantized Models with\\nLangChain-Model I/O: Loading Quantized Models with\\nLangChain\\ntokens and tokenizers, Tokens and Embeddings-Creating\\nContextualized Word Embeddings with Language Models\\ncomparing trained tokenizers, Comparing Trained LLM\\nTokenizers-Phi-3 (and Llama 2)\\ndownloading and running LLMs, Downloading and Running an\\nLLM-Downloading and Running an LLM\\ninput preparation, How Tokenizers Prepare the Inputs to the\\nLanguage Model', 'text breakdown, How Does the Tokenizer Break Down Text?\\ntoken embeddings, Token Embeddings-Creating Contextualized\\nWord Embeddings with Language Models\\ntokenization schemes, Word Versus Subword Versus Character\\nVersus Byte Tokens-Word Versus Subword Versus Character\\nVersus Byte Tokens\\ntokenizer properties, Tokenizer Properties-The domain of the data\\ntopic modeling, Text Clustering and Topic Modeling, From Text\\nClustering to Topic Modeling-The Text Generation Lego Block\\ntraining paradigm of, The Training Paradigm of Large Language\\nModels-The Training Paradigm of Large Language Models\\nTransformer architecture, Looking Inside Large Language Models-\\nSummary\\ndecoding strategy, Choosing a Single Token from the Probability\\nDistribution (Sampling/Decoding)-Choosing a Single Token from\\nthe Probability Distribution (Sampling/Decoding)\\nforward pass components, The Components of the Forward Pass-\\nThe Components of the Forward Pass\\ninputs and outputs of, The Inputs and Outputs of a Trained\\nTransformer LLM-The Inputs and Outputs of a Trained\\nTransformer LLM\\nkeys and values cache, Speeding Up Generation by Caching Keys\\nand Values-Speeding Up Generation by Caching Keys and Values\\nparallel token processing and context size, Parallel Token\\nProcessing and Context Size-Parallel Token Processing and\\nContext Size', 'recent improvements to, Recent Improvements to the Transformer\\nArchitecture-Other Architectural Experiments and Improvements\\nTransformer blocks, Inside the Transformer Block-Self-attention:\\nCombining information\\nutility of, Large Language Model Applications: What Makes Them So\\nUseful?\\nLM head (language modeling head), The Components of the Forward Pass-\\nThe Components of the Forward Pass\\nLMQL, Grammar: Constrained Sampling\\nlocal attention, Local/sparse attention\\nLoRA (low-rank adaptation), Low-Rank Adaptation (LoRA)-Low-Rank\\nAdaptation (LoRA), Model Quantization\\n(see also QLoRA)\\nlora_alpha parameter, LoRA Configuration\\nloss functions, Loss Functions-Multiple negatives ranking loss\\ncosine similarity loss, Cosine similarity-Cosine similarity\\nmultiple negatives ranking loss, Multiple negatives ranking loss-\\nMultiple negatives ranking loss\\nlr_scheduler_type parameter, Training Configuration\\nM \\nMamba, The Year of Generative AI\\nMAP (mean average precision), Retrieval Evaluation Metrics-Scoring\\nacross multiple queries with mean average precision\\nMarginMSE loss, Loss Functions', 'masked language modeling (MLM), Continued Pretraining with Masked\\nLanguage Modeling-Continued Pretraining with Masked Language\\nModeling\\nmask_token [MASK], BERT base model (uncased) (2018)\\nMassive Text Embedding Benchmark (MTEB), Model Selection,\\nEmbedding Documents, Loading the embedding model, In-Depth\\nEvaluation\\nmatplotlib library, Inspecting the Clusters\\nmaximal marginal relevance (MMR), Maximal marginal relevance\\nmax_new_tokens parameter, Generating Your First Text\\nMcCarthy, John, What Is Language AI?\\nmean average precision (MAP)\\nmemory of conversations, Memory: Helping LLMs to Remember\\nConversations-Conversation Summary\\nconversation buffer, Conversation Buffer-Conversation Buffer\\nconversation summary, Conversation Summary-Conversation\\nSummary\\nwindowed conversation buffer, Windowed Conversation Buffer-\\nWindowed Conversation Buffer\\nMeta Llama model, Open Models\\nMicrosoft Bing, Semantic Search and Retrieval-Augmented Generation,\\nReranking\\nMicrosoft Bing AI, Retrieval-Augmented Generation (RAG)\\nMicrosoft Phi model, Open Models\\nmicrosoft/mpnet-base model, Train Model', 'min_cluster_size parameter, Cluster the Reduced Embeddings\\nmin_dist parameter, Reducing the Dimensionality of Embeddings\\nMIRACL, Reranking example\\nMistral, Open Models, Choosing a Text Generation Model, BLIP-2:\\nBridging the Modality Gap\\nMLM (masked language modeling), Continued Pretraining with Masked\\nLanguage Modeling-Continued Pretraining with Masked Language\\nModeling\\nMMLU, Benchmarks, Leaderboards\\nMMR (maximal marginal relevance), Maximal marginal relevance\\nMNLI (Multi-Genre Natural Language Inference) corpus, Generating\\nContrastive Examples, Multiple negatives ranking loss\\nMNR (multiple negatives ranking) loss, Multiple negatives ranking loss-\\nMultiple negatives ranking loss\\nmodel I/O, Model I/O: Loading Quantized Models with LangChain-Model\\nI/O: Loading Quantized Models with LangChain\\nmonoBERT, How reranking models work\\nMTEB (Massive Text Embedding Benchmark), Model Selection,\\nEmbedding Documents, Loading the embedding model, In-Depth\\nEvaluation\\nMulti-Genre Natural Language Inference (MNLI) corpus, Generating\\nContrastive Examples, Multiple negatives ranking loss\\nmulti-hop RAG, Multi-hop RAG\\nmulti-query attention, Multi-query and grouped-query attention-Optimizing\\nattention: From multi-head to multi-query to grouped query', 'multi-query RAG, Multi-query RAG\\nmultilevel perceptrons, The Components of the Forward Pass\\n(see also feedforward neural networks)\\nmultimodality, Multimodal Large Language Models-Summary\\ndefined, Multimodal Large Language Models\\nembedding models, Multimodal Embedding Models-OpenCLIP\\nCLIP, CLIP: Connecting Text and Images-How Can CLIP\\nGenerate Multimodal Embeddings?\\nOpenCLIP, OpenCLIP-OpenCLIP\\ntext generation models, Making Text Generation Models Multimodal-\\nUse Case 2: Multimodal Chat-Based Prompting\\nBLIP-2, BLIP-2: Bridging the Modality Gap-BLIP-2: Bridging\\nthe Modality Gap\\nchat-based prompting, Use Case 2: Multimodal Chat-Based\\nPrompting-Use Case 2: Multimodal Chat-Based Prompting\\nimage captioning, Use Case 1: Image Captioning-Use Case 1:\\nImage Captioning\\npreprocessing images, Preprocessing images\\npreprocessing text, Preprocessing text\\nVision Transformer, Transformers for Vision-Transformers for Vision\\nmultiple negatives ranking (MNR) loss, Multiple negatives ranking loss-\\nMultiple negatives ranking loss\\nN \\nnamed-entity recognition (see NER)', 'natural language inference (NLI), Generating Contrastive Examples\\nnatural language processing (NLP), What Is Language AI?, What Is\\nContrastive Learning?\\nnDCG (normalized discounted cumulative gain), Reranking example,\\nScoring across multiple queries with mean average precision\\nnearest neighbor search\\npretrained word embeddings, Using pretrained Word Embeddings\\nrecommendation system embeddings, Recommending Songs by\\nEmbeddings\\nvector databases versus, Nearest neighbor search versus vector\\ndatabases\\nnegative sampling, The Word2vec Algorithm and Contrastive Training\\nNER (named-entity recognition), Named-Entity Recognition-Fine-Tuning\\nfor Named-Entity Recognition, Adapters\\nfine-tuning for, Fine-Tuning for Named-Entity Recognition\\npreparing data for, Preparing Data for Named-Entity Recognition-\\nPreparing Data for Named-Entity Recognition\\nneural networks, Better Representations with Dense Vector Embeddings\\nNLI (natural language inference), Generating Contrastive Examples\\nNLP (natural language processing), What Is Language AI?, What Is\\nContrastive Learning?\\nnoise-contrastive estimation, The Word2vec Algorithm and Contrastive\\nTraining\\nnonplayable characters (NPCs), What Is Language AI?', 'nontrainable (frozen) models, Text Classification with Representation\\nModels, Supervised Classification, Supervised Classification\\nnormalization, Transformer block, The Transformer Block\\nnormalized discounted cumulative gain (nDCG), Reranking example,\\nScoring across multiple queries with mean average precision\\nNPCs (nonplayable characters), What Is Language AI?\\nNTXentLoss, Multiple negatives ranking loss\\nnucleus sampling, top_p\\nNumPy, Nearest neighbor search versus vector databases\\nnum_train_epochs parameter, Train Model, Training Configuration\\nNVIDIA GPUs, Hardware and Software Requirements, Generating Your\\nFirst Text\\nn_components parameter, Reducing the Dimensionality of Embeddings\\nO \\nOdds Ratio Preference Optimization (ORPO), Training\\none-shot prompting, In-Context Learning: Providing Examples\\nchain-of-thought versus, Chain-of-Thought: Think Before Answering\\nin-context learning, In-Context Learning: Providing Examples\\nOpen LLM Leaderboard, Model I/O: Loading Quantized Models with\\nLangChain, Leaderboards\\nopen-source LLMs, Open Models-Open Source Frameworks\\nOpenAI, ChatGPT for Classification\\n(see also ChatGPT; GPTs)', 'creating accounts, API Keys, ChatGPT for Classification\\ngenerating embeddings, Supervised Classification\\nOpenCLIP, OpenCLIP-OpenCLIP\\noptim parameter, Training Configuration\\nORPO (Odds Ratio Preference Optimization), Training\\noutput verification, Output Verification-Grammar: Constrained Sampling\\nconstrained sampling, Grammar: Constrained Sampling-Grammar:\\nConstrained Sampling\\nproviding examples, Providing Examples\\nP \\npad_token [PAD], BERT base model (uncased) (2018)\\nparallel processing, Attention is all you need, Summary\\nparallel prompts, Chain Prompting: Breaking up the Problem\\nPCA (Principal Component Analysis), Reducing the Dimensionality of\\nEmbeddings\\nPEFT (parameter-efficient fine-tuning), Parameter-Efficient Fine-Tuning\\n(PEFT)-Compressing the model for (more) efficient training\\nadapters, Adapters-Adapters\\ncompression, Compressing the model for (more) efficient training-\\nCompressing the model for (more) efficient training\\nLoRA, Low-Rank Adaptation (LoRA)-Low-Rank Adaptation (LoRA)\\npeft library, LoRA Configuration\\npeft_config parameter, LoRA Configuration', 'Perplexity, Retrieval-Augmented Generation (RAG)\\npersona, in text-generation prompts, The Potential Complexity of a Prompt\\nper_device_eval_batch_size argument, Train Model\\nper_device_train_batch_size argument, Train Model\\nPhi-3\\ncomparing to other trained tokenizers, Phi-3 (and Llama 2)\\nforward pass, The Components of the Forward Pass\\nloading quantized models, Model I/O: Loading Quantized Models with\\nLangChain\\nprompt template, A Single Link in the Chain: Prompt Template\\nquantization, Model I/O: Loading Quantized Models with LangChain\\nPhi-3-mini, Generating Your First Text, Choosing a Text Generation Model\\nPinecone, Nearest neighbor search versus vector databases\\npositional embeddings, Positional Embeddings (RoPE)-Positional\\nEmbeddings (RoPE)\\nPPO (Proximal Policy Optimization), Reward model training step\\nprecision predictions, confusion matrices, Using a Task-Specific Model\\npredictions, task-specific model, Using a Task-Specific Model\\npreference tuning, ChatGPT for Classification, The Three LLM Training\\nSteps: Pretraining, Supervised Fine-Tuning, and Preference Tuning,\\nPreference-Tuning / Alignment / RLHF-Training\\nDirect Preference Optimization, Training No Reward Model-Training\\nfine-tuning, Training\\nmodel quantization, Model Quantization', 'templating alignment data, Templating Alignment Data\\ntraining configuration, Training Configuration\\nreward models, Automating Preference Evaluation Using Reward\\nModels-Reward model training step\\ninputs and outputs of, The Inputs and Outputs of a Reward Model\\ntraining, Training a Reward Model-Reward model training step\\npretraining, defined, The Training Paradigm of Large Language Models\\nprimacy effect, Instruction-Based Prompting\\nPrincipal Component Analysis (PCA), Reducing the Dimensionality of\\nEmbeddings\\nprojection matrices, How attention is calculated\\nprompt engineering, Text Classification with Generative Models, Prompt\\nEngineering-Summary\\nchain prompting, Chain Prompting: Breaking up the Problem-Chain\\nPrompting: Breaking up the Problem\\nin-context learning, In-Context Learning: Providing Examples-In-\\nContext Learning: Providing Examples\\ninstruction-based prompting, Instruction-Based Prompting-Instruction-\\nBased Prompting\\noutput verification, Output Verification-Grammar: Constrained\\nSampling\\nconstrained sampling, Grammar: Constrained Sampling-\\nGrammar: Constrained Sampling\\nproviding examples, Providing Examples', 'potential complexity of prompts, The Potential Complexity of a\\nPrompt-The Potential Complexity of a Prompt\\nprompt components, The Basic Ingredients of a Prompt-The Basic\\nIngredients of a Prompt\\nreasoning with generative models, Reasoning with Generative Models-\\nTree-of-Thought: Exploring Intermediate Steps\\ntext generation models, Using Text Generation Models-top_p\\nchoosing, Choosing a Text Generation Model\\ncontrolling output, Controlling Model Output-top_p\\nloading, Loading a Text Generation Model-Loading a Text\\nGeneration Model\\nProximal Policy Optimization (PPO), Reward model training step\\nPython, learning about, Prerequisites\\nQ \\nQ-Former (Querying Transformer), BLIP-2: Bridging the Modality Gap-\\nBLIP-2: Bridging the Modality Gap\\nQ8 model, Model I/O: Loading Quantized Models with LangChain\\nQLoRA (quantized low-rank adaptation), Instruction Tuning with QLoRA-\\nMerge Weights\\nfine-tuning, Training\\nLoRA configuration, LoRA Configuration\\nmerging weights, Merge Weights\\nmodel quantization, Model Quantization\\ntemplating instruction data, Templating Instruction Data', 'training configuration, Training Configuration\\nquantization, Model I/O: Loading Quantized Models with LangChain,\\nCompressing the model for (more) efficient training-Compressing the\\nmodel for (more) efficient training\\nquantization_config parameter, LoRA Configuration\\nquantized low-rank adaptation (see QLoRA)\\n Querying Transformer (Q-Former), BLIP-2: Bridging the Modality Gap-\\nBLIP-2: Bridging the Modality Gap\\nR \\nr parameter, LoRA Configuration\\nRAG (retrieval-augmented generation), Token Embeddings, Text\\nEmbeddings (for Sentences and Whole Documents), Overview of Semantic\\nSearch and RAG, Retrieval-Augmented Generation (RAG)-RAG\\nEvaluation\\nagentic RAG, Agentic RAG\\nbasic pipeline, Retrieval-Augmented Generation (RAG)\\nconverting search system to, From Search to RAG\\nevaluating results, RAG Evaluation\\ngrounded generation, Example: Grounded Generation with an LLM\\nAPI\\nwith local models, Example: RAG with Local Models-The RAG\\nprompt\\nmulti-hop RAG, Multi-hop RAG\\nmulti-query RAG, Multi-query RAG', 'query rewriting, Query rewriting\\nquery routing, Query routing\\nRagas, RAG Evaluation\\nrandom_state parameter, Reducing the Dimensionality of Embeddings\\nrate limit errors, ChatGPT for Classification\\nReAct\\nin LangChain, ReAct in LangChain-ReAct in LangChain\\nstep-by-step reasoning, The Driving Power Behind Agents: Step-by-\\nstep Reasoning\\nreasoning\\nwith generative models, Reasoning with Generative Models-Tree-of-\\nThought: Exploring Intermediate Steps\\nchain-of-thought, Chain-of-Thought: Think Before Answering-\\nChain-of-Thought: Think Before Answering\\nself-consistency, Self-Consistency: Sampling Outputs\\ntree-of-thought, Tree-of-Thought: Exploring Intermediate Steps-\\nTree-of-Thought: Exploring Intermediate Steps\\nstep-by-step, The Driving Power Behind Agents: Step-by-step\\nReasoning, The Driving Power Behind Agents: Step-by-step\\nReasoning\\nrecall predictions, confusion matrices, Using a Task-Specific Model\\nrecency effect, Instruction-Based Prompting\\nrecommendation systems, Embeddings for Recommendation Systems-\\nTraining a Song Embedding Model', 'recurrent neural networks (RNNs), Encoding and Decoding Context with\\nAttention\\nreduce_outliers() function, BERTopic: A Modular Topic Modeling\\nFramework\\nregulation, Responsible LLM Development and Usage\\nrelevance scoring, Attention is all you need, How attention is calculated-\\nSelf-attention: Relevance scoring, Positional Embeddings (RoPE)\\nrepository, Hardware and Software Requirements\\nrepresentation models, Representation Models: Encoder-Only Models-\\nRepresentation Models: Encoder-Only Models\\ndefined, Representing Language as a Bag-of-Words\\nfine-tuning for classification, Fine-Tuning Representation Models for\\nClassification-Summary\\nfew-shot classification, Few-Shot Classification-Fine-Tuning for\\nFew-Shot Classification\\nmasked language modeling, Continued Pretraining with Masked\\nLanguage Modeling-Continued Pretraining with Masked\\nLanguage Modeling\\nnamed-entity recognition, Named-Entity Recognition-Fine-\\nTuning for Named-Entity Recognition\\nsupervised classification, Supervised Classification-Freezing\\nLayers\\ngenerative models versus, Representation Models: Encoder-Only\\nModels\\ntext classification, Text Classification with Representation Models-\\nWhat If We Do Not Have Labeled Data?', 'classification tasks that leverage embeddings, Classification Tasks\\nThat Leverage Embeddings-What If We Do Not Have Labeled\\nData?\\nmodel selection, Model Selection-Model Selection\\ntask-specific models, Using a Task-Specific Model\\nrepresentation_model parameter, The Text Generation Lego Block\\nreranking, Overview of Semantic Search and RAG, Reranking-How\\nreranking models work\\nBERTopic, Adding a Special Lego Block\\nexample of, Reranking example-Reranking example\\nfunction of reranking models, How reranking models work\\nsentence transformers, Open source retrieval and reranking with\\nsentence transformers\\nresponse validation, in chain prompting, Chain Prompting: Breaking up the\\nProblem\\nretrieval evaluation metrics, Retrieval Evaluation Metrics-Scoring across\\nmultiple queries with mean average precision\\nscoring multiple queries with mean average precision, Scoring across\\nmultiple queries with mean average precision\\nscoring single queries with average precision, Scoring a single query\\nwith average precision\\nretrieval-augmented generation (see RAG)\\nreturn_full_text parameter, Generating Your First Text\\nreward models, Automating Preference Evaluation Using Reward Models-\\nReward model training step', 'inputs and outputs of, The Inputs and Outputs of a Reward Model\\ntraining, Training a Reward Model-Reward model training step\\nRMSNorm, The Transformer Block\\nRNNs (recurrent neural networks), Encoding and Decoding Context with\\nAttention\\nRoBERTa, Word Versus Subword Versus Character Versus Byte Tokens,\\nModel Selection\\nRoPE (rotary positional embeddings), Positional Embeddings (RoPE)-\\nPositional Embeddings (RoPE)\\nRorschach test, Use Case 1: Image Captioning\\nRotten Tomatoes dataset, The Sentiment of Movie Reviews, Fine-Tuning a\\nPretrained BERT Model\\nROUGE, Word-Level Metrics\\nRWKV, The Year of Generative AI\\nS \\nSBERT, SBERT-SBERT, Augmented SBERT-Augmented SBERT\\nself-attention, Attention Is All You Need, Self-attention: Relevance scoring-\\nSelf-attention: Combining information\\nself-consistency, Self-Consistency: Sampling Outputs\\nsemantic search, Semantic Search and Retrieval-Augmented Generation-\\nScoring across multiple queries with mean average precision\\ndefined, Semantic Search and Retrieval-Augmented Generation\\ndense retrieval, Overview of Semantic Search and RAG, Dense\\nRetrieval-Fine-tuning embedding models for dense retrieval', 'caveats of, Caveats of dense retrieval\\nexample of, Dense retrieval example-Search the index\\nfine-tuning embedding models for, Fine-tuning embedding\\nmodels for dense retrieval\\nnearest neighbor search versus vector databases, Nearest neighbor\\nsearch versus vector databases\\ntext chunking, Chunking long texts-Multiple vectors per\\ndocument\\nreranking, Overview of Semantic Search and RAG, Reranking-How\\nreranking models work\\nexample of, Reranking example-Reranking example\\nfunction of reranking models, How reranking models work\\nsentence transformers, Open source retrieval and reranking with\\nsentence transformers\\nretrieval evaluation metrics, Retrieval Evaluation Metrics-Scoring\\nacross multiple queries with mean average precision\\nscoring multiple queries with mean average precision, Scoring\\nacross multiple queries with mean average precision\\nscoring single queries with average precision, Scoring a single\\nquery with average precision\\nSemantic Textual Similarity Benchmark (STSB), Train Model\\nsemi-hard negatives, Multiple negatives ranking loss\\nsentence-transformers, Text Embeddings (for Sentences and Whole\\nDocuments), Supervised Classification, BERTopic: A Modular Topic\\nModeling Framework, Open source retrieval and reranking with sentence', 'transformers, OpenCLIP, SBERT-SBERT, Supervised, SetFit: Efficient\\nFine-Tuning with Few Training Examples\\nSentencePiece, Flan-T5 (2022)\\n[SEP] token, BERT base model (uncased) (2018), Creating Contextualized\\nWord Embeddings with Language Models, Preparing Data for Named-\\nEntity Recognition\\nsequence-to-sequence models, Text Classification with Generative Models,\\nUsing the Text-to-Text Transfer Transformer\\nSetFit, Fine-Tuning Representation Models for Classification, SetFit:\\nEfficient Fine-Tuning with Few Training Examples-SetFit: Efficient Fine-\\nTuning with Few Training Examples\\nSFT (supervised fine-tuning), The Three LLM Training Steps: Pretraining,\\nSupervised Fine-Tuning, and Preference Tuning-Merge Weights\\nfull fine-tuning, Full Fine-Tuning\\nparameter-efficient fine-tuning, Parameter-Efficient Fine-Tuning\\n(PEFT)-Compressing the model for (more) efficient training\\nadapters, Adapters-Adapters\\ncompression, Compressing the model for (more) efficient\\ntraining-Compressing the model for (more) efficient training\\nLoRA, Low-Rank Adaptation (LoRA)-Low-Rank Adaptation\\n(LoRA)\\nQLoRA, Instruction Tuning with QLoRA-Merge Weights\\nfine-tuning, Training\\nLoRA configuration, LoRA Configuration\\nmerging weights, Merge Weights\\nmodel quantization, Model Quantization', 'templating instruction data, Templating Instruction Data\\ntraining configuration, Training Configuration\\nshared memory (SRAM), Flash Attention\\nshortlisting, Reranking example\\nsilver datasets, Augmented SBERT\\nSimCSE (Simple Contrastive Learning of Sentence Embeddings),\\nUnsupervised Learning\\nskip-gram, The Word2vec Algorithm and Contrastive Training\\nsoftmax loss function, Loss Functions\\nsong recommendation systems, Embeddings for Recommendation Systems-\\nTraining a Song Embedding Model\\nsparse attention, Local/sparse attention\\nspecial tokens, Comparing Trained LLM Tokenizers, Tokenizer parameters\\nspecificity, in instruction-based prompting, Instruction-Based Prompting\\nSRAM (shared memory), Flash Attention\\nStableLM, Choosing a Text Generation Model\\nStarCoder2, StarCoder2 (2024)\\nstep-by-step reasoning, The Driving Power Behind Agents: Step-by-step\\nReasoning-The Driving Power Behind Agents: Step-by-step Reasoning\\nstructured output, validating, Output Verification\\nSTSB (Semantic Textual Similarity Benchmark), Train Model\\nsubword tokens, Word Versus Subword Versus Character Versus Byte\\nTokens', 'supervised classification, Supervised Classification-Supervised\\nClassification\\nfine-tuning representation models for, Supervised Classification-\\nFreezing Layers\\nfreezing layers, Freezing Layers-Freezing Layers\\npretrained BERT models, Fine-Tuning a Pretrained BERT Model-\\nFine-Tuning a Pretrained BERT Model\\nsupervised fine-tuning (see SFT)\\nsystem 1 and 2 thinking processes, Reasoning with Generative Models\\nT \\nT5 (Text-to-Text Transfer Transformer), Using the Text-to-Text Transfer\\nTransformer-Using the Text-to-Text Transfer Transformer\\ntarget_modules parameter, LoRA Configuration\\ntask-specific models, Text Classification with Representation Models\\ntemperature parameter, Temperature-top_p, Self-Consistency: Sampling\\nOutputs\\nTesla T4, Training\\ntest splits, The Sentiment of Movie Reviews, Using a Task-Specific Model\\ntext chunking, Getting the text archive and chunking it, Chunking long\\ntexts-Multiple vectors per document\\napproaches for, Multiple vectors per document\\nmultiple vectors per document, Multiple vectors per document\\none vector per document, One vector per document\\ntext classification, Text Classification-Summary', 'with generative models, Text Classification with Generative Models-\\nChatGPT for Classification\\nChatGPT, ChatGPT for Classification-ChatGPT for Classification\\nT5, Using the Text-to-Text Transfer Transformer-Using the Text-\\nto-Text Transfer Transformer\\nmovie reviews, The Sentiment of Movie Reviews-The Sentiment of\\nMovie Reviews\\nwith representation models, Text Classification with Representation\\nModels-What If We Do Not Have Labeled Data?\\nclassification tasks that leverage embeddings, Classification Tasks\\nThat Leverage Embeddings-What If We Do Not Have Labeled\\nData?\\nmodel selection, Model Selection-Model Selection\\ntask-specific models, Using a Task-Specific Model-Using a Task-\\nSpecific Model\\ntext clustering, Text Clustering and Topic Modeling-Inspecting the Clusters\\nCLIP embedding model and, CLIP: Connecting Text and Images\\ncommon pipeline for, A Common Pipeline for Text Clustering-\\nInspecting the Clusters\\ncluster model, Cluster the Reduced Embeddings-Cluster the\\nReduced Embeddings\\ndimensionality reduction model, Reducing the Dimensionality of\\nEmbeddings-Reducing the Dimensionality of Embeddings\\nembedding model, Embedding Documents\\ninspecting clusters, Inspecting the Clusters-Inspecting the\\nClusters', 'text embedding models, Text Embeddings (for Sentences and Whole\\nDocuments)-Text Embeddings (for Sentences and Whole Documents),\\nCreating Text Embedding Models-Summary\\ncontrastive learning, What Is Contrastive Learning?-What Is\\nContrastive Learning?\\ncreating, Creating an Embedding Model-Multiple negatives ranking\\nloss\\nevaluating, In-Depth Evaluation\\ngenerating contrastive examples, Generating Contrastive\\nExamples\\nloss functions, Loss Functions-Multiple negatives ranking loss\\ntraining, Train Model-Train Model\\nfine-tuning, Fine-Tuning an Embedding Model\\nAugmented SBERT, Augmented SBERT-Augmented SBERT\\nsupervised, Supervised-Supervised\\nSBERT, SBERT-SBERT\\nunsupervised learning, Unsupervised Learning\\ntext generation, Generating Your First Text-Generating Your First Text,\\nAdvanced Text Generation Techniques and Tools-Summary\\nagents, Agents: Creating a System of LLMs-ReAct in LangChain\\nReAct in LangChain, ReAct in LangChain-ReAct in LangChain\\nstep-by-step reasoning, The Driving Power Behind Agents: Step-\\nby-step Reasoning-The Driving Power Behind Agents: Step-by-\\nstep Reasoning', 'chains, Chains: Extending the Capabilities of LLMs-A Chain with\\nMultiple Prompts\\nchaining single prompt, A Single Link in the Chain: Prompt\\nTemplate-A Single Link in the Chain: Prompt Template\\nsequential chaining of multiple prompts, A Chain with Multiple\\nPrompts-A Chain with Multiple Prompts\\nmemory of conversations, Memory: Helping LLMs to Remember\\nConversations-Conversation Summary\\nconversation buffer, Conversation Buffer-Conversation Buffer\\nconversation summary, Conversation Summary-Conversation\\nSummary\\nwindowed conversation buffer, Windowed Conversation Buffer-\\nWindowed Conversation Buffer\\nmodel I/O, Model I/O: Loading Quantized Models with LangChain-\\nModel I/O: Loading Quantized Models with LangChain\\nmultimodality, Making Text Generation Models Multimodal-Use Case\\n2: Multimodal Chat-Based Prompting\\nBLIP-2, BLIP-2: Bridging the Modality Gap-BLIP-2: Bridging\\nthe Modality Gap\\nchat-based prompting, Use Case 2: Multimodal Chat-Based\\nPrompting-Use Case 2: Multimodal Chat-Based Prompting\\nimage captioning, Use Case 1: Image Captioning-Use Case 1:\\nImage Captioning\\npreprocessing images, Preprocessing images\\npreprocessing text, Preprocessing text\\nprompt engineering, Using Text Generation Models-top_p', 'choosing models, Choosing a Text Generation Model\\ncontrolling output, Controlling Model Output-top_p\\nloading models, Loading a Text Generation Model-Loading a\\nText Generation Model\\ntopic modeling, The Text Generation Lego Block-The Text Generation\\nLego Block\\ntext-in-text-out model, The Inputs and Outputs of a Trained Transformer\\nLLM\\nText-to-Text Transfer Transformer (T5), Using the Text-to-Text Transfer\\nTransformer-Using the Text-to-Text Transfer Transformer\\nthenlper/gte-small model, Embedding Documents\\n%%timeit magic command, Speeding Up Generation by Caching Keys and\\nValues\\nTinyLlama, Instruction Tuning with QLoRA, Preference Tuning with DPO\\ntokenization-free encoding, Word Versus Subword Versus Character Versus\\nByte Tokens\\ntokens and tokenizers, Generating Your First Text, Tokens and Embeddings-\\nCreating Contextualized Word Embeddings with Language Models\\nbag-of-words model, Representing Language as a Bag-of-Words\\ncomparing trained tokenizers, Comparing Trained LLM Tokenizers-\\nPhi-3 (and Llama 2)\\nBERT base model (cased), BERT base model (cased) (2018)\\nBERT base model (uncased), BERT base model (uncased) (2018)\\nFlan-T5, Flan-T5 (2022)\\nGalactica, Galactica', 'GPT-2, GPT-2 (2019)\\nGPT-4, GPT-4 (2023)\\nPhi-3 and Llama 2, Phi-3 (and Llama 2)\\nStarCoder2, StarCoder2 (2024)\\ndecoding strategy, Choosing a Single Token from the Probability\\nDistribution (Sampling/Decoding)-Choosing a Single Token from the\\nProbability Distribution (Sampling/Decoding)\\ndownloading and running LLMs, Downloading and Running an LLM-\\nDownloading and Running an LLM\\nforward pass, The Components of the Forward Pass-The Components\\nof the Forward Pass\\ninput preparation, How Tokenizers Prepare the Inputs to the Language\\nModel\\nmasked language modeling, Using the Text-to-Text Transfer\\nTransformer\\nparallel token processing and context size, Parallel Token Processing\\nand Context Size-Parallel Token Processing and Context Size\\nspecial tokens, Comparing Trained LLM Tokenizers\\ntask-specific representation model, Using a Task-Specific Model, Text\\nClassification with Generative Models\\ntext breakdown, How Does the Tokenizer Break Down Text?\\ntext-focused versus code-focused models, The domain of the data\\ntoken embeddings, Token Embeddings-Creating Contextualized Word\\nEmbeddings with Language Models, The Components of the Forward\\nPass, Summary', 'creating contextualized word embeddings, Creating\\nContextualized Word Embeddings with Language Models-\\nCreating Contextualized Word Embeddings with Language\\nModels\\ntokenizer’s vocabulary and, A Language Model Holds\\nEmbeddings for the Vocabulary of Its Tokenizer\\ntoken spans, Using the Text-to-Text Transfer Transformer\\ntokenization schemes, Word Versus Subword Versus Character Versus\\nByte Tokens-Word Versus Subword Versus Character Versus Byte\\nTokens\\nbyte tokens, Word Versus Subword Versus Character Versus Byte\\nTokens\\ncharacter tokens, Word Versus Subword Versus Character Versus\\nByte Tokens\\nsubword tokens, Word Versus Subword Versus Character Versus\\nByte Tokens\\nword tokens, Word Versus Subword Versus Character Versus Byte\\nTokens\\ntokenizer properties, Tokenizer Properties-The domain of the data\\ndatasets, The domain of the data\\nmethods, Tokenization methods\\nparameters, Tokenizer parameters\\nwhite space characters, GPT-2 (2019)\\ntone of voice, in text-generation prompts, The Potential Complexity of a\\nPrompt', 'topic modeling, Text Clustering and Topic Modeling, From Text Clustering\\nto Topic Modeling-The Text Generation Lego Block\\nBERTopic, BERTopic: A Modular Topic Modeling Framework-\\nBERTopic: A Modular Topic Modeling Framework\\nrepresentation blocks, Adding a Special Lego Block-The Text\\nGeneration Lego Block\\ntop_k parameter, top_p\\ntop_p parameter, top_p, Self-Consistency: Sampling Outputs\\ntrain splits, The Sentiment of Movie Reviews\\nTrainingArguments class, Fine-Tuning a Pretrained BERT Model\\ntransfer learning, Representation Models: Encoder-Only Models\\nTransformer architecture, Attention Is All You Need-Attention Is All You\\nNeed, Looking Inside Large Language Models-Summary\\nattention layer, The Components of the Forward Pass, Inside the\\nTransformer Block\\ndecoding strategy, Choosing a Single Token from the Probability\\nDistribution (Sampling/Decoding)-Choosing a Single Token from the\\nProbability Distribution (Sampling/Decoding)\\nfeedforward layer, Inside the Transformer Block\\nforward pass components, The Components of the Forward Pass-The\\nComponents of the Forward Pass\\ninputs and outputs of, The Inputs and Outputs of a Trained\\nTransformer LLM-The Inputs and Outputs of a Trained Transformer\\nLLM\\nkeys and values cache, Speeding Up Generation by Caching Keys and\\nValues-Speeding Up Generation by Caching Keys and Values', 'optimizing attention, Optimizing attention: From multi-head to multi-\\nquery to grouped query\\nparallel token processing and context size, Parallel Token Processing\\nand Context Size-Parallel Token Processing and Context Size\\nrecent improvements to, Recent Improvements to the Transformer\\nArchitecture-Other Architectural Experiments and Improvements\\nmore efficient attention, More Efficient Attention-Flash Attention\\npositional embeddings, Positional Embeddings (RoPE)-Positional\\nEmbeddings (RoPE)\\nTransformer blocks, The Transformer Block\\nTransformer blocks, Inside the Transformer Block-Self-attention:\\nCombining information\\nattention calculation, How attention is calculated-How attention is\\ncalculated\\nattention layer, The attention layer at a glance\\nattention mechanism, Attention is all you need-Attention is all\\nyou need\\nfeedforward neural networks, The feedforward neural network at\\na glance\\nself-attention and relevance scoring, Self-attention: Relevance\\nscoring-Self-attention: Combining information\\nVision Transformer, Transformers for Vision-Transformers for Vision\\ntransparency and accountability, Responsible LLM Development and Usage\\ntree-of-thought, Tree-of-Thought: Exploring Intermediate Steps-Tree-of-\\nThought: Exploring Intermediate Steps', 'TruthfulQA, Benchmarks, Leaderboards\\nTSDAE (Transformer-Based Sequential Denoising Auto-Encoder)\\nfor domain adaptation, Using TSDAE for Domain Adaptation\\noverview of, Transformer-Based Sequential Denoising Auto-Encoder-\\nTransformer-Based Sequential Denoising Auto-Encoder\\nU \\nUltraChat dataset, Templating Instruction Data\\nUMAP (Uniform Manifold Approximation and Projection), Reducing the\\nDimensionality of Embeddings\\nunigram language model, Flan-T5 (2022)\\nunk_token [UNK], BERT base model (uncased) (2018)\\nuse_cache parameter, Speeding Up Generation by Caching Keys and Values\\nV \\nvalid output, verifying, Output Verification\\nvalidation splits, The Sentiment of Movie Reviews\\nvector databases\\ndense retrieval, Dense Retrieval\\nnearest neighbor search versus, Nearest neighbor search versus vector\\ndatabases\\nretrieval-augmented generation, Loading the embedding model\\nvideo random-access memory (VRAM), Hardware and Software\\nRequirements, Limited Resources Are All You Need\\nvisualization', 'BERTopic, BERTopic: A Modular Topic Modeling Framework\\ncluster analysis, Inspecting the Clusters\\ndimensionality reduction and, Inspecting the Clusters\\nViT (Vision Transformer), Transformers for Vision-Transformers for Vision,\\nBLIP-2: Bridging the Modality Gap\\nvocabulary, of tokenizers, Tokenizer parameters, A Language Model Holds\\nEmbeddings for the Vocabulary of Its Tokenizer, The Components of the\\nForward Pass, Summary\\nVRAM (video random-access memory), Hardware and Software\\nRequirements, Limited Resources Are All You Need\\nW \\nwarmup_ratio parameter, Training Configuration\\nwarmup_steps argument, Train Model\\nWeaviate, Nearest neighbor search versus vector databases\\nwhitespace characters, GPT-2 (2019)\\nwindowed conversation buffer memory, Windowed Conversation Buffer-\\nWindowed Conversation Buffer, Conversation Summary\\nword embeddings, Word Embeddings Beyond LLMs-The Word2vec\\nAlgorithm and Contrastive Training\\npretrained, Using pretrained Word Embeddings\\nword2vec algorithm and contrastive training, The Word2vec Algorithm\\nand Contrastive Training-The Word2vec Algorithm and Contrastive\\nTraining\\nword tokens, Word Versus Subword Versus Character Versus Byte Tokens', 'word-level metrics, in generative model evaluation, Word-Level Metrics\\nword2vec algorithm, Better Representations with Dense Vector\\nEmbeddings, Types of Embeddings-Encoding and Decoding Context with\\nAttention\\ncontrastive training and, The Word2vec Algorithm and Contrastive\\nTraining-The Word2vec Algorithm and Contrastive Training, What Is\\nContrastive Learning?\\nembedding songs, Recommending Songs by Embeddings\\nWordPiece, How Does the Tokenizer Break Down Text?\\ncased BERT base model, BERT base model (cased) (2018)\\nuncased BERT base model, BERT base model (uncased) (2018)\\n<work> token, Galactica\\nY \\nYear of Generative AI, The Year of Generative AI-The Year of Generative\\nAI\\nZ \\nzero-shot classification, What If We Do Not Have Labeled Data?-What If\\nWe Do Not Have Labeled Data?\\nCLIP, CLIP: Connecting Text and Images\\nSetFit, Fine-Tuning for Few-Shot Classification\\nzero-shot prompting\\nchain-of-thought, Chain-of-Thought: Think Before Answering\\nin-context learning, In-Context Learning: Providing Examples', 'OceanofPDF.com', 'About the Authors\\nJay Alammar is Director and Engineering Fellow at Cohere (pioneering\\nprovider of large language models as an API). In this role, he advises and\\neducates enterprises and the developer community on using language\\nmodels for practical use cases. Through his popular AI/ML blog, Jay has\\nhelped millions of researchers and engineers visually understand machine\\nlearning tools and concepts from the basic (ending up in the documentation\\nof packages like NumPy and pandas) to the cutting-edge (Transformers,\\nBERT, GPT-3, Stable Diffusion). Jay is also a co-creator of popular\\nmachine learning and natural language processing courses on\\nDeeplearning.ai and Udacity.\\nMaarten Grootendorst is a Senior Clinical Data Scientist at IKNL\\n(Netherlands Comprehensive Cancer Organization). He holds master’s\\ndegrees in organizational psychology, clinical psychology, and data science,\\nwhich he leverages to communicate complex machine learning concepts to\\na wide audience. With his popular blogs, he has reached millions of readers\\nby explaining the fundamentals of artificial intelligence—often from a\\npsychological point of view. He is the author and maintainer of several open\\nsource packages that rely on the strength of large language models, such as\\nBERTopic, PolyFuzz, and KeyBERT. His packages are downloaded\\nmillions of times and used by data professionals and organizations\\nworldwide.\\nOceanofPDF.com', 'Colophon \\nThe animal on the cover of Hands-On Large Language Models is a red\\nkangaroo (Osphranter rufus). They are the largest of all kangaroos, with a\\nbody length that can get up to a little over 5 feet and a tail as long as 3 feet.\\nThey are very fast and can hop to speeds over 35 miles per hour. They can\\njump 6 feet high and leap a distance of 25 feet in a single bound. The\\nposition of their eyes allows them see up to 300 degrees.\\nRed kangaroos are named after the color of their fur. While the name makes\\nsense for the males—they have short, red-brown fur—females are typically\\nmore of a blue-grey color with a tinge of brown throughout. The red color\\nin their fur comes from a red oil excreted from the glands in their skin.\\nBecause of their color, Australians refer to male red kangaroos as “big\\nreds.” However, because females are faster than males, they are often called\\n“blue fliers.”\\nPreferring open, dry areas with some trees for shade, red kangaroos can be\\nfound across Australia’s mainland except in the upper north, lower\\nsouthwest, and east coast regions of the country. Surrounding\\nenvironmental conditions can affect reproduction. Because of this, females\\ncan pause or postpone pregnancy or birth until conditions are better. They\\noften use this ability to delay birth of a new baby (joey) until the previous\\none has left their pouch.\\nThe cover illustration is by Karen Montgomery, based on an antique line\\nengraving from Cassell’s Popular Natural History. The series design is by\\nEdie Freedman, Ellie Volckhausen, and Karen Montgomery. The cover\\nfonts are Gilroy Semibold and Guardian Sans. The text font is Adobe\\nMinion Pro; the heading font is Adobe Myriad Condensed; and the code\\nfont is Dalton Maag’s Ubuntu Mono.\\nOceanofPDF.com']\n"
     ]
    }
   ],
   "source": [
    "print(page_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2c5e2a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "# # Create a MarkdownTextSplitter for semantic chunking\n",
    "# semantic_splitter = MarkdownTextSplitter(\n",
    "#     chunk_size=200,    # Number of characters per chunk\n",
    "#     chunk_overlap=50   # Overlap to preserve context\n",
    "# )\n",
    "\n",
    "# # Split the text into semantic chunks\n",
    "# semantic_chunks = semantic_splitter.split_text(page_texts)\n",
    "# print(\"\\nSemantic Chunks (Markdown-based):\\n\", semantic_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ccef48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #now embeddings for the text\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# embed_model=SentenceTransformer(\"\")\n",
    "\n",
    "# text_embeddings=embed_model.encode(page_texts)\n",
    "\n",
    "# index_embeddings=embed_model.encode(page_ids)\n",
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "embed_model=HuggingFaceBgeEmbeddings(model_name='thenlper/gte-small')\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "metadata= [{\"page\":int(i)} for i in page_ids]\n",
    "database= FAISS.from_texts(page_texts, embed_model, metadatas=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0f2bc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#query rewriting\n",
    "# def querewrite(query):\n",
    "rewtemp=(\n",
    "        \"You are an AI assistant. Rewrite the following user query into a clear, concise search query suitable for retrieving relevant documents. \"\n",
    "        \"User Query:+ {query} \\nRewritten Query:\"\n",
    "    )\n",
    "rewprompt= PromptTemplate(template= rewtemp, input_variables=['query'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9b90191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1=  rewprompt | lafilamod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e934d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"],\n",
    "     template=\"{page_content}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a70db602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import RegexParser\n",
    "parser= RegexParser(\n",
    "    regex=r\"answer: (.*?)\\nScore:(.*)\",\n",
    "    output_keys=[\"answer\",\"score\"],\n",
    ")\n",
    "# template1=\"\"\"\n",
    "# <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "# you are a search retrieval agent that answers precisely to the query.the format needs to be like:\n",
    "# Answer: <some answer>\n",
    "# Score: <number between 0 and 1>\n",
    "# <|eot_id|>\n",
    "# <|start_header_id|>user<|end_header_id|>\n",
    "# provide a relevant answer according to the user query:\n",
    "# {context}\n",
    "# the format needs to be like:\n",
    "# Answer: <some answer>\n",
    "# Score: <number between 0 and 1>\n",
    "\n",
    "# <|eod_id|>\n",
    "# <|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# rag_prompt = PromptTemplate.from_template(\"\"\"\n",
    "# Given the following document and a question, provide a helpful answer and a score between 0 and 1 indicating how well the document answers the question.\n",
    "\n",
    "# Document:\n",
    "# {context}\n",
    "\n",
    "# Question:\n",
    "# {question}\n",
    "\n",
    "# Answer the question and rate the relevance.\n",
    "\n",
    "# Output format:\n",
    "# Answer: <your answer>\n",
    "# Score: <score between 0 and 1>\n",
    "# \"\"\")\n",
    "template1map=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an intelligent assistant helping to answer a user's question using a provided document.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "provide a relevant answer according to the user query:\n",
    "{question}.\n",
    "\n",
    "Document: {summaries}\n",
    "\n",
    "the format needs to be like:\n",
    "Answer: <some answer>\n",
    "Score: <number between 0 and 1>\n",
    "\n",
    "<|eod_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "template1combine=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a smart assistant combining multiple partial answers from different documents to answer a user’s question.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "question:{question}\n",
    "partial answers: {summaries}\n",
    "\n",
    "based on the above construct a concrete final answer for the user:\n",
    "<|eod_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "map_prompt= PromptTemplate(template=template1map, input_variables=[ \"question\",\"summaries\"])\n",
    "combine_prompt= PromptTemplate(template=template1combine, input_variables=[\"question\", \"summaries\"])\n",
    "document_variable_name=\"summaries\"\n",
    "# from langchain.chains import RetrievalQA\n",
    "# rag= RetrievalQA.from_chain_type(llm=lafilamod, chain_type=\"map_rerank\", retriever= database.as_retriever(), chain_type_kwargs={'prompt': rag_prompt},output_parser=parser, verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "90a911ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA, LLMChain, MapReduceDocumentsChain, StuffDocumentsChain, ReduceDocumentsChain\n",
    "\n",
    "# rag= RetrievalQA.from_chain_type(llm=lafilamod, chain_type=\"map_reduce\", retriever= database.as_retriever(),chain_type_kwargs={'map_prompt': map_prompt,\"combine_prompt\": combine_prompt})\n",
    "\n",
    "llm_chain_map= LLMChain(llm=lafilamod, prompt= map_prompt)\n",
    "llm_chain_combine=LLMChain(llm=lafilamod, prompt=combine_prompt)\n",
    "\n",
    "combine_docschain=StuffDocumentsChain(llm_chain=llm_chain_combine,     document_prompt=document_prompt,\n",
    "document_variable_name=document_variable_name)\n",
    "\n",
    "reduce_doc_chain=ReduceDocumentsChain(combine_documents_chain=combine_docschain)\n",
    "\n",
    "map_reduce_chain= MapReduceDocumentsChain(\n",
    "    llm_chain= llm_chain_map,\n",
    "    reduce_documents_chain= reduce_doc_chain,\n",
    "    document_variable_name=document_variable_name\n",
    ")\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag= RetrievalQA( retriever= database.as_retriever(), combine_documents_chain= map_reduce_chain )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bae54fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain2= rag | chain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a1531b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a rewritten version of your query that can help retrieve relevant documents:\n",
      "\n",
      "**Search Query:** \"Large Language Model (LLM) Agent Definition\"\n",
      "\n",
      "This rewritten query uses specific keywords and phrases to help search engines or databases quickly identify relevant documents. The use of parentheses around \"Large Language Model\" helps to clarify the meaning of LLM, which is a type of AI model.\n"
     ]
    }
   ],
   "source": [
    "rag_response=chain2.invoke(\"what is an llm agent?\")\n",
    "print(rag_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d5a3daeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ql/hdbyk3lx6vq0x6fs9227fc_m0000gn/T/ipykernel_50628/3518605215.py:15: ResourceWarning: unclosed <socket.socket fd=83, family=2, type=1, proto=6, laddr=('127.0.0.1', 57614), raddr=('127.0.0.1', 11434)>\n",
      "  ragstuff= RetrievalQA.from_chain_type(llm=lafilamod, chain_type=\"stuff\", retriever= database.as_retriever(), chain_type_kwargs={'prompt': stuff_prompt, 'document_variable_name': 'context'}, verbose=True,)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "template1=\"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "you are a search retrieval agent that answers precisely to the query. \n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "provide a relevant answer according to the user query:\n",
    "{question}\n",
    "Relevant text:\n",
    "{context}\n",
    "\n",
    "<|eod_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "stuff_prompt=PromptTemplate(template= template1, input_variables=[\"question\",\"context\"])\n",
    "ragstuff= RetrievalQA.from_chain_type(llm=lafilamod, chain_type=\"stuff\", retriever= database.as_retriever(), chain_type_kwargs={'prompt': stuff_prompt, 'document_variable_name': 'context'}, verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c1efdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(question):\n",
    "    response= ragstuff.invoke(question)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cdfd8b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'what is an llm agent, how to use it for search retrieval',\n",
       " 'result': '**What is an LLM Agent?**\\n\\nAn LLM (Large Language Model) agent is a system that leverages a language model to determine its actions and make decisions. It can interact with the real world through the use of tools, allowing it to perform tasks beyond what an LLM can do in isolation.\\n\\n**How to Use an LLM Agent for Search Retrieval**\\n\\nTo use an LLM agent for search retrieval, you can follow these steps:\\n\\n1. **Create a tool**: Define a tool that the agent can use to interact with the outside world. For example, you can create a tool that allows the agent to access a web search engine like DuckDuckGo.\\n2. **Prepare tools**: Load the tools into an array and pass them to the LLM agent.\\n3. **Create the ReAct agent**: Use the `create_react_agent` function from the LangChain library to create an instance of the ReAct agent.\\n4. **Pass the agent to the AgentExecutor**: Pass the ReAct agent to the `AgentExecutor` class, which handles executing the steps.\\n\\nHere\\'s an example code snippet:\\n```python\\nfrom langchain.agents import AgentExecutor, create_react_agent\\n\\n# Create a tool for searching the web\\nsearch = DuckDuckGoSearchResults()\\nsearch_tool = Tool(\\n    name=\"duckduck\",\\n    description=\"A web search engine. Use this to as a search engine for general queries.\",\\n    func=search.run,\\n)\\n\\n# Prepare tools\\ntools = load_tools([\"llm-math\"], llm=openai_llm)\\ntools.append(search_tool)\\n\\n# Create the ReAct agent and pass it to the AgentExecutor\\nagent = create_react_agent()\\nagent_executor = AgentExecutor(agent, tools)\\n\\n# Use the agent to search for something\\ninput_text = \"What is the price of a MacBook Pro?\"\\noutput = agent_executor.execute(input_text)\\nprint(output)  # Output: $2,249.00 (approximately 1911.65 EUR with an exchange rate of 0.85 EUR for 1 USD.)\\n```\\nIn this example, the LLM agent uses the search tool to find the price of a MacBook Pro and returns the result.\\n\\n**Query Routing**\\n\\nTo enhance the search retrieval capabilities of the LLM agent, you can implement query routing. This involves specifying which data sources the model should use based on the type of question being asked. For example, if the question is about HR information, the model should search the company\\'s HR information system (e.g., Notion), and if the question is about customer data, it should search the customer relationship management (CRM) system (e.g., Salesforce).\\n\\n**RAG Evaluation**\\n\\nTo evaluate the performance of an LLM agent, you can use metrics such as fluency, relevance, and accuracy. The \"Evaluating verifiability in generative search engines\" paper provides a framework for evaluating RAG models along four axes: fluency, relevance, accuracy, and novelty.\\n\\nI hope this helps! Let me know if you have any further questions or need more information on how to use LLM agents for search retrieval.'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(\"what is an llm agent, how to use it for search retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f9f90de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding conversation summary to the llm now\n",
    "##but do we need memory??##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faad4cb",
   "metadata": {},
   "source": [
    "what we need is. a better retrieval logic that very accurately takes on to the right page(end goal) \n",
    "but right now we need that **at least the top 2** has the right one in the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0d3cd9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 356\n",
      "Content: Query routing\n",
      "An additional enhancement is to give the model the ability to search\n",
      "multiple data sources. We can, for example, specify for the model that if it\n",
      "gets a question about HR, it should search the company’s HR information\n",
      "system (e.g., Notion) but if the question is about customer data, that it\n",
      "should search the customer relationship management (CRM) (e.g.,\n",
      "Salesforce).\n",
      "Agentic RAG\n",
      "You may be able to now see that the list of previous enhancements slowly\n",
      "delegates more and more responsibility to the LLM to solve more and more\n",
      "complex problems. This relies on the LLM’s capability to gauge the\n",
      "required information needs as well as its ability to utilize multiple data\n",
      "sources. This new nature of the LLM starts to become closer and closer to\n",
      "an agent that acts on the world. The data sources can also now be abstracted\n",
      "into tools. We saw, for example, that we can search Notion, but by the same\n",
      "token, we should be able to post to Notion as well.\n",
      "Not all LLMs will have the RAG capabilities mentioned here. At the time\n",
      "of writing, likely only the largest managed models may be able to attempt\n",
      "this behavior. Thankfully, Cohere’s Command R+ excels at these tasks and\n",
      "is available as an open-weights model as well.\n",
      "RAG Evaluation\n",
      "There are still ongoing developments in how to evaluate RAG models. A\n",
      "good paper to read on this topic is “Evaluating verifiability in generative\n",
      "search engines” (2023), which runs human evaluations on different\n",
      "generative search systems.2 \n",
      "It evaluates results along four axes:\n",
      "Fluency\n",
      "Whether the generated text is fluent and cohesive.\n",
      "\n",
      "Page: 318\n",
      "Content: Figure 8-3. A RAG system formulates an answer to a question and (preferably) cites its information\n",
      "sources.\n",
      "The rest of the chapter covers these three types of systems in more detail.\n",
      "While these are the major categories, they are not the only LLM\n",
      "applications in the domain of search.\n",
      "Semantic Search with Language Models\n",
      "Let’s now dive into more detail on the major categories of systems that can\n",
      "upgrade the search capabilities of our language models. We’ll start with\n",
      "dense retrieval and then move on through reranking and RAG.\n",
      "Dense Retrieval\n",
      "Recall that embeddings turn text into numeric representations. Those can be\n",
      "thought of as points in space, as we can see in Figure 8-4. Points that are\n",
      "close together mean that the text they represent is similar. So in this\n",
      "example, text 1 and text 2 are more similar to each other (because they are\n",
      "near each other) than text 3 (because it’s farther away).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#current retrieval not great but fast\n",
    "results = database.similarity_search(\"agentic RAG\", k=2)\n",
    "for doc in results:\n",
    "    page = doc.metadata.get(\"page\")\n",
    "    print(f\"Page: {page}\\nContent: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7a602d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query={'query':\"what is agentic rag\"}\n",
    "actual_query=chain1.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1690e53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a rewritten version of the user query that can help retrieve relevant documents:\n",
      "\n",
      "**Search Query:** \"definition of agentic role\" OR \"characteristics of agentic behavior\"\n",
      "\n",
      "This rewritten query uses specific keywords and phrases to capture the essence of the original question. By using terms like \"definition\", \"role\", and \"behavior\", we can search for documents that provide a clear explanation of what an \"agentic rag\" is, or related concepts that describe its characteristics.\n"
     ]
    }
   ],
   "source": [
    "print(actual_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4be7c2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 329\n",
      "Content: you train a retrieval model on internet and Wikipedia data, and then deploy\n",
      "it on legal texts (without having enough legal data as part of the training\n",
      "set), the model will not work as well in that legal domain.\n",
      "The final thing we’d like to point out is that this is a case where each\n",
      "sentence contained a piece of information, and we showed queries that\n",
      "specifically ask for that information. What about questions whose answers\n",
      "span multiple sentences? This highlights one of the important design\n",
      "parameters of dense retrieval systems: what is the best way to chunk long\n",
      "texts? And why do we need to chunk them in the first place?\n",
      "Chunking long texts\n",
      "One limitation of Transformer language models is that they are limited in\n",
      "context sizes, meaning we cannot feed them very long texts that go above\n",
      "the number of words or tokens that the model supports. So how do we\n",
      "embed long texts?\n",
      "There are several possible ways, and two possible approaches shown in\n",
      "Figure 8-7 include indexing one vector per document and indexing multiple\n",
      "vectors per document.\n",
      "Figure 8-7. It’s possible to create one vector representing an entire document, but it’s better for\n",
      "longer documents to be split into smaller chunks that get their own embeddings.\n",
      "\n",
      "Page: 320\n",
      "Content: Figure 8-5. Dense retrieval relies on the property that search queries will be close to their relevant\n",
      "results.\n",
      "Judging by the distances in Figure 8-5, “text 2” is the best result for this\n",
      "query, followed by “text 1.” Two questions could arise here, however:\n",
      "Should text 3 even be returned as a result? That’s a decision for\n",
      "you, the system designer. It’s sometimes desirable to have a max\n",
      "threshold of similarity score to filter out irrelevant results (in case\n",
      "the corpus has no relevant results for the query).\n",
      "Are a query and its best result semantically similar? Not always.\n",
      "This is why language models need to be trained on question-\n",
      "answer pairs to become better at retrieval. This process is\n",
      "explained in more detail in Chapter 10.\n",
      "Figure 8-6 shows how we chunk a document before proceeding to embed\n",
      "each chunk. Those embedding vectors are then stored in the vector database\n",
      "and are ready for retrieval.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#current retrieval not great but fast\n",
    "results = database.similarity_search(\"actual_query\", k=2)\n",
    "for doc in results:\n",
    "    page = doc.metadata.get(\"page\")\n",
    "    print(f\"Page: {page}\\nContent: {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7d23966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing keyword search\n",
    "# from sklearn.feature_extraction import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from tqdm import tqdm\n",
    "def tokenizer(text):\n",
    "    tokenizer_corpus=[]\n",
    "    for text in text.lower().split():\n",
    "        text=text.strip(string.punctuation)\n",
    "        if len(text)>0 and text not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenizer_corpus.append(text)\n",
    "    return tokenizer_corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ee4ef7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/598 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 598/598 [00:00<00:00, 29981.16it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_doc=[]\n",
    "for passage in tqdm(page_texts):\n",
    "    tokenized_doc.append(tokenizer(passage))\n",
    "\n",
    "\n",
    "bm25= BM25Okapi(tokenized_doc) #notice how naming it tokenize_doc=... will give error later \n",
    "def keyword_search(query, top_k=5, num_candidates=10):\n",
    "    # print(\"input question:\", query)\n",
    "\n",
    "    bm_25_scores= bm25.get_scores(tokenizer(query))\n",
    "    bm_25_scores= bm_25_scores / (bm_25_scores.max() + 1e-9) #adding a max normalization layer for values between 0 and 1\n",
    "\n",
    "    top_n= np.argpartition(bm_25_scores, -num_candidates)[-num_candidates:]\n",
    "    bm25_hits= [{'corpus_id': idx , 'score':bm_25_scores[idx]} for idx in top_n]\n",
    "    # bm25_hits= [{ idx ,bm_25_scores[idx]} for idx in top_n]\n",
    "\n",
    "    bm25_hits=sorted(bm25_hits, key= lambda x: x['score'], reverse=True)\n",
    "\n",
    "    scores_only=[]\n",
    "\n",
    "    # print(\"top 5 results\")\n",
    "    # for hit in bm25_hits[0:top_k]:\n",
    "    #     print(\"\\t{:.3f}\\t\".format(hit['score']),\n",
    "    #     page_texts[hit['corpus_id']].replace(\"\\n\",\" \")) # fetch the content then replace line breaks with space\n",
    "\n",
    "    #     scores_only.append(hit['score'])\n",
    "    # print(scores_only)\n",
    "    return  [(hit['corpus_id'], hit['score']) for hit in bm25_hits]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c8014aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(575, 0.999999999921749),\n",
       " (561, 0.8969531239724363),\n",
       " (536, 0.8716653374604687),\n",
       " (356, 0.6711819067485029),\n",
       " (542, 0.5235863667105769),\n",
       " (562, 0.45513670097404435),\n",
       " (348, 0.4118913815791562),\n",
       " (354, 0.37692730331059543),\n",
       " (318, 0.3381180448580582),\n",
       " (353, 0.32935187669117916)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "keyword_search(\"what is agentic rag\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "6a28fa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(575, 0.999999999921749),\n",
       " (561, 0.8969531239724363),\n",
       " (536, 0.8716653374604687),\n",
       " (356, 0.6711819067485029),\n",
       " (542, 0.5235863667105769),\n",
       " (562, 0.45513670097404435),\n",
       " (348, 0.4118913815791562),\n",
       " (354, 0.37692730331059543),\n",
       " (318, 0.3381180448580582),\n",
       " (353, 0.32935187669117916)]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_scores_with_id=keyword_search(\"what is agentic Rag?\")\n",
    "keyword_scores_with_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef5257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e8d7359",
   "metadata": {},
   "source": [
    "**langchain stores the text in doc.page_content and metadata in .metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8f76ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is chatgpt code-- not needed\n",
    "\n",
    "# from rank_bm25 import BM25Okapi\n",
    "# from sklearn.feature_extraction import _stop_words\n",
    "# import string\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "\n",
    "# # Tokenizer\n",
    "# def tokenizer(text):\n",
    "#     tokens = []\n",
    "#     for t in text.lower().split():\n",
    "#         t = t.strip(string.punctuation)\n",
    "#         if len(t) > 0:  # no stopword filtering unless you want it\n",
    "#             tokens.append(t)\n",
    "#     return tokens\n",
    "\n",
    "# # Tokenize corpus\n",
    "# tokenized_doc = [tokenizer(p) for p in tqdm(page_texts)]\n",
    "\n",
    "# # Fit BM25\n",
    "# bm25 = BM25Okapi(tokenized_doc)\n",
    "\n",
    "# def keyword_search(query, min_score=0.0):\n",
    "#     query_tokens = tokenizer(query)\n",
    "#     scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "#     # Keep all matches above min_score\n",
    "#     hits = [\n",
    "#         {\"idx\": idx, \"score\": score, \"text\": page_texts[idx]}\n",
    "#         for idx, score in enumerate(scores)\n",
    "#         if score > min_score\n",
    "#     ]\n",
    "    \n",
    "#     # Sort by score (highest first)\n",
    "#     hits = sorted(hits, key=lambda x: x[\"score\"], reverse=True)\n",
    "    \n",
    "#     return hits\n",
    "\n",
    "# # Example\n",
    "# results = keyword_search(\"langchain\", min_score=0)\n",
    "# for hit in results:\n",
    "#     print(f\"Score: {hit['score']:.3f} | {hit['text'][:80]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7478b68a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "13850c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'page-number': 356, 'score': 0.8307425764322219, 'text': 'Query routing\\nAn additional enhancement is to give the model the ability to search\\nmultiple data sources. We can, for example, specify for the model that if it\\ngets a question about HR, it should search the company’s HR information\\nsystem (e.g., Notion) but if the question is about customer data, that it\\nshould search the customer relationship management (CRM) (e.g.,\\nSalesforce).\\nAgentic RAG\\nYou may be able to now see that the list of previous enhancements slowly\\ndelegates more and more responsibility to the LLM to solve more and more\\ncomplex problems. This relies on the LLM’s capability to gauge the\\nrequired information needs as well as its ability to utilize multiple data\\nsources. This new nature of the LLM starts to become closer and closer to\\nan agent that acts on the world. The data sources can also now be abstracted\\ninto tools. We saw, for example, that we can search Notion, but by the same\\ntoken, we should be able to post to Notion as well.\\nNot all LLMs will have the RAG capabilities mentioned here. At the time\\nof writing, likely only the largest managed models may be able to attempt\\nthis behavior. Thankfully, Cohere’s Command R+ excels at these tasks and\\nis available as an open-weights model as well.\\nRAG Evaluation\\nThere are still ongoing developments in how to evaluate RAG models. A\\ngood paper to read on this topic is “Evaluating verifiability in generative\\nsearch engines” (2023), which runs human evaluations on different\\ngenerative search systems.2 \\nIt evaluates results along four axes:\\nFluency\\nWhether the generated text is fluent and cohesive.'}, {'page-number': 318, 'score': 0.8224408695341826, 'text': 'Figure 8-3. A RAG system formulates an answer to a question and (preferably) cites its information\\nsources.\\nThe rest of the chapter covers these three types of systems in more detail.\\nWhile these are the major categories, they are not the only LLM\\napplications in the domain of search.\\nSemantic Search with Language Models\\nLet’s now dive into more detail on the major categories of systems that can\\nupgrade the search capabilities of our language models. We’ll start with\\ndense retrieval and then move on through reranking and RAG.\\nDense Retrieval\\nRecall that embeddings turn text into numeric representations. Those can be\\nthought of as points in space, as we can see in Figure 8-4. Points that are\\nclose together mean that the text they represent is similar. So in this\\nexample, text 1 and text 2 are more similar to each other (because they are\\nnear each other) than text 3 (because it’s farther away).'}, {'page-number': 316, 'score': 0.8025796940090302, 'text': 'Overview of Semantic Search and RAG\\nThere’s a lot of research on how to best use language models for search.\\nThree broad categories of these models are dense retrieval, reranking, and\\nRAG. Here is an overview of these three categories that the rest of the\\nchapter will then explain in more detail:\\nDense retrieval\\nDense retrieval systems rely on the concept of embeddings, the same\\nconcept we’ve encountered in the previous chapters, and turn the search\\nproblem into retrieving the nearest neighbors of the search query (after\\nboth the query and the documents are converted into embeddings).\\nFigure 8-1 shows how dense retrieval takes a search query, consults its\\narchive of texts, and outputs a set of relevant results.\\nFigure 8-1. Dense retrieval is one of the key types of semantic search, relying on the similarity\\nof text embeddings to retrieve relevant results.\\nReranking\\nSearch systems are often pipelines of multiple steps. A reranking\\nlanguage model is one of these steps and is tasked with scoring the'}, {'page-number': 562, 'score': 0.7958404972258353, 'text': 'multi-hop RAG, Multi-hop RAG\\nmulti-query RAG, Multi-query RAG\\nquery rewriting, Query rewriting\\nquery routing, Query routing\\nwith local models, Example: RAG with Local Models, The RAG\\nprompt\\nsemantic search, Semantic Search and Retrieval-Augmented\\nGeneration-Scoring across multiple queries with mean average\\nprecision\\ndense retrieval, Overview of Semantic Search and RAG, Dense\\nRetrieval-Fine-tuning embedding models for dense retrieval\\nreranking, Overview of Semantic Search and RAG, Reranking-\\nHow reranking models work\\nretrieval evaluation metrics, Retrieval Evaluation Metrics-Scoring\\nacross multiple queries with mean average precision\\ntext classification, Text Classification-Summary\\nwith generative models, Text Classification with Generative\\nModels-ChatGPT for Classification\\nmovie reviews, The Sentiment of Movie Reviews, The Sentiment\\nof Movie Reviews\\nwith representation models, Text Classification with\\nRepresentation Models-What If We Do Not Have Labeled Data?\\ntext clustering, Text Clustering and Topic Modeling-Inspecting the\\nClusters\\ntext embedding models, Creating Text Embedding Models-Summary'}, {'page-number': 357, 'score': 0.7892711100952907, 'text': 'Perceived utility\\nWhether the generated answer is helpful and informative.\\nCitation recall\\nThe proportion of generated statements about the external world that are\\nfully supported by their citations.\\nCitation precision\\nThe proportion of generated citations that support their associated\\nstatements.\\nWhile human evaluation is always preferred, there are approaches that\\nattempt to automate these evaluations by having a capable LLM act as a\\njudge (called LLM-as-a-judge) and score the different generations along the\\ndifferent axes. Ragas is a software library that does exactly this. It also\\nscores some additional useful metrics like:\\nFaithfulness\\nWhether the answer is consistent with the provided context\\nAnswer relevance\\nHow relevant the answer is to the question\\nThe Ragas documentation site provides more details about the formulas to\\nactually calculate these metrics.\\nSummary\\nIn this chapter, we looked at different ways of using language models to\\nimprove existing search systems and even be the core of new, more'}]\n",
      "[(356, 0.8307425764322219), (318, 0.8224408695341826), (316, 0.8025796940090302), (562, 0.7958404972258353), (357, 0.7892711100952907)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = [\n",
    "    {\n",
    "    'page-number': doc.metadata.get(\"page\"),\n",
    "    'score' : score,\n",
    "    'text': doc.page_content\n",
    "    }\n",
    "\n",
    "    for doc, score in database.similarity_search_with_relevance_scores(\"what is agentic rag\", k=5)\n",
    "]\n",
    "print(results)\n",
    "scores= [(item['page-number'],item['score']) for item in results]\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b49a435",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "89ae9a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(356, 0.8307425764322219), (318, 0.8224408695341826), (316, 0.8025796940090302), (562, 0.7958404972258353), (357, 0.7892711100952907)]\n",
      "[(575, 0.999999999921749), (561, 0.8969531239724363), (536, 0.8716653374604687), (356, 0.6711819067485029), (542, 0.5235863667105769), (562, 0.45513670097404435), (348, 0.4118913815791562), (354, 0.37692730331059543), (318, 0.3381180448580582), (353, 0.32935187669117916)]\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(keyword_scores_with_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a7dd4",
   "metadata": {},
   "source": [
    "both scores and keyword_score_with_id are tuples so we convert em to dicts so they(scores) are mapped to the page_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "4d0866c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7350061746219905 Query routing\n",
      "An additional enhancement is to give the model the ability to search\n",
      "multiple data sources. We can, for example, specify for the model that if it\n",
      "gets a question about HR, it should search the company’s HR information\n",
      "system (e.g., Notion) but if the question is about customer data, that it\n",
      "should search the customer relationship management (CRM) (e.g.,\n",
      "Salesforce).\n",
      "Agentic RAG\n",
      "You may be able to now see that the list of previous enhancements slowly\n",
      "delegates more and more responsibility to the LLM to solve more and more\n",
      "complex problems. This relies on the LLM’s capability to gauge the\n",
      "required information needs as well as its ability to utilize multiple data\n",
      "sources. This new nature of the LLM starts to become closer and closer to\n",
      "an agent that acts on the world. The data sources can also now be abstracted\n",
      "into tools. We saw, for example, that we can search Notion, but by the same\n",
      "token, we should be able to post to Notion as well.\n",
      "Not all LLMs will have the RAG capabilities mentioned here. At the time\n",
      "of writing, likely only the largest managed models may be able to attempt\n",
      "this behavior. Thankfully, Cohere’s Command R+ excels at these tasks and\n",
      "is available as an open-weights model as well.\n",
      "RAG Evaluation\n",
      "There are still ongoing developments in how to evaluate RAG models. A\n",
      "good paper to read on this topic is “Evaluating verifiability in generative\n",
      "search engines” (2023), which runs human evaluations on different\n",
      "generative search systems.2 \n",
      "It evaluates results along four axes:\n",
      "Fluency\n",
      "Whether the generated text is fluent and cohesive.\n",
      "0.5999999999530494 training configuration, Training Configuration\n",
      "quantization, Model I/O: Loading Quantized Models with LangChain,\n",
      "Compressing the model for (more) efficient training-Compressing the\n",
      "model for (more) efficient training\n",
      "quantization_config parameter, LoRA Configuration\n",
      "quantized low-rank adaptation (see QLoRA)\n",
      " Querying Transformer (Q-Former), BLIP-2: Bridging the Modality Gap-\n",
      "BLIP-2: Bridging the Modality Gap\n",
      "R \n",
      "r parameter, LoRA Configuration\n",
      "RAG (retrieval-augmented generation), Token Embeddings, Text\n",
      "Embeddings (for Sentences and Whole Documents), Overview of Semantic\n",
      "Search and RAG, Retrieval-Augmented Generation (RAG)-RAG\n",
      "Evaluation\n",
      "agentic RAG, Agentic RAG\n",
      "basic pipeline, Retrieval-Augmented Generation (RAG)\n",
      "converting search system to, From Search to RAG\n",
      "evaluating results, RAG Evaluation\n",
      "grounded generation, Example: Grounded Generation with an LLM\n",
      "API\n",
      "with local models, Example: RAG with Local Models-The RAG\n",
      "prompt\n",
      "multi-hop RAG, Multi-hop RAG\n",
      "multi-query RAG, Multi-query RAG\n",
      "0.5914182194747608 multi-hop RAG, Multi-hop RAG\n",
      "multi-query RAG, Multi-query RAG\n",
      "query rewriting, Query rewriting\n",
      "query routing, Query routing\n",
      "with local models, Example: RAG with Local Models, The RAG\n",
      "prompt\n",
      "semantic search, Semantic Search and Retrieval-Augmented\n",
      "Generation-Scoring across multiple queries with mean average\n",
      "precision\n",
      "dense retrieval, Overview of Semantic Search and RAG, Dense\n",
      "Retrieval-Fine-tuning embedding models for dense retrieval\n",
      "reranking, Overview of Semantic Search and RAG, Reranking-\n",
      "How reranking models work\n",
      "retrieval evaluation metrics, Retrieval Evaluation Metrics-Scoring\n",
      "across multiple queries with mean average precision\n",
      "text classification, Text Classification-Summary\n",
      "with generative models, Text Classification with Generative\n",
      "Models-ChatGPT for Classification\n",
      "movie reviews, The Sentiment of Movie Reviews, The Sentiment\n",
      "of Movie Reviews\n",
      "with representation models, Text Classification with\n",
      "Representation Models-What If We Do Not Have Labeled Data?\n",
      "text clustering, Text Clustering and Topic Modeling-Inspecting the\n",
      "Clusters\n",
      "text embedding models, Creating Text Embedding Models-Summary\n",
      "0.5381718743834618 chain prompting, Chain Prompting: Breaking up the Problem-\n",
      "Chain Prompting: Breaking up the Problem\n",
      "in-context learning, In-Context Learning: Providing Examples-In-\n",
      "Context Learning: Providing Examples\n",
      "instruction-based prompting, Instruction-Based Prompting-\n",
      "Instruction-Based Prompting\n",
      "output verification, Output Verification-Grammar: Constrained\n",
      "Sampling\n",
      "potential complexity of prompts, The Potential Complexity of a\n",
      "Prompt-The Potential Complexity of a Prompt\n",
      "prompt components, The Basic Ingredients of a Prompt-The\n",
      "Basic Ingredients of a Prompt\n",
      "reasoning with generative models, Reasoning with Generative\n",
      "Models-Tree-of-Thought: Exploring Intermediate Steps\n",
      "text generation models, Using Text Generation Models-top_p\n",
      "representation models, Representation Models: Encoder-Only Models-\n",
      "Representation Models: Encoder-Only Models\n",
      "responsible development and usage of, Responsible LLM\n",
      "Development and Usage\n",
      "retrieval-augmented generation, Overview of Semantic Search and\n",
      "RAG, Retrieval-Augmented Generation (RAG)-RAG Evaluation\n",
      "agentic RAG, Agentic RAG\n",
      "converting search system to, From Search to RAG\n",
      "evaluating results, RAG Evaluation\n",
      "grounded generation, Example: Grounded Generation with an\n",
      "LLM API\n",
      "0.531847174728508 Figure 8-3. A RAG system formulates an answer to a question and (preferably) cites its information\n",
      "sources.\n",
      "The rest of the chapter covers these three types of systems in more detail.\n",
      "While these are the major categories, they are not the only LLM\n",
      "applications in the domain of search.\n",
      "Semantic Search with Language Models\n",
      "Let’s now dive into more detail on the major categories of systems that can\n",
      "upgrade the search capabilities of our language models. We’ll start with\n",
      "dense retrieval and then move on through reranking and RAG.\n",
      "Dense Retrieval\n",
      "Recall that embeddings turn text into numeric representations. Those can be\n",
      "thought of as points in space, as we can see in Figure 8-4. Points that are\n",
      "close together mean that the text they represent is similar. So in this\n",
      "example, text 1 and text 2 are more similar to each other (because they are\n",
      "near each other) than text 3 (because it’s farther away).\n"
     ]
    }
   ],
   "source": [
    "bm25_dict = dict(keyword_scores_with_id)       # {356: 0.8307, 421: 0.7923, ...}\n",
    "vector_dict = dict(scores)   # same format for vector search\n",
    "alpha=0.6\n",
    "combined = []\n",
    "for doc_id in set(bm25_dict) | set(vector_dict):  # all doc IDs from both searches\n",
    "    bm25_score = bm25_dict.get(doc_id, 0.0)\n",
    "    vector_score = vector_dict.get(doc_id, 0.0)\n",
    "    final_score = alpha * bm25_score + (1 - alpha) * vector_score\n",
    "    combined.append((doc_id, final_score))\n",
    "\n",
    "# sort by final score, then fetch text\n",
    "combined.sort(key=lambda x: x[1], reverse=True)\n",
    "for doc_id, score in combined[:5]:\n",
    "    print(score, page_texts[doc_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "147bfb3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[237], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc_id \u001b[38;5;129;01min\u001b[39;00m page_ids:\n\u001b[0;32m----> 4\u001b[0m     combined_score\u001b[38;5;241m=\u001b[39malpha\u001b[38;5;241m*\u001b[39m\u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39malpha)\u001b[38;5;241m*\u001b[39mkeyword_scores_with_id[doc_id]\n\u001b[1;32m      5\u001b[0m     combined_results\u001b[38;5;241m.\u001b[39mappend((doc_id, combined_score))\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "combined_results=[]\n",
    "alpha=0.5\n",
    "for doc_id in page_ids:\n",
    "    combined_score=alpha*scores[doc_id]+ (1-alpha)*keyword_scores_with_id[doc_id]\n",
    "    combined_results.append((doc_id, combined_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9be3d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers.weaviate_hybrid_search import WeaviateHybridSearchRetriever\n",
    "# retriever = WeaviateHybridSearchRetriever(\n",
    "#     alpha = 0.5,               # defaults to 0.5, which is equal weighting between keyword and semantic search\n",
    "#     client = query,           # keyword arguments to pass to the Weaviate client\n",
    "#     index_name = \"page_ids\",  # The name of the index to use\n",
    "#     text_key = \"page_texts\",         # The name of the text key to use\n",
    "#     attributes = [],           # The attributes to return in the results\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c0e80e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
